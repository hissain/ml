{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hissain/mlworks/blob/main/codes/RNN_Skip_Gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "kH3NgfTuJJXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constant definition\n",
        "RANDOM_SEED = 1\n",
        "CONTEXT_SIZE = 3\n",
        "EMBEDDING_DIM = 10"
      ],
      "metadata": {
        "id": "ydRlAUfNJYAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPHTsGZiVE5y",
        "outputId": "a128204f-f0b8-456c-c07e-65ff11b3b952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7da1343959f0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word level tokenization\n",
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        self.mapping = {}\n",
        "        self.reverse_mapping = {}\n",
        "\n",
        "    def encode(self, text):\n",
        "        words = text.split()\n",
        "        tokens = []\n",
        "        for word in words:\n",
        "            if word not in self.mapping:\n",
        "                mapped_int = len(self.mapping)\n",
        "                self.mapping[word] = mapped_int\n",
        "                self.reverse_mapping[mapped_int] = word\n",
        "            tokens.append(self.mapping[word])\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        words = [self.reverse_mapping[token] for token in tokens]\n",
        "        return \" \".join(words)\n"
      ],
      "metadata": {
        "id": "KixNPnOqJs4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "rDEtkR14g7Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking a random paragraph\n",
        "text = \"We are living in an AI era . One day AI will take all the Human jobs .\""
      ],
      "metadata": {
        "id": "mV-GVqEohGcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating dataset\n",
        "dataset = []\n",
        "tokens = tokenizer.encode(text)\n",
        "for i in range(len(tokens) - CONTEXT_SIZE):\n",
        "    dataset.append((tokens[i + CONTEXT_SIZE], tokens[i:i + CONTEXT_SIZE]))"
      ],
      "metadata": {
        "id": "4Y4vNjoGJvX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.mapping)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpXbcKGPg9ch",
        "outputId": "86d72fd3-b0f0-4ec7-9654-789c8abb5978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Used for tokenization purpose\n",
        "def get_one_hot(tokens):\n",
        "    return F.one_hot(\n",
        "        torch.tensor(tokens),\n",
        "        num_classes=vocab_size\n",
        "    ).flatten().type(torch.float)"
      ],
      "metadata": {
        "id": "FOTiazubYQXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the SkipGram model\n",
        "class SkipGram(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "    super(SkipGram, self).__init__()\n",
        "    self.hidden = nn.Linear(vocab_size, embedding_dim)\n",
        "    self.output = nn.Linear(embedding_dim, vocab_size * context_size)\n",
        "\n",
        "  def forward(self, input):\n",
        "    return self.output(self.hidden(get_one_hot(input)))"
      ],
      "metadata": {
        "id": "cGjResgcYqKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model = SkipGram(vocab_size, EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "QLNEtvRZYr2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "for epoch in range(100):\n",
        "    for input, label in dataset:\n",
        "        predictions = model(input)\n",
        "        loss = loss_function(predictions, get_one_hot(label))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "sRpSJfJTYtMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "input_text = \"will\"\n",
        "correct_output = \"One day AI\"\n",
        "\n",
        "with torch.no_grad():\n",
        "    prediction = model(tokenizer.encode(input_text))\n",
        "    print(torch.argmax(prediction[0:16]).item(), tokenizer.reverse_mapping[torch.argmax(prediction[0:16]).item()])\n",
        "    print(torch.argmax(prediction[16:32]).item(), tokenizer.reverse_mapping[torch.argmax(prediction[16:32]).item()])\n",
        "    print(torch.argmax(prediction[32:48]).item(), tokenizer.reverse_mapping[torch.argmax(prediction[32:48]).item()])"
      ],
      "metadata": {
        "id": "zy3ZKHI5JhMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e273f86c-d19e-4d16-ccdb-b81cb4179190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 One\n",
            "9 day\n",
            "5 AI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding of a word\n",
        "token = tokenizer.mapping[\"will\"]\n",
        "print(model.hidden.weight[:, token])"
      ],
      "metadata": {
        "id": "TI9fW5PadvqQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa340b7-2620-4560-a48c-f9429f72ab3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.5661,  0.6565,  0.6039,  0.5392, -0.2362,  0.5543,  0.7447,  0.7718,\n",
            "        -0.6273, -0.6929], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ppyhfqB0rOkY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}