{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2643936",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 valid links. Scraping the content...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c97d081296466e950e02da732f8231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition length: 4\n",
      "Embedding dimension: (3072,)\n",
      "embedding_sample dimension: 3072\n",
      "Embedding partition 1 of 4\n",
      "Embedding dimension: (3072,)\n",
      "Embedding partition 2 of 4\n",
      "Embedding dimension: (3072,)\n",
      "Embedding partition 3 of 4\n",
      "Embedding dimension: (3072,)\n",
      "Embedding partition 4 of 4\n",
      "Embedding dimension: (3072,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'IndexFlatL2' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 170\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Store the partitions in FAISS\u001b[39;00m\n\u001b[1;32m    169\u001b[0m faiss_index, doc_ids \u001b[38;5;241m=\u001b[39m store_in_faiss(partitions, embedding_func\u001b[38;5;241m=\u001b[39mget_embedding)\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(faiss_index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, doc_ids length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(doc_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Query and retrieval with RAG\u001b[39;00m\n\u001b[1;32m    173\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhere is the location of Bangladesh?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'IndexFlatL2' has no len()"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "# Configure Selenium Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')  # Run in headless mode\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "service = Service()\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def get_visitable_links(base_url):\n",
    "    \"\"\"Scrapes all valid links from the base_url.\"\"\"\n",
    "    driver.get(base_url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    links = []\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        full_url = urljoin(base_url, a_tag['href'])\n",
    "        if '#' not in full_url and '%' not in full_url and full_url.startswith(base_url) and full_url not in links:\n",
    "            links.append(full_url)\n",
    "    return links\n",
    "\n",
    "def scrape_text_from_url(url):\n",
    "    \"\"\"Scrapes the textual content from a given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve {url}\")\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Use get_text() without strip=True to preserve spaces between inline elements\n",
    "    text_content = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "    \n",
    "    # Optionally, normalize excessive whitespace\n",
    "    text_content = ' '.join(text_content.split())\n",
    "    \n",
    "    return text_content\n",
    "\n",
    "def partition_text(text, max_length=512):\n",
    "    \"\"\"Partitions the text into smaller parts to feed into RAG.\"\"\"\n",
    "    sentences = text.split('. ')\n",
    "    partitions = []\n",
    "    current_part = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        current_length += len(sentence.split())\n",
    "        current_part.append(sentence)\n",
    "        \n",
    "        if current_length > max_length:\n",
    "            partitions.append('. '.join(current_part))\n",
    "            current_part = []\n",
    "            current_length = 0\n",
    "\n",
    "    if current_part:\n",
    "        partitions.append('. '.join(current_part))\n",
    "\n",
    "    return partitions\n",
    "\n",
    "\n",
    "ollama_url_emb = \"http://localhost:11434/api/embeddings\"\n",
    "ollama_url_gen = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "def get_embedding(text):\n",
    "    payload = {\n",
    "        \"model\": \"llama3.2:latest\",\n",
    "        \"prompt\": text\n",
    "    }\n",
    "    \n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(ollama_url_emb, headers=headers, data=json.dumps(payload))\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        embedding = np.array(result['embedding'])\n",
    "        print(f\"Embedding dimension: {embedding.shape}\")  # Add this line to check the embedding shape\n",
    "        return embedding\n",
    "    else:\n",
    "        print(f\"Error from Ollama: {response.status_code}\")\n",
    "        return np.zeros(768)  # Return zero vector on error (adjust dimension based on model)\n",
    "\n",
    "def store_in_faiss(partitions, embedding_func):\n",
    "    \"\"\"Stores document embeddings in a FAISS index.\"\"\"\n",
    "    embedding_sample = embedding_func(partitions[0])  # Get one embedding to check dimension\n",
    "    dimension = embedding_sample.shape[0]  # Automatically determine dimension\n",
    "    print(f\"embedding_sample dimension: {dimension}\")\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance\n",
    "\n",
    "    doc_vectors = []\n",
    "    doc_ids = []\n",
    "    \n",
    "    for i, partition in enumerate(partitions):\n",
    "        print(f\"Embedding partition {i+1} of {len(partitions)}\")  # Debugging line to trace embedding calls\n",
    "        embedding = embedding_func(partition)  # Calling get_embedding()\n",
    "        index.add(np.array([embedding]))  # Add to FAISS index\n",
    "        doc_vectors.append(embedding)\n",
    "        doc_ids.append(i)\n",
    "    \n",
    "    return index, doc_ids\n",
    "\n",
    "def retrieve_with_rag(query, faiss_index, doc_ids):\n",
    "    query_embedding = get_embedding(query)\n",
    "    print(f'embedding length: {len(query_embedding)}')\n",
    "    \n",
    "    # Search in FAISS index\n",
    "    print(\"Performing FAISS search...\")\n",
    "    D, I = faiss_index.search(np.array([query_embedding]), k=2)  # Retrieve top-1 closest documents\n",
    "    print(f\"FAISS search done. D: {D}, I: {I}\")\n",
    "\n",
    "    # Check if FAISS returned valid results\n",
    "    if len(I) == 0 or len(I[0]) == 0:\n",
    "        print(\"No documents found.\")\n",
    "        return\n",
    "    \n",
    "    # For simplicity, concatenate the closest documents and feed to RAG\n",
    "    retrieved_docs = []\n",
    "    for i in I[0]:\n",
    "        if i >= len(partitions):\n",
    "            print(f\"Index {i} out of bounds for partition list.\")\n",
    "            continue\n",
    "        doc_id = doc_ids[i]\n",
    "        retrieved_docs.append(partitions[doc_id])  # Get the document text from partition list\n",
    "    \n",
    "    combined_docs = \"\\n\".join(retrieved_docs)\n",
    "    print(f\"Retrieved docs: {combined_docs}\")\n",
    "    \n",
    "    # Perform RAG with retrieved docs\n",
    "    rag_prompt = f\"Context:\\n{combined_docs}\\n\\nQuery: {query}\\nAnswer:\"\n",
    "    print(f\"RAG Prompt: {rag_prompt}\")\n",
    "    \n",
    "    response = requests.post(ollama_url_gen, headers={\"Content-Type\": \"application/json\"},\n",
    "                             data=json.dumps({\"model\": \"llama3.2:latest\", \"prompt\": rag_prompt}))\n",
    "    \n",
    "    print(response)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"completion\"]\n",
    "    else:\n",
    "        return f\"Error from Ollama: {response.status_code}\"\n",
    "\n",
    "    \n",
    "# Main logic to scrape and partition data\n",
    "base_url = 'https://en.wikipedia.org/wiki/Bangladesh'\n",
    "valid_links = get_visitable_links(base_url)[1:4]\n",
    "\n",
    "print(f\"Found {len(valid_links)} valid links. Scraping the content...\")\n",
    "\n",
    "# Scrape text content from all valid links\n",
    "all_text = \"\"\n",
    "for link in tqdm(valid_links):\n",
    "    text_content = scrape_text_from_url(link)\n",
    "    all_text += text_content + \"\\n\\n\"\n",
    "    \n",
    "# Partition the text\n",
    "partitions = partition_text(all_text, max_length=512)\n",
    "print(f'partition length: {len(partitions)}')\n",
    "\n",
    "# Store the partitions in FAISS\n",
    "faiss_index, doc_ids = store_in_faiss(partitions, embedding_func=get_embedding)\n",
    "\n",
    "# Query and retrieval with RAG\n",
    "query = \"Where is the location of Bangladesh?\"\n",
    "rag_response = retrieve_with_rag(query, faiss_index, doc_ids)\n",
    "\n",
    "print(f\"RAG Response:\\n{rag_response}\")\n",
    "\n",
    "# Close the Selenium browser\n",
    "#driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
