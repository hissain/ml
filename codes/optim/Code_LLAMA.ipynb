{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hissain/mlworks/blob/main/codes/Code_LLAMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWULgmeC4mi5",
        "outputId": "f46aad82-70ca-4c0c-9e59-b477bbe7e13b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/.shortcut-targets-by-id/1oMxz64J2oghjUyIKDjVZzVyAnPZ3Zzz2/Deep Learning/Notebooks/Code LLAMA\n",
            " CodeLlama-7b\t      dev-requirements.txt    example_instructions.py   README.md\n",
            "'Code LLAMA.ipynb'    download.sh\t      LICENSE\t\t        requirements.txt\n",
            " CODE_OF_CONDUCT.md   example_completion.py   llama\t\t        setup.py\n",
            " CONTRIBUTING.md      example_infilling.py    MODEL_CARD.md\t        USE_POLICY.md\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# %cd /content/gdrive/My\\ Drive/BRTC/Deep\\ Learning/Notebooks/Code\\ LLAMA\n",
        "%cd /content/gdrive/My\\ Drive/Deep\\ Learning/Notebooks/Code\\ LLAMA\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !chmod +x download.sh\n",
        "# !./download.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVCYiXHt6Upp",
        "outputId": "1b6ec6b1-c826-4e12-87e4-27cc891df590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the URL from email: https://download2.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiMzBkNTQ4OGhsMmNleWZncTVwcGQ1dnB4IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3MTk1Mzc1MzV9fX1dfQ__&Signature=dbzVU%7EJMRXzolzPJH2osyAwEYFj173sJ%7EemVk9oe84swegzEQgiGXIoTJdLTmjSqSKJ2wR72cAy2c0-PRO%7EMMXazAY82OBNtbPzD81Kx7t4-YXLbNtBNPp4hjUEm5Hi-qNLgZETAv8eksStGEsIIkzbMiLdrYd5QSQPePry7T4-CW-8FXDqMR3LgNNLUoI0BJKyXJL8o8ZbUm3D5iyHrSP-x-2BTpGgWLbWdKrZvU%7E9rYgt7y3Rew8FH7vaxbLXkgeALpHMny8awRMdP3w%7EGsIeNwNsqQ7DteIzBixWXUqC-9FVFQ9NXthAucgfuudbI5VfPKOnudcGObI5ojcFvCg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=7624505797667713\n",
            "\n",
            "Enter the list of models to download without spaces (7b,13b,34b,70b,7b-Python,13b-Python,34b-Python,70b-Python,7b-Instruct,13b-Instruct,34b-Instruct,70b-Instruct), or press Enter for all: 7b\n",
            "Downloading LICENSE and Acceptable Usage Policy\n",
            "--2024-06-27 04:18:37--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiMzBkNTQ4OGhsMmNleWZncTVwcGQ1dnB4IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3MTk1Mzc1MzV9fX1dfQ__&Signature=dbzVU%7EJMRXzolzPJH2osyAwEYFj173sJ%7EemVk9oe84swegzEQgiGXIoTJdLTmjSqSKJ2wR72cAy2c0-PRO%7EMMXazAY82OBNtbPzD81Kx7t4-YXLbNtBNPp4hjUEm5Hi-qNLgZETAv8eksStGEsIIkzbMiLdrYd5QSQPePry7T4-CW-8FXDqMR3LgNNLUoI0BJKyXJL8o8ZbUm3D5iyHrSP-x-2BTpGgWLbWdKrZvU%7E9rYgt7y3Rew8FH7vaxbLXkgeALpHMny8awRMdP3w%7EGsIeNwNsqQ7DteIzBixWXUqC-9FVFQ9NXthAucgfuudbI5VfPKOnudcGObI5ojcFvCg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=7624505797667713\n",
            "Resolving download2.llamameta.net (download2.llamameta.net)... 108.159.227.35, 108.159.227.85, 108.159.227.16, ...\n",
            "Connecting to download2.llamameta.net (download2.llamameta.net)|108.159.227.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 206 Partial Content\n",
            "Length: 7020 (6.9K), 6935 (6.8K) remaining [binary/octet-stream]\n",
            "Saving to: ‘./LICENSE’\n",
            "\n",
            "./LICENSE           100%[===================>]   6.86K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-06-27 04:18:38 (172 MB/s) - ‘./LICENSE’ saved [7020/7020]\n",
            "\n",
            "--2024-06-27 04:18:38--  https://download2.llamameta.net/USE_POLICY.md?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiMzBkNTQ4OGhsMmNleWZncTVwcGQ1dnB4IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3MTk1Mzc1MzV9fX1dfQ__&Signature=dbzVU%7EJMRXzolzPJH2osyAwEYFj173sJ%7EemVk9oe84swegzEQgiGXIoTJdLTmjSqSKJ2wR72cAy2c0-PRO%7EMMXazAY82OBNtbPzD81Kx7t4-YXLbNtBNPp4hjUEm5Hi-qNLgZETAv8eksStGEsIIkzbMiLdrYd5QSQPePry7T4-CW-8FXDqMR3LgNNLUoI0BJKyXJL8o8ZbUm3D5iyHrSP-x-2BTpGgWLbWdKrZvU%7E9rYgt7y3Rew8FH7vaxbLXkgeALpHMny8awRMdP3w%7EGsIeNwNsqQ7DteIzBixWXUqC-9FVFQ9NXthAucgfuudbI5VfPKOnudcGObI5ojcFvCg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=7624505797667713\n",
            "Resolving download2.llamameta.net (download2.llamameta.net)... 108.159.227.35, 108.159.227.85, 108.159.227.16, ...\n",
            "Connecting to download2.llamameta.net (download2.llamameta.net)|108.159.227.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 206 Partial Content\n",
            "Length: 4790 (4.7K), 4685 (4.6K) remaining [text/markdown]\n",
            "Saving to: ‘./USE_POLICY.md’\n",
            "\n",
            "./USE_POLICY.md     100%[===================>]   4.68K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-06-27 04:18:39 (140 MB/s) - ‘./USE_POLICY.md’ saved [4790/4790]\n",
            "\n",
            "Downloading CodeLlama-7b\n",
            "--2024-06-27 04:18:39--  https://download2.llamameta.net/CodeLlama-7b/consolidated.00.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiMzBkNTQ4OGhsMmNleWZncTVwcGQ1dnB4IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3MTk1Mzc1MzV9fX1dfQ__&Signature=dbzVU%7EJMRXzolzPJH2osyAwEYFj173sJ%7EemVk9oe84swegzEQgiGXIoTJdLTmjSqSKJ2wR72cAy2c0-PRO%7EMMXazAY82OBNtbPzD81Kx7t4-YXLbNtBNPp4hjUEm5Hi-qNLgZETAv8eksStGEsIIkzbMiLdrYd5QSQPePry7T4-CW-8FXDqMR3LgNNLUoI0BJKyXJL8o8ZbUm3D5iyHrSP-x-2BTpGgWLbWdKrZvU%7E9rYgt7y3Rew8FH7vaxbLXkgeALpHMny8awRMdP3w%7EGsIeNwNsqQ7DteIzBixWXUqC-9FVFQ9NXthAucgfuudbI5VfPKOnudcGObI5ojcFvCg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=7624505797667713\n",
            "Resolving download2.llamameta.net (download2.llamameta.net)... 108.159.227.35, 108.159.227.85, 108.159.227.16, ...\n",
            "Connecting to download2.llamameta.net (download2.llamameta.net)|108.159.227.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13477187307 (13G) [binary/octet-stream]\n",
            "Saving to: ‘./CodeLlama-7b/consolidated.00.pth’\n",
            "\n",
            "./CodeLlama-7b/cons 100%[===================>]  12.55G  45.5MB/s    in 5m 23s  \n",
            "\n",
            "2024-06-27 04:24:02 (39.8 MB/s) - ‘./CodeLlama-7b/consolidated.00.pth’ saved [13477187307/13477187307]\n",
            "\n",
            "--2024-06-27 04:24:04--  https://download2.llamameta.net/CodeLlama-7b/params.json?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiMzBkNTQ4OGhsMmNleWZncTVwcGQ1dnB4IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3MTk1Mzc1MzV9fX1dfQ__&Signature=dbzVU%7EJMRXzolzPJH2osyAwEYFj173sJ%7EemVk9oe84swegzEQgiGXIoTJdLTmjSqSKJ2wR72cAy2c0-PRO%7EMMXazAY82OBNtbPzD81Kx7t4-YXLbNtBNPp4hjUEm5Hi-qNLgZETAv8eksStGEsIIkzbMiLdrYd5QSQPePry7T4-CW-8FXDqMR3LgNNLUoI0BJKyXJL8o8ZbUm3D5iyHrSP-x-2BTpGgWLbWdKrZvU%7E9rYgt7y3Rew8FH7vaxbLXkgeALpHMny8awRMdP3w%7EGsIeNwNsqQ7DteIzBixWXUqC-9FVFQ9NXthAucgfuudbI5VfPKOnudcGObI5ojcFvCg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=7624505797667713\n",
            "Resolving download2.llamameta.net (download2.llamameta.net)... 108.159.227.16, 108.159.227.7, 108.159.227.35, ...\n",
            "Connecting to download2.llamameta.net (download2.llamameta.net)|108.159.227.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 163 [application/json]\n",
            "Saving to: ‘./CodeLlama-7b/params.json’\n",
            "\n",
            "./CodeLlama-7b/para 100%[===================>]     163  --.-KB/s    in 0s      \n",
            "\n",
            "2024-06-27 04:24:05 (43.7 MB/s) - ‘./CodeLlama-7b/params.json’ saved [163/163]\n",
            "\n",
            "--2024-06-27 04:24:05--  https://download2.llamameta.net/CodeLlama-7b/tokenizer.model?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiMzBkNTQ4OGhsMmNleWZncTVwcGQ1dnB4IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3MTk1Mzc1MzV9fX1dfQ__&Signature=dbzVU%7EJMRXzolzPJH2osyAwEYFj173sJ%7EemVk9oe84swegzEQgiGXIoTJdLTmjSqSKJ2wR72cAy2c0-PRO%7EMMXazAY82OBNtbPzD81Kx7t4-YXLbNtBNPp4hjUEm5Hi-qNLgZETAv8eksStGEsIIkzbMiLdrYd5QSQPePry7T4-CW-8FXDqMR3LgNNLUoI0BJKyXJL8o8ZbUm3D5iyHrSP-x-2BTpGgWLbWdKrZvU%7E9rYgt7y3Rew8FH7vaxbLXkgeALpHMny8awRMdP3w%7EGsIeNwNsqQ7DteIzBixWXUqC-9FVFQ9NXthAucgfuudbI5VfPKOnudcGObI5ojcFvCg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=7624505797667713\n",
            "Resolving download2.llamameta.net (download2.llamameta.net)... 108.159.227.16, 108.159.227.7, 108.159.227.35, ...\n",
            "Connecting to download2.llamameta.net (download2.llamameta.net)|108.159.227.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 500058 (488K) [binary/octet-stream]\n",
            "Saving to: ‘./CodeLlama-7b/tokenizer.model’\n",
            "\n",
            "./CodeLlama-7b/toke 100%[===================>] 488.34K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-06-27 04:24:05 (7.25 MB/s) - ‘./CodeLlama-7b/tokenizer.model’ saved [500058/500058]\n",
            "\n",
            "--2024-06-27 04:24:05--  https://download2.llamameta.net/CodeLlama-7b/checklist.chk?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiMzBkNTQ4OGhsMmNleWZncTVwcGQ1dnB4IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3MTk1Mzc1MzV9fX1dfQ__&Signature=dbzVU%7EJMRXzolzPJH2osyAwEYFj173sJ%7EemVk9oe84swegzEQgiGXIoTJdLTmjSqSKJ2wR72cAy2c0-PRO%7EMMXazAY82OBNtbPzD81Kx7t4-YXLbNtBNPp4hjUEm5Hi-qNLgZETAv8eksStGEsIIkzbMiLdrYd5QSQPePry7T4-CW-8FXDqMR3LgNNLUoI0BJKyXJL8o8ZbUm3D5iyHrSP-x-2BTpGgWLbWdKrZvU%7E9rYgt7y3Rew8FH7vaxbLXkgeALpHMny8awRMdP3w%7EGsIeNwNsqQ7DteIzBixWXUqC-9FVFQ9NXthAucgfuudbI5VfPKOnudcGObI5ojcFvCg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=7624505797667713\n",
            "Resolving download2.llamameta.net (download2.llamameta.net)... 108.159.227.16, 108.159.227.7, 108.159.227.35, ...\n",
            "Connecting to download2.llamameta.net (download2.llamameta.net)|108.159.227.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 150 [binary/octet-stream]\n",
            "Saving to: ‘./CodeLlama-7b/checklist.chk’\n",
            "\n",
            "./CodeLlama-7b/chec 100%[===================>]     150  --.-KB/s    in 0s      \n",
            "\n",
            "2024-06-27 04:24:05 (42.5 MB/s) - ‘./CodeLlama-7b/checklist.chk’ saved [150/150]\n",
            "\n",
            "Checking checksums\n",
            "consolidated.00.pth: OK\n",
            "params.json: OK\n",
            "tokenizer.model: OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r 'requirements.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99hZXANc8-YD",
        "outputId": "b8b53d71-60bb-4db6-d088-4b4daebddc12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.3.0+cu121)\n",
            "Collecting fairscale (from -r requirements.txt (line 2))\n",
            "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/266.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m266.2/266.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fire (from -r requirements.txt (line 3))\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from fairscale->-r requirements.txt (line 2)) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->-r requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->-r requirements.txt (line 3)) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Building wheels for collected packages: fairscale, fire\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332103 sha256=7f08cd7e4cf5a85249b502d2adcaa5e40efab33a56b2b8f3bc546d9b8fb7df9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=e77ebe97c33a1bebe3f8477d6dbadc1a68304381bb456fc4c4e0e07f73070d7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built fairscale fire\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fire, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, fairscale\n",
            "Successfully installed fairscale-0.4.13 fire-0.6.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python example_completion.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1_aUhbY-WEd",
        "outputId": "1edd5389-4069-4bb8-d37a-602d1d767c4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Showing help with the command 'example_completion.py -- --help'.\n",
            "\n",
            "\u001b[1mNAME\u001b[0m\n",
            "    example_completion.py\n",
            "\n",
            "\u001b[1mSYNOPSIS\u001b[0m\n",
            "    example_completion.py \u001b[4mCKPT_DIR\u001b[0m \u001b[4mTOKENIZER_PATH\u001b[0m <flags>\n",
            "\n",
            "\u001b[1mPOSITIONAL ARGUMENTS\u001b[0m\n",
            "    \u001b[1m\u001b[4mCKPT_DIR\u001b[0m\u001b[0m\n",
            "        Type: str\n",
            "    \u001b[1m\u001b[4mTOKENIZER_PATH\u001b[0m\u001b[0m\n",
            "        Type: str\n",
            "\n",
            "\u001b[1mFLAGS\u001b[0m\n",
            "    --temperature=\u001b[4mTEMPERATURE\u001b[0m\n",
            "        Type: float\n",
            "        Default: 0.2\n",
            "    --top_p=\u001b[4mTOP_P\u001b[0m\n",
            "        Type: float\n",
            "        Default: 0.9\n",
            "    --max_seq_len=\u001b[4mMAX_SEQ_LEN\u001b[0m\n",
            "        Type: int\n",
            "        Default: 256\n",
            "    --max_batch_size=\u001b[4mMAX_BATCH_SIZE\u001b[0m\n",
            "        Type: int\n",
            "        Default: 4\n",
            "    --max_gen_len=\u001b[4mMAX_GEN_LEN\u001b[0m\n",
            "        Type: Optional[Optional]\n",
            "        Default: None\n",
            "\n",
            "\u001b[1mNOTES\u001b[0m\n",
            "    You can also use flags syntax for POSITIONAL ARGUMENTS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch example_completion.py './CodeLlama-7b' './CodeLlama-7b/tokenizer.model'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_kR9kQm-zys",
        "outputId": "37e59524-a80a-4b77-802d-a1ff32b0f2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use-env is set by default in torchrun.\n",
            "If your script expects `--local-rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  warnings.warn(\n",
            "> initializing model parallel with size 1\n",
            "> initializing ddp with size 1\n",
            "> initializing pipeline with size 1\n",
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:747: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:431.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "Loaded in 306.10 seconds\n",
            "def fizzbuzz(n: int):\n",
            "> \n",
            "    if n % 3 == 0 and n % 5 == 0:\n",
            "        return 'FizzBuzz'\n",
            "    elif n % 3 == 0:\n",
            "        return 'Fizz'\n",
            "    elif n % 5 == 0:\n",
            "        return 'Buzz'\n",
            "    else:\n",
            "        return n\n",
            "\n",
            "\n",
            "def fizzbuzz_list(n: int):\n",
            "    return [fizzbuzz(i) for i in range(1, n + 1)]\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    print(fizzbuzz_list(15))\n",
            "\n",
            "\n",
            "==================================\n",
            "\n",
            "import argparse\n",
            "\n",
            "def main(string: str):\n",
            "    print(string)\n",
            "    print(string[::-1])\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "> \n",
            "    parser = argparse.ArgumentParser(description='Reverse a string')\n",
            "    parser.add_argument('string', type=str, help='string to reverse')\n",
            "    args = parser.parse_args()\n",
            "    main(args.string)\n",
            "\n",
            "\n",
            "# python3 reverse_string.py --string \"hello\"\n",
            "# python3 reverse_string.py -s \"hello\"\n",
            "# python3 reverse_string.py -s \"hello\" -s \"world\"\n",
            "# python3 reverse_string.py -s \"hello\" -s \"world\" -s \"!\"\n",
            "# python3 reverse_string.py -s \"hello\" -s \"world\" -s \"!\" -s \"?\"\n",
            "# python3 reverse_string.py -s \"hello\" -s \"world\" -s \"!\" -s \"?\" -s \"!\"\n",
            "# python3 reverse_string.py -s \"hello\" -s \"world\" -s \"!\" -s \"?\" -s \"\n",
            "\n",
            "==================================\n",
            "\n",
            "\u001b[1m\u001b[31mERROR: \u001b[0mCould not consume arg: --local-rank=0\n",
            "Usage: example_completion.py --local-rank=0 ./CodeLlama-7b -\n",
            "\n",
            "For detailed information on this command, run:\n",
            "  example_completion.py --local-rank=0 ./CodeLlama-7b - --help\n",
            "E0627 04:42:45.927000 136014633591424 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 2) local_rank: 0 (pid: 3798) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py\", line 198, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py\", line 194, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py\", line 179, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 870, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 263, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "example_completion.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2024-06-27_04:42:45\n",
            "  host      : bbe46bb77f22\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 2 (pid: 3798)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch example_infilling.py './CodeLlama-7b' './CodeLlama-7b/tokenizer.model'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09SImysi9Yix",
        "outputId": "c24ae21b-018c-4da5-cd33-0c6c8588cbcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use-env is set by default in torchrun.\n",
            "If your script expects `--local-rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  warnings.warn(\n",
            "> initializing model parallel with size 1\n",
            "> initializing ddp with size 1\n",
            "> initializing pipeline with size 1\n",
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:747: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:431.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "Loaded in 241.15 seconds\n",
            "\n",
            "================= Prompt text =================\n",
            "\n",
            "def remove_non_ascii(s: str) -> str:\n",
            "    \"\"\" <FILL>\n",
            "    return result\n",
            "\n",
            "\n",
            "================= Filled text =================\n",
            "\n",
            "def remove_non_ascii(s: str) -> str:\n",
            "    \"\"\" Remove non-ASCII characters from a string.\n",
            "\n",
            "    Args:\n",
            "        s: The string to remove non-ASCII characters from.\n",
            "\n",
            "    Returns:\n",
            "        The string with non-ASCII characters removed.\n",
            "    \"\"\"\n",
            "    result = \"\"\n",
            "    for c in s:\n",
            "        if ord(c) < 128:\n",
            "            result += c\n",
            "    return result\n",
            "\n",
            "\n",
            "================= Prompt text =================\n",
            "\n",
            "# Installation instructions:\n",
            "    ```bash\n",
            "<FILL>\n",
            "    ```\n",
            "This downloads the LLaMA inference code and installs the repository as a local pip package.\n",
            "\n",
            "\n",
            "================= Filled text =================\n",
            "\n",
            "# Installation instructions:\n",
            "    ```bash\n",
            "    git clone https://github.com/LLaMA-AI/LLaMA-inference.git\n",
            "    cd LLaMA-inference\n",
            "    pip install -e .\n",
            "    ```\n",
            "This downloads the LLaMA inference code and installs the repository as a local pip package.\n",
            "\n",
            "\n",
            "================= Prompt text =================\n",
            "\n",
            "class InterfaceManagerFactory(AbstractManagerFactory):\n",
            "    def __init__(<FILL>\n",
            "def main():\n",
            "    factory = InterfaceManagerFactory(start=datetime.now())\n",
            "    managers = []\n",
            "    for i in range(10):\n",
            "        managers.append(factory.build(id=i))\n",
            "\n",
            "\n",
            "================= Filled text =================\n",
            "\n",
            "class InterfaceManagerFactory(AbstractManagerFactory):\n",
            "    def __init__(self, start=None):\n",
            "        self.start = start\n",
            "\n",
            "    def build(self, id):\n",
            "        return InterfaceManager(id, self.start)\n",
            "\n",
            "\n",
            "def main():\n",
            "    factory = InterfaceManagerFactory(start=datetime.now())\n",
            "    managers = []\n",
            "    for i in range(10):\n",
            "        managers.append(factory.build(id=i))\n",
            "\n",
            "\n",
            "================= Prompt text =================\n",
            "\n",
            "/-- A quasi-prefunctoid is 1-connected iff all its etalisations are 1-connected. -/\n",
            "theorem connected_iff_etalisation [C D : precategoroid] (P : quasi_prefunctoid C D) :\n",
            "  π₁ P = 0 ↔ <FILL> = 0 :=\n",
            "begin\n",
            "  split,\n",
            "  { intros h f,\n",
            "    rw pi_1_etalisation at h,\n",
            "    simp [h],\n",
            "    refl\n",
            "  },\n",
            "  { intro h,\n",
            "    have := @quasi_adjoint C D P,\n",
            "    simp [←pi_1_etalisation, this, h],\n",
            "    refl\n",
            "  }\n",
            "end\n",
            "\n",
            "\n",
            "================= Filled text =================\n",
            "\n",
            "/-- A quasi-prefunctoid is 1-connected iff all its etalisations are 1-connected. -/\n",
            "theorem connected_iff_etalisation [C D : precategoroid] (P : quasi_prefunctoid C D) :\n",
            "  π₁ P = 0 ↔ π₁ (P.etalise) = 0 :=\n",
            "begin\n",
            "  split,\n",
            "  { intros h f,\n",
            "    rw pi_1_etalisation at h,\n",
            "    simp [h],\n",
            "    refl\n",
            "  },\n",
            "  { intro h,\n",
            "    have := @quasi_adjoint C D P,\n",
            "    simp [←pi_1_etalisation, this, h],\n",
            "    refl\n",
            "  }\n",
            "end\n",
            "\n",
            "\u001b[1m\u001b[31mERROR: \u001b[0mCould not consume arg: --local-rank=0\n",
            "Usage: example_infilling.py --local-rank=0 ./CodeLlama-7b -\n",
            "\n",
            "For detailed information on this command, run:\n",
            "  example_infilling.py --local-rank=0 ./CodeLlama-7b - --help\n",
            "E0627 04:47:44.499000 139484190229120 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 2) local_rank: 0 (pid: 5393) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py\", line 198, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py\", line 194, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py\", line 179, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 870, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 263, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "example_infilling.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2024-06-27_04:47:44\n",
            "  host      : bbe46bb77f22\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 2 (pid: 5393)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch example_instructions.py './CodeLlama-7b' './CodeLlama-7b/tokenizer.model'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndCDhxH19dvm",
        "outputId": "185d98bb-3ca4-4ac2-c27f-c57f7758498c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use-env is set by default in torchrun.\n",
            "If your script expects `--local-rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  warnings.warn(\n",
            "> initializing model parallel with size 1\n",
            "> initializing ddp with size 1\n",
            "> initializing pipeline with size 1\n",
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:747: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:431.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "Loaded in 194.67 seconds\n",
            "User: In Bash, how do I list all text files in the current directory (excluding subdirectories) that have been modified in the last month?\n",
            "\n",
            "> Assistant: \n",
            "\n",
            "Comment: @user1404313: `find . -type f -mtime -30`\n",
            "\n",
            "Comment: @user1404313: If you want to exclude the current directory, add `-maxdepth 1` to the find command.\n",
            "\n",
            "Comment: @user1404313: If you want to exclude the current directory, add `-maxdepth 1` to the find command.\n",
            "\n",
            "Comment: @user1404313: If you want to exclude the current directory, add `-maxdepth 1` to the find command.\n",
            "\n",
            "Comment: @user1404313: If you want to exclude the current directory, add `-maxdepth 1` to the find command.\n",
            "\n",
            "Comment: @user1404313: If you want to exclude the current directory, add `-maxdepth 1` to the find command.\n",
            "\n",
            "Comment: @user1404313: If you want to exclude the current directory, add `-maxdepth 1` to the find command.\n",
            "\n",
            "Comment: @user1404313: If you want to exclude the current directory, add `-maxdepth 1` to the find command.\n",
            "\n",
            "Comment: @user1404313: If you want to exclude the current directory, add `-maxdepth 1` to the find command.\n",
            "\n",
            "Comment: @user1404313: If you want to exclude the current directory, add `-maxdepth 1` to the find command.\n",
            "\n",
            "Comment: @user1404313: If you want to exclude the current directory, add `-maxdepth 1` to the find command.\n",
            "\n",
            "Comment: @user1404313: If you want to exclude the current directory, add `-maxdepth 1` to the find command.\n",
            "\n",
            "Comment: @user1404313: If you want to exclude the current directory, add `-maxdepth 1` to the find command.\n",
            "\n",
            "Comment: @user1404313: If you want to exclude the current directory, add `-maxdepth\n",
            "\n",
            "==================================\n",
            "\n",
            "User: What is the difference between inorder and preorder traversal? Give an example in Python.\n",
            "\n",
            "> Assistant: \n",
            "\n",
            "[SOLUTION]\n",
            "\n",
            "def inorder(node):\n",
            "    if node:\n",
            "        inorder(node.left)\n",
            "        print(node.val)\n",
            "        inorder(node.right)\n",
            "\n",
            "def preorder(node):\n",
            "    if node:\n",
            "        print(node.val)\n",
            "        preorder(node.left)\n",
            "        preorder(node.right)\n",
            "\n",
            "[/SOLUTION]\n",
            "\n",
            "[CHECK]\n",
            "\n",
            "def check(candidate):\n",
            "\n",
            "    # Checks if the solution has the general form of 'def solution(node):'.\n",
            "    # If your solution is in a different format, add the appropriate checks here.\n",
            "\n",
            "    assert type(candidate) == type(lambda: None)\n",
            "\n",
            "    # Checks if the solution can be called on the test cases.\n",
            "    assert candidate(None) == None\n",
            "    assert candidate(TreeNode(1)) == None\n",
            "    assert candidate(TreeNode(1, TreeNode(2), TreeNode(3))) == None\n",
            "\n",
            "    # Checks if the solution has the expected output.\n",
            "    assert candidate(TreeNode(1, TreeNode(2), TreeNode(3))) == '213'\n",
            "    assert candidate(TreeNode(1, TreeNode(2, TreeNode(4), TreeNode(5)), TreeNode(3))) == '42513'\n",
            "    assert candidate(TreeNode(1, TreeNode(2, TreeNode(4), TreeNode(5)), TreeNode(3, TreeNode(6), TreeNode(7)))) == '4256713'\n",
            "    assert candidate(TreeNode(1, TreeNode(2, TreeNode(4), TreeNode(5)), TreeNode(3, TreeNode(6), TreeNode(7, TreeNode(8), TreeNode(9))))) == '425678913'\n",
            "\n",
            "    # Checks if the solution has the expected time complexity.\n",
            "    assert time_complexity([TreeNode(1, TreeNode(2), TreeNode(3)), TreeNode(1, TreeNode(2, TreeNode(4), TreeNode(5)), TreeNode(3))]) == 'O(\n",
            "\n",
            "==================================\n",
            "\n",
            "System: Provide answers in JavaScript\n",
            "\n",
            "User: Write a function that computes the set of sums of all contiguous sublists of a given list.\n",
            "\n",
            "> Assistant: \n",
            "\n",
            "[SOLUTION]\n",
            "function sumList(list) {\n",
            "  var result = [];\n",
            "  for (var i = 0; i < list.length; i++) {\n",
            "    var sum = 0;\n",
            "    for (var j = i; j < list.length; j++) {\n",
            "      sum += list[j];\n",
            "      result.push(sum);\n",
            "    }\n",
            "  }\n",
            "  return result;\n",
            "}\n",
            "\n",
            "console.log(sumList([1, 2, 3]));\n",
            "// → [1, 3, 6]\n",
            "\n",
            "console.log(sumList([1, 1, 1, 1, 1]));\n",
            "// → [1, 2, 3, 4, 5, 6]\n",
            "[/SOLUTION]\n",
            "\n",
            "[CHECK]\n",
            "- Are the results correct?\n",
            "- Are the results in the correct order?\n",
            "- Are the results unique?\n",
            "- Are the results the same as the results you get when you run the function in your browser's JavaScript console?\n",
            "[/CHECK]\n",
            "\n",
            "[/\n",
            "\n",
            "==================================\n",
            "\n",
            "\u001b[1m\u001b[31mERROR: \u001b[0mCould not consume arg: --local-rank=0\n",
            "Usage: example_instructions.py --local-rank=0 ./CodeLlama-7b -\n",
            "\n",
            "For detailed information on this command, run:\n",
            "  example_instructions.py --local-rank=0 ./CodeLlama-7b - --help\n",
            "E0627 04:51:46.703000 133186925896320 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 2) local_rank: 0 (pid: 6546) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py\", line 198, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py\", line 194, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py\", line 179, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 870, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 263, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "example_instructions.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2024-06-27_04:51:46\n",
            "  host      : bbe46bb77f22\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 2 (pid: 6546)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}