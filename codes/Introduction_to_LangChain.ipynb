{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hissain/mlworks/blob/main/codes/Introduction_to_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain is a framework designed to simplify the development of applications that use large language models (LLMs). It helps developers connect LLMs with external data sources, handle prompt engineering, manage memory across interactions, and create complex workflows. Here’s a beginner-level overview:\n",
        "\n",
        "### 1. **Purpose of LangChain**:\n",
        "   - LangChain helps create **end-to-end LLM-powered applications**, integrating language models with other services and data. It focuses on **chaining** different components and **orchestrating workflows** involving language models.\n",
        "\n",
        "### 2. **Key Components**:\n",
        "   - **LLMs and Prompts**: LangChain helps developers interact with LLMs like OpenAI's GPT models. It makes it easy to create, optimize, and use prompts effectively.\n",
        "   - **Chains**: Chains are sequences of calls that connect LLMs to other tools, APIs, or workflows. For example, a chain might take user input, fetch relevant data, and then format a response using an LLM.\n",
        "   - **Agents**: Agents are workflows where an LLM acts as a decision-maker, determining which actions to take in response to user queries. They use a **toolkit**, such as accessing databases, APIs, or other models.\n",
        "   - **Memory**: LangChain provides memory modules, allowing applications to **remember context** between interactions, enabling more **dynamic and interactive conversations**.\n",
        "\n",
        "### 3. **Why Use LangChain?**:\n",
        "   - **Simplifies Prompt Management**: LangChain helps manage and optimize the prompts sent to LLMs, ensuring consistent and effective communication.\n",
        "   - **Tool Integration**: It allows seamless integration of LLMs with other external services or custom tools, like web search, databases, or even external APIs.\n",
        "   - **Complex Workflow Orchestration**: It makes developing more complex workflows involving LLMs straightforward by offering different pre-built components like chains and agents.\n",
        "   - **Handling Context**: By using memory, LangChain helps to maintain the conversation’s context, which is useful in building chatbots or assistants that require multiple exchanges.\n",
        "\n",
        "### 4. **Example Use Cases**:\n",
        "   - **Chatbots**: You can use LangChain to build intelligent chatbots that maintain context and access relevant information when needed.\n",
        "   - **Question Answering Systems**: LangChain can chain LLMs with tools that fetch information from the web or databases to answer user questions accurately.\n",
        "   - **Content Generation**: You can create pipelines that generate content, perform edits, and verify facts in a single workflow."
      ],
      "metadata": {
        "id": "u0326cD973sT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUY9p47MHcZQ",
        "outputId": "8145f783-a596-4e0f-f797-d8b0ebe72c83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.6 (from langchain)\n",
            "  Downloading langchain_core-0.3.6-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.129-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.6->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (4.12.2)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain-0.3.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.6-py3-none-any.whl (399 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.129-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.2/292.2 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, h11, jsonpatch, httpcore, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.1 langchain-core-0.3.6 langchain-text-splitters-0.3.0 langsmith-0.1.129 orjson-3.10.7 tenacity-8.5.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Installation\n",
        "!pip install langchain\n",
        "!pip install -qU langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import\n",
        "import getpass\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "CC0SYrxsH0hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API Key setup\n",
        "\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQrAcz3VHuEM",
        "outputId": "adef35e3-4d17-4c30-b698-742b29c20660"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGroq(model=\"llama3-8b-8192\")"
      ],
      "metadata": {
        "id": "qzOnWtHuIA4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Let's first use the model directly.\n",
        "ChatModels are instances of LangChain \"Runnables\",\n",
        "which means they expose a standard interface for interacting with them.\n",
        "To just simply call the model, we can pass in a list of messages to the .invoke method.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Translate the following from English into Bangla\"),\n",
        "    HumanMessage(content=\"how are you?\"),\n",
        "]\n",
        "\n",
        "model.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WG_-1xNICKl",
        "outputId": "22389860-6c8a-4002-b141-eb6adb49cfb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='আপনার চলছে কেমন? (Aponar cholche kemon?)', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 27, 'total_tokens': 58, 'completion_time': 0.025833333, 'prompt_time': 0.00296822, 'queue_time': 0.017709988, 'total_time': 0.028801553}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_af05557ca2', 'finish_reason': 'stop', 'logprobs': None}, id='run-080424d4-270b-46da-aca9-df51cad4547b-0', usage_metadata={'input_tokens': 27, 'output_tokens': 31, 'total_tokens': 58})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OutputParsers\n",
        "\"\"\"\n",
        "Notice that the response from the model is an AIMessage.\n",
        "This contains a string response along with other metadata about the response.\n",
        "Oftentimes we may just want to work with the string response.\n",
        "We can parse out just this response by using a simple output parser.\n",
        "\"\"\"\n",
        "\n",
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "_f0RS9rZIhhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us use both\n",
        "\n",
        "result = model.invoke(messages)\n",
        "\n",
        "parser.invoke(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Cj8EEtvNIqEP",
        "outputId": "cf7b943e-19b3-4098-a3c3-69fbd6d70a43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'আপনার কেমন আছেন? (Aponar kemon acheen?)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chaining\n",
        "\n",
        "\"\"\"\n",
        "More commonly, we can \"chain\" the model with this output parser.\n",
        "This means this output parser will get called every time in this chain.\n",
        "This chain takes on the input type of the language model (string or list of message)\n",
        "and returns the output type of the output parser (string).\n",
        "\n",
        "We can easily create the chain using the | operator.\n",
        "The | operator is used in LangChain to combine two elements together.\n",
        "\"\"\"\n",
        "\n",
        "chain = model | parser\n",
        "\n",
        "chain.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8FVnNsZSIvV4",
        "outputId": "5971159e-329e-4aaf-b6e4-a2b9c511ae0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are looking for the translation of \"how are you?\" into Bangla, which is:\\n\\nআপনি কেমন আছেন? (Aponi kemone achhen?)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt Templates\n",
        "\"\"\"\n",
        "PromptTemplates are a concept in LangChain designed to assist with this transformation.\n",
        "They take in raw user input and return data (a prompt) that is ready to pass into a language model.\n",
        "\n",
        "Let's create a PromptTemplate here. It will take in two user variables:\n",
        "\n",
        "language: The language to translate text into\n",
        "text: The text to translate\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Translate the following into {language}:\"),\n",
        "        (\"user\", \"{text}\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "_GPUP5dBJiXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = prompt_template.invoke({\"language\": \"Bangla\", \"text\": \"how are you\"})\n",
        "\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR9kNdlFJywD",
        "outputId": "d4f4b419-e5b5-42b1-87e5-b3c4ca51a64e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Translate the following into Bangla:', additional_kwargs={}, response_metadata={}), HumanMessage(content='how are you', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We can see that it returns a ChatPromptValue that consists of two messages.\n",
        "If we want to access the messages directly we do:\n",
        "\"\"\"\n",
        "\n",
        "result.to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGsMbKMwJ8as",
        "outputId": "fb74c036-5359-4d45-ba72-436b364bb3c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Translate the following into Bangla:', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='how are you', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining All\n",
        "\"\"\"\n",
        "We can now combine this with the model and the output parser from above using the pipe (|) operator:\n",
        "\"\"\"\n",
        "chain = prompt_template | model | parser\n",
        "\n",
        "chain.invoke({\"language\": \"Bangla\", \"text\": \"how are you\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-Qbc-5ItKEmq",
        "outputId": "f8d16e03-45fb-44a0-daa0-75199a74891a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'আপনি কেমন আছেন? (Aponi kemon achhen?)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serving with LangServe\n",
        "\n",
        "# !pip install \"langserve[all]\""
      ],
      "metadata": {
        "id": "BQcdCaQZKPkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Server code\n",
        "\n",
        "#!/usr/bin/env python\n",
        "import getpass\n",
        "import os\n",
        "from fastapi import FastAPI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_groq import ChatGroq\n",
        "from langserve import add_routes\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "# 1. Create prompt template\n",
        "system_template = \"Translate the following into {language}:\"\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    ('system', system_template),\n",
        "    ('user', '{text}')\n",
        "])\n",
        "\n",
        "# 2. Create model\n",
        "model = ChatGroq(model=\"llama3-8b-8192\")\n",
        "\n",
        "# 3. Create parser\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# 4. Create chain\n",
        "chain = prompt_template | model | parser\n",
        "\n",
        "\n",
        "# 4. App definition\n",
        "app = FastAPI(\n",
        "  title=\"LangChain Server\",\n",
        "  version=\"1.0\",\n",
        "  description=\"A simple API server using LangChain's Runnable interfaces\",\n",
        ")\n",
        "\n",
        "# 5. Adding chain route\n",
        "add_routes(\n",
        "    app,\n",
        "    chain,\n",
        "    path=\"/chain\",\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "\n",
        "    uvicorn.run(app, host=\"localhost\", port=8000)"
      ],
      "metadata": {
        "id": "c-hy_X-lKRuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Commands\n",
        "# python serve.py"
      ],
      "metadata": {
        "id": "HI8N-X1oKYj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Client code\n",
        "from langserve import RemoteRunnable\n",
        "\n",
        "remote_chain = RemoteRunnable(\"http://localhost:8000/chain/\")\n",
        "remote_chain.invoke({\"language\": \"Bangla\", \"text\": \"how are you\"})"
      ],
      "metadata": {
        "id": "fuk6dMkdKcxK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}