{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hissain/mlworks/blob/main/codes/RNN_cells_in_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdowMQHxCnue"
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "%matplotlib inline\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5v1qBIWQIj9",
        "outputId": "48eb3dbc-e898-4db4-9677-4b27038b4f0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c627c3299d0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recurrent Neural Networks (RNNs)**:\n",
        "   - PyTorch provides the `torch.nn.RNN` module for implementing basic RNNs. You can specify the input size, hidden size, number of layers, and whether to use bidirectional connections.\n",
        "   - The input shape expected by `torch.nn.RNN` is `(seq_len, batch, input_size)`, where `seq_len` is the length of the input sequence, `batch` is the batch size, and `input_size` is the dimensionality of the input feature vector at each time step.\n",
        "   - RNNs process input sequences sequentially, maintaining an internal hidden state that captures information from previous time steps. You can use the output at each time step or only the final output, depending on the task."
      ],
      "metadata": {
        "id": "9o_Hh3zQ_k5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple RNN model\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])  # Use the output of the last time step\n",
        "        return out\n",
        "\n",
        "# Example usage\n",
        "input_size = 10\n",
        "hidden_size = 20\n",
        "num_layers = 2\n",
        "output_size = 1\n",
        "seq_length = 5\n",
        "batch_size = 3\n",
        "\n",
        "model = SimpleRNN(input_size, hidden_size, num_layers, output_size)\n",
        "input_tensor = torch.randn(batch_size, seq_length, input_size)\n",
        "output = model(input_tensor)\n",
        "print(output.shape)  # Output shape: (batch_size, output_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esCipWCE-L16",
        "outputId": "500134f0-9a43-43d2-df30-594b853d640f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Long Short-Term Memory networks (LSTMs)**:\n",
        "   - PyTorch offers the `torch.nn.LSTM` module for implementing LSTM networks. LSTMs are a type of RNN designed to address the vanishing gradient problem and better capture long-range dependencies in sequences.\n",
        "   - Similar to `torch.nn.RNN`, you can specify parameters such as input size, hidden size, number of layers, and bidirectionality.\n",
        "   - The input shape expected by `torch.nn.LSTM` is the same as that of `torch.nn.RNN`.\n",
        "   - LSTMs have a more complex architecture than basic RNNs, incorporating input, forget, and output gates, as well as a memory cell that can retain information over long sequences."
      ],
      "metadata": {
        "id": "fajQV2cq_uKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])  # Use the output of the last time step\n",
        "        return out\n",
        "\n",
        "# Example usage\n",
        "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
        "output = model(input_tensor)\n",
        "print(output.shape)  # Output shape: (batch_size, output_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abnLYLNl_xpG",
        "outputId": "ecd0e3a0-a883-42d8-d789-52bd7ffafefb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gated Recurrent Units (GRUs)**:\n",
        "   - PyTorch provides the `torch.nn.GRU` module for implementing GRU networks. GRUs are similar to LSTMs but have a simplified architecture, combining the forget and input gates into a single update gate.\n",
        "   - Like with `torch.nn.RNN` and `torch.nn.LSTM`, you can specify input size, hidden size, number of layers, and bidirectionality when creating a GRU layer.\n",
        "   - The input shape expected by `torch.nn.GRU` is the same as that of `torch.nn.RNN` and `torch.nn.LSTM`."
      ],
      "metadata": {
        "id": "4hO0R09d_yxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the GRU model\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = self.fc(out[:, -1, :])  # Use the output of the last time step\n",
        "        return out\n",
        "\n",
        "# Example usage\n",
        "model = GRUModel(input_size, hidden_size, num_layers, output_size)\n",
        "output = model(input_tensor)\n",
        "print(output.shape)  # Output shape: (batch_size, output_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXq-27SD_9Pk",
        "outputId": "bcbbde76-ae16-435d-a4f3-164dec5fd553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1])\n"
          ]
        }
      ]
    }
  ]
}