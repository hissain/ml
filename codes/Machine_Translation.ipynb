{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hissain/mlworks/blob/main/codes/Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfw_vcO7mArp"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhDzwNC_cSPU"
      },
      "source": [
        "This notebook is prepared from [this source](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/032d653a4f5a9c1ec32b9fc7c989ffe1/seq2seq_translation_tutorial.ipynb#scrollTo=cE7uNTa2SEpW)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5Bblp--hb6JC"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuOQIUqLcG7e"
      },
      "source": [
        "Here, two recurrent neural networks work together to transform one sequence to another. An encoder network condenses an input sequence into a vector, and a decoder network unfolds that vector into a new sequence.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/seq2seq.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mid-76LRgz1N"
      },
      "source": [
        "Here, we will be representing each word in a language as a one-hot\n",
        "vector, or giant vector of zeros except for a single one (at the index\n",
        "of the word). Compared to the dozens of characters that might exist in a\n",
        "language, there are many many more words, so the encoding vector is much\n",
        "larger. We will however cheat a bit and trim the data to only use a few\n",
        "thousand words per language.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/word-encoding.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJmW6pbGmDWo"
      },
      "source": [
        "### Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vs3BCOH6OcbJ"
      },
      "outputs": [],
      "source": [
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cldgjcm1bCfp"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVo8HmoiSFpo",
        "outputId": "ffd7a82e-691f-41bd-8853-2a82128d6fdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE0vfMK4l3GL"
      },
      "source": [
        "### Dataset creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-3d8lOcdBRF"
      },
      "source": [
        "Download the data from [here](https://drive.google.com/file/d/12eWXVWwr3XL96w-CuKlzPfuWIuW_PvGi/view?usp=sharing). This dataset is collected from [link](http://www.manythings.org/anki/ben-eng.zip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "E-qTtit_8yL8"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "w_-77bflmx_s"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "hidden_size = 256\n",
        "num_layers = 3\n",
        "batch_size = 256\n",
        "\n",
        "N_EPOCHS = 150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Ij67S9aRhQRH"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "        self.word2index = {\"SOS\":0, \"EOS\":1}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "\n",
        "        self.word2count = {}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.index2word[self.n_words] = word\n",
        "\n",
        "            self.word2count[word] = 1\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def get_tokens(self, sentences):\n",
        "        tokens = []\n",
        "        for sentence in sentences:\n",
        "            sentence_token = [EOS_token] * MAX_LENGTH\n",
        "            for i, word in enumerate(sentence.split()):\n",
        "                if i >= MAX_LENGTH:\n",
        "                    break\n",
        "                sentence_token[i] = self.word2index[word]\n",
        "            tokens.append(torch.tensor(sentence_token))\n",
        "        return torch.stack(tokens, dim=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0K6X--AMiwZc"
      },
      "outputs": [],
      "source": [
        "# Lowercase, trim, and remove non-letter characters\n",
        "def preprocess_text(s):\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"([।!?])\", r\" \\1\", s)\n",
        "    return s.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "nghizyILjHWc"
      },
      "outputs": [],
      "source": [
        "def read(lang1, lang2):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open(f'../datasets/en-bn/{lang1}-{lang2}.txt', encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[preprocess_text(s) for s in l.split('\\t')[:2]] for l in lines]\n",
        "\n",
        "    input_lang = Tokenizer(lang1)\n",
        "    output_lang = Tokenizer(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "lrP1xh_Njx7W",
        "outputId": "8692dce3-6fbf-45c9-8c68-9fc0818e5040"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 6509 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "en 3165\n",
            "bn 4471\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go .</td>\n",
              "      <td>যাও ।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Go .</td>\n",
              "      <td>যান ।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Go .</td>\n",
              "      <td>যা ।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Run  !</td>\n",
              "      <td>পালাও  !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Run  !</td>\n",
              "      <td>পালান  !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    input    output\n",
              "0    Go .     যাও ।\n",
              "1    Go .     যান ।\n",
              "2    Go .      যা ।\n",
              "3  Run  !  পালাও  !\n",
              "4  Run  !  পালান  !"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def prepare_data(lang1, lang2):\n",
        "    input_lang, output_lang, pairs = read(lang1, lang2)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.add_sentence(pair[0])\n",
        "        output_lang.add_sentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    dataset = pd.DataFrame(data=pairs, columns=['input', 'output'])\n",
        "    # dataset['output'] = dataset['output'].apply(lambda x: \"SOS \" + x + \" EOS\")\n",
        "    return input_lang, output_lang, dataset\n",
        "\n",
        "input_lang, output_lang, pairs = prepare_data('en', 'bn')\n",
        "pairs.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl9hZcc-CfoI",
        "outputId": "591be77e-4968-40e5-d878-fd82e3bec408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 10])\n",
            "tensor([[ 2,  3,  1,  1,  1,  1,  1,  1,  1,  1],\n",
            "        [ 2,  3,  1,  1,  1,  1,  1,  1,  1,  1],\n",
            "        [ 2,  3,  1,  1,  1,  1,  1,  1,  1,  1],\n",
            "        [ 4,  6,  1,  1,  1,  1,  1,  1,  1,  1],\n",
            "        [ 4,  6,  1,  1,  1,  1,  1,  1,  1,  1],\n",
            "        [ 7,  8,  1,  1,  1,  1,  1,  1,  1,  1],\n",
            "        [ 9,  6,  1,  1,  1,  1,  1,  1,  1,  1],\n",
            "        [10,  6,  1,  1,  1,  1,  1,  1,  1,  1]])\n"
          ]
        }
      ],
      "source": [
        "inputs = ['Go .', 'Go .', 'Go .', 'Run  !', 'Run  !', 'Who  ?', 'Wow  !', 'Fire  !']\n",
        "inputs = input_lang.get_tokens(inputs)\n",
        "print(inputs.size())\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm9gfLX1l0rF"
      },
      "source": [
        "### Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "a0em7VJ8PdSK"
      },
      "outputs": [],
      "source": [
        "# Define the Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, input_seqs, hidden):\n",
        "        # print(input_seqs.size())\n",
        "        embedded = self.embedding(input_seqs)\n",
        "        # print(embedded.size())\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "teBLDSJNmpfH"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, num_layers):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs, decoder_hidden\n",
        "\n",
        "    def forward_step(self, input, hidden):\n",
        "        output = F.relu(self.embedding(input))\n",
        "        output, hidden = self.rnn(output, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "WVANMpxt55Ct"
      },
      "outputs": [],
      "source": [
        "# Define the Seq2Seq Model\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seqs, target_seqs=None):\n",
        "        batch_size = input_seqs.size(0)\n",
        "        target_vocab_size = self.decoder.out.out_features\n",
        "\n",
        "        encoder_hidden = self.encoder.init_hidden(batch_size)\n",
        "        encoder_output, encoder_hidden = self.encoder(input_seqs, encoder_hidden)\n",
        "\n",
        "        decoder_output, decoder_hidden = self.decoder(\n",
        "            encoder_output,\n",
        "            encoder_hidden,\n",
        "            target_seqs\n",
        "        )\n",
        "\n",
        "        return decoder_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Pdgk7EpnPvxv"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "input_size = input_lang.n_words\n",
        "output_size = output_lang.n_words\n",
        "\n",
        "encoder = Encoder(input_size, hidden_size, num_layers)\n",
        "decoder = Decoder(hidden_size, output_size, num_layers)\n",
        "\n",
        "# Move model to device\n",
        "model = Seq2Seq(encoder, decoder).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "HPLFoHduPySS"
      },
      "outputs": [],
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5_ky7W5KQKq",
        "outputId": "facef7ab-1e4c-45d7-8c73-21751edd43f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Train Loss: 806441.037\n",
            "Epoch: 02 | Train Loss: 771490.146\n",
            "Epoch: 03 | Train Loss: 762700.184\n",
            "Epoch: 04 | Train Loss: 737697.350\n",
            "Epoch: 05 | Train Loss: 736923.672\n",
            "Epoch: 06 | Train Loss: 731633.666\n",
            "Epoch: 07 | Train Loss: 724998.471\n",
            "Epoch: 08 | Train Loss: 712320.208\n",
            "Epoch: 09 | Train Loss: 704486.602\n",
            "Epoch: 10 | Train Loss: 693964.934\n",
            "Epoch: 11 | Train Loss: 684892.390\n",
            "Epoch: 12 | Train Loss: 681904.144\n",
            "Epoch: 13 | Train Loss: 674734.675\n",
            "Epoch: 14 | Train Loss: 668384.838\n",
            "Epoch: 15 | Train Loss: 663065.026\n",
            "Epoch: 16 | Train Loss: 658551.104\n",
            "Epoch: 17 | Train Loss: 653281.809\n",
            "Epoch: 18 | Train Loss: 650563.725\n",
            "Epoch: 19 | Train Loss: 647577.294\n",
            "Epoch: 20 | Train Loss: 645539.924\n",
            "Epoch: 21 | Train Loss: 642609.330\n",
            "Epoch: 22 | Train Loss: 647308.371\n",
            "Epoch: 23 | Train Loss: 643341.818\n",
            "Epoch: 24 | Train Loss: 637528.523\n",
            "Epoch: 25 | Train Loss: 631946.917\n",
            "Epoch: 26 | Train Loss: 626064.383\n",
            "Epoch: 27 | Train Loss: 620745.490\n",
            "Epoch: 28 | Train Loss: 614416.796\n",
            "Epoch: 29 | Train Loss: 609574.415\n",
            "Epoch: 30 | Train Loss: 606975.144\n",
            "Epoch: 31 | Train Loss: 605007.350\n",
            "Epoch: 32 | Train Loss: 600972.887\n",
            "Epoch: 33 | Train Loss: 598460.756\n",
            "Epoch: 34 | Train Loss: 598981.366\n",
            "Epoch: 35 | Train Loss: 599141.599\n",
            "Epoch: 36 | Train Loss: 608827.678\n",
            "Epoch: 37 | Train Loss: 614976.791\n",
            "Epoch: 38 | Train Loss: 598534.859\n",
            "Epoch: 39 | Train Loss: 591385.421\n",
            "Epoch: 40 | Train Loss: 586170.883\n",
            "Epoch: 41 | Train Loss: 581214.780\n",
            "Epoch: 42 | Train Loss: 579237.484\n",
            "Epoch: 43 | Train Loss: 575437.810\n",
            "Epoch: 44 | Train Loss: 573602.743\n",
            "Epoch: 45 | Train Loss: 571473.661\n",
            "Epoch: 46 | Train Loss: 568163.768\n",
            "Epoch: 47 | Train Loss: 566766.820\n",
            "Epoch: 48 | Train Loss: 563905.059\n",
            "Epoch: 49 | Train Loss: 561583.172\n",
            "Epoch: 50 | Train Loss: 560272.512\n",
            "Epoch: 51 | Train Loss: 559174.746\n",
            "Epoch: 52 | Train Loss: 558956.633\n",
            "Epoch: 53 | Train Loss: 559322.672\n",
            "Epoch: 54 | Train Loss: 556856.323\n",
            "Epoch: 55 | Train Loss: 556509.123\n",
            "Epoch: 56 | Train Loss: 557114.687\n",
            "Epoch: 57 | Train Loss: 557048.652\n",
            "Epoch: 58 | Train Loss: 560021.301\n",
            "Epoch: 59 | Train Loss: 560660.496\n",
            "Epoch: 60 | Train Loss: 563867.814\n",
            "Epoch: 61 | Train Loss: 561135.102\n",
            "Epoch: 62 | Train Loss: 558862.487\n",
            "Epoch: 63 | Train Loss: 557711.154\n",
            "Epoch: 64 | Train Loss: 555482.421\n",
            "Epoch: 65 | Train Loss: 551271.993\n",
            "Epoch: 66 | Train Loss: 550660.807\n",
            "Epoch: 67 | Train Loss: 550514.922\n",
            "Epoch: 68 | Train Loss: 546659.184\n",
            "Epoch: 69 | Train Loss: 544570.942\n",
            "Epoch: 70 | Train Loss: 543231.426\n",
            "Epoch: 71 | Train Loss: 540557.194\n",
            "Epoch: 72 | Train Loss: 539693.430\n",
            "Epoch: 73 | Train Loss: 538328.443\n",
            "Epoch: 74 | Train Loss: 537287.570\n",
            "Epoch: 75 | Train Loss: 535507.615\n",
            "Epoch: 76 | Train Loss: 534078.258\n",
            "Epoch: 77 | Train Loss: 533188.004\n",
            "Epoch: 78 | Train Loss: 532230.536\n",
            "Epoch: 79 | Train Loss: 530691.550\n",
            "Epoch: 80 | Train Loss: 529533.370\n",
            "Epoch: 81 | Train Loss: 528139.051\n",
            "Epoch: 82 | Train Loss: 527040.996\n",
            "Epoch: 83 | Train Loss: 526617.699\n",
            "Epoch: 84 | Train Loss: 525287.772\n",
            "Epoch: 85 | Train Loss: 524270.410\n",
            "Epoch: 86 | Train Loss: 523687.197\n",
            "Epoch: 87 | Train Loss: 522671.260\n",
            "Epoch: 88 | Train Loss: 522010.015\n",
            "Epoch: 89 | Train Loss: 521398.346\n",
            "Epoch: 90 | Train Loss: 520713.228\n",
            "Epoch: 91 | Train Loss: 520392.296\n",
            "Epoch: 92 | Train Loss: 519882.059\n",
            "Epoch: 93 | Train Loss: 519206.059\n",
            "Epoch: 94 | Train Loss: 519045.521\n",
            "Epoch: 95 | Train Loss: 518843.366\n",
            "Epoch: 96 | Train Loss: 518190.534\n",
            "Epoch: 97 | Train Loss: 518107.044\n",
            "Epoch: 98 | Train Loss: 517651.282\n",
            "Epoch: 99 | Train Loss: 517438.925\n",
            "Epoch: 100 | Train Loss: 517257.969\n",
            "Epoch: 101 | Train Loss: 516765.130\n",
            "Epoch: 102 | Train Loss: 516603.025\n",
            "Epoch: 103 | Train Loss: 516335.199\n",
            "Epoch: 104 | Train Loss: 516292.735\n",
            "Epoch: 105 | Train Loss: 516052.706\n",
            "Epoch: 106 | Train Loss: 516024.688\n",
            "Epoch: 107 | Train Loss: 515873.519\n",
            "Epoch: 108 | Train Loss: 515859.535\n",
            "Epoch: 109 | Train Loss: 515789.516\n",
            "Epoch: 110 | Train Loss: 515723.646\n",
            "Epoch: 111 | Train Loss: 515620.023\n",
            "Epoch: 112 | Train Loss: 515571.868\n",
            "Epoch: 113 | Train Loss: 515469.017\n",
            "Epoch: 114 | Train Loss: 515381.096\n",
            "Epoch: 115 | Train Loss: 515335.674\n",
            "Epoch: 116 | Train Loss: 515319.227\n",
            "Epoch: 117 | Train Loss: 515355.250\n",
            "Epoch: 118 | Train Loss: 515214.144\n",
            "Epoch: 119 | Train Loss: 515230.981\n",
            "Epoch: 120 | Train Loss: 515138.643\n",
            "Epoch: 121 | Train Loss: 515163.623\n",
            "Epoch: 122 | Train Loss: 515190.238\n",
            "Epoch: 123 | Train Loss: 514974.241\n",
            "Epoch: 124 | Train Loss: 515073.737\n",
            "Epoch: 125 | Train Loss: 514879.152\n",
            "Epoch: 126 | Train Loss: 514891.084\n",
            "Epoch: 127 | Train Loss: 514839.670\n",
            "Epoch: 128 | Train Loss: 514916.348\n",
            "Epoch: 129 | Train Loss: 514769.189\n",
            "Epoch: 130 | Train Loss: 514826.603\n",
            "Epoch: 131 | Train Loss: 514676.004\n",
            "Epoch: 132 | Train Loss: 514796.115\n",
            "Epoch: 133 | Train Loss: 514721.369\n",
            "Epoch: 134 | Train Loss: 514800.700\n",
            "Epoch: 135 | Train Loss: 514836.661\n",
            "Epoch: 136 | Train Loss: 514741.961\n",
            "Epoch: 137 | Train Loss: 514767.486\n",
            "Epoch: 138 | Train Loss: 514623.571\n",
            "Epoch: 139 | Train Loss: 514617.172\n",
            "Epoch: 140 | Train Loss: 514483.697\n",
            "Epoch: 141 | Train Loss: 514548.751\n",
            "Epoch: 142 | Train Loss: 514476.102\n",
            "Epoch: 143 | Train Loss: 514475.869\n",
            "Epoch: 144 | Train Loss: 514474.338\n",
            "Epoch: 145 | Train Loss: 514450.706\n",
            "Epoch: 146 | Train Loss: 514360.637\n",
            "Epoch: 147 | Train Loss: 514324.800\n",
            "Epoch: 148 | Train Loss: 514405.023\n",
            "Epoch: 149 | Train Loss: 514264.825\n",
            "Epoch: 150 | Train Loss: 514270.854\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i in range(len(pairs) // batch_size + 1):\n",
        "        inputs = pairs.iloc[(i)*batch_size:(i+1)*batch_size, 0]\n",
        "        targets = pairs.iloc[(i)*batch_size:(i+1)*batch_size, 1]\n",
        "\n",
        "        if len(inputs) == 0:\n",
        "            break\n",
        "        # print(inputs.tolist(), targets.tolist())\n",
        "        # inputs = [input_lang.get_one_hot_sentence(input.split()) for input in inputs]\n",
        "        # inputs = torch.row_stack(inputs)\n",
        "        inputs = input_lang.get_tokens(inputs.tolist())\n",
        "        targets = output_lang.get_tokens(targets.tolist())\n",
        "\n",
        "        # print(inputs.view(-1).size())\n",
        "        # print(targets.size())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(inputs, targets)\n",
        "        # predictions = torch.argmax(predictions, dim=2)\n",
        "\n",
        "        # print(predictions.size(), targets.size())\n",
        "        target_one_hot = F.one_hot(targets, output_lang.n_words)\n",
        "\n",
        "        loss = criterion(predictions.float().flatten(), target_one_hot.float().flatten())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f'Epoch: {epoch+1:02} | Train Loss: {epoch_loss:.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "LgBfvrQPG0id"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' কেউ কাজ শুরু করে ।'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def evaluate(model, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        inputs = input_lang.get_tokens([sentence])\n",
        "        predictions = model(inputs)\n",
        "        predictions = torch.argmax(predictions, dim=2)[0]\n",
        "\n",
        "        output = \"\"\n",
        "        for prediction in predictions:\n",
        "            word = output_lang.index2word[prediction.item()]\n",
        "            if word == \"EOS\":\n",
        "                break\n",
        "            output += \" \" + word\n",
        "        return output\n",
        "\n",
        "\n",
        "evaluate(model, \"help .\", input_lang, output_lang)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuAgCfWhZoSR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "MzbbXSV3QGWq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" full guys She's She's She's midnight .\""
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(model, \"what do you do .\", input_lang, input_lang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' তুমি কি ?'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(model, \"কেমন আছেন ?\", output_lang, output_lang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " তুমি বলেন কথা করো ।\n",
            " ওটা গুরুত্বপূর্ণ ভুল ।\n"
          ]
        }
      ],
      "source": [
        "print(evaluate(model, \"are you kidding .\", input_lang, output_lang))\n",
        "print(evaluate(model, \"Queen is nobody .\", input_lang, output_lang))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
