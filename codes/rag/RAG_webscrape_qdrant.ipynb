{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560667b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import faiss\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium_stealth import stealth\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urljoin\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque\n",
    "\n",
    "root = 'https://github.com'\n",
    "base_url = 'https://github.com/hissain'\n",
    "\n",
    "ollama_url_inf = \"http://localhost:11434/api/show\"\n",
    "ollama_url_emb = \"http://localhost:11434/api/embeddings\"\n",
    "ollama_url_gen = \"http://localhost:11434/api/generate\"\n",
    "ollama_model_name = \"llama3.2:latest\"\n",
    "\n",
    "VERBOSE = True\n",
    "DEPTH = 1\n",
    "MAX_LEN = 2\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n",
    "]\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--enable-javascript')\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument(f\"user-agent={random.choice(USER_AGENTS)}\")\n",
    "\n",
    "service = Service()\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def apply_stealth(driver):\n",
    "    stealth(driver, languages=[\"en-US\", \"en\"], vendor=\"Google Inc.\", platform=\"Win32\", \n",
    "        webgl_vendor=\"Intel Inc.\", renderer=\"Intel Iris OpenGL Engine\", fix_hairline=True,\n",
    "        run_on_insecure_content=True, fake_media_devices=True)\n",
    "\n",
    "apply_stealth(driver)\n",
    "\n",
    "def extract_clean_text_from_html(html_content):\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Remove unwanted tags\n",
    "    for script in soup([\"script\", \"style\", \"footer\", \"header\", \"nav\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # Get clean text\n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def scrape_recursive(base_url, depth=DEPTH, maxLen=MAX_LEN):\n",
    "\n",
    "    visited_urls = set()\n",
    "    text_data = []\n",
    "    queue = deque([(base_url, 0)])  # (url, current_depth)\n",
    "    \n",
    "    while queue:\n",
    "        url, current_depth = queue.popleft()\n",
    "        if len(visited_urls) >= maxLen:\n",
    "            continue\n",
    "        if url in visited_urls or current_depth > depth:\n",
    "            continue\n",
    "        if not url.startswith(root):\n",
    "            continue\n",
    "        if '#' in url:\n",
    "            continue\n",
    "        print(\"Processing: \", url)\n",
    "        \n",
    "        visited_urls.add(url)\n",
    "        \n",
    "        try:\n",
    "            apply_stealth(driver)\n",
    "            driver.get(url)\n",
    "            time.sleep(1)\n",
    "            \n",
    "            page_text = extract_clean_text_from_html(driver.page_source)\n",
    "            text_data.append((url, page_text))\n",
    "            \n",
    "            if current_depth < depth:\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                links = soup.find_all('a', href=True)\n",
    "                \n",
    "                for link in links:\n",
    "                    new_url = urljoin(base_url, link['href'])\n",
    "                    if new_url not in visited_urls and new_url.startswith(root):\n",
    "                        queue.append((new_url, current_depth + 1))\n",
    "        \n",
    "        except Exception as e:\n",
    "            if VERBOSE:\n",
    "                print(f\"Error with URL {url}: {e}\")\n",
    "    \n",
    "    return text_data\n",
    "\n",
    "try:\n",
    "    scraped_data = scrape_recursive(base_url)\n",
    "finally:\n",
    "    #driver.quit()\n",
    "    print(\"Finished!\")\n",
    "\n",
    "print(\"Total url pased: \", len(scraped_data))\n",
    "\n",
    "for url, text in scraped_data:\n",
    "    print(f\"URL: {url}\\ncontent size: {len(text)}\\nText: {text[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e81ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = '\\n'.join([s for _, s in scraped_data])\n",
    "print(len(all_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79ce979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "import aiohttp\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import PointStruct, Filter\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "qdrant_client = QdrantClient(\"http://localhost:6333\")\n",
    "collection_name = \"rag_collection\"\n",
    "\n",
    "def partition_text(text, max_length):\n",
    "    sentences = text.split('. ')\n",
    "    partitions = []\n",
    "    current_part = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in tqdm(sentences, desc=\"Partitioning text\"):\n",
    "        sentence_length = len(sentence.split())\n",
    "        \n",
    "        if current_length + sentence_length > max_length:\n",
    "            partitions.append('. '.join(current_part))\n",
    "            current_part = []\n",
    "            current_length = 0\n",
    "        \n",
    "        current_part.append(sentence)\n",
    "        current_length += sentence_length\n",
    "    \n",
    "    if current_part:\n",
    "        partitions.append('. '.join(current_part))\n",
    "\n",
    "    return partitions\n",
    "\n",
    "async def get_embedding_shape():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        payload = {\"model\": ollama_model_name}\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        async with session.post(ollama_url_inf, headers=headers, data=json.dumps(payload)) as response:\n",
    "            if response.status == 200:\n",
    "                result = await response.json()\n",
    "                return result['model_info'][\"llama.embedding_length\"]\n",
    "            else:\n",
    "                print(f\"ERROR: Error from Ollama: {response.status}\")\n",
    "                return 0\n",
    "\n",
    "async def get_embeddings(partitions):\n",
    "    embeddings = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for partition in tqdm(partitions, desc=\"Fetching embeddings\"):\n",
    "            payload = {\"model\": ollama_model_name, \"prompt\": partition}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            async with session.post(ollama_url_emb, headers=headers, data=json.dumps(payload)) as response:\n",
    "                if response.status == 200:\n",
    "                    result = await response.json()\n",
    "                    embeddings.append(np.array(result.get('embedding', np.zeros(768))))\n",
    "                else:\n",
    "                    print(f\"ERROR: Error from Ollama: {response.status}\")\n",
    "                    embeddings.append(np.zeros(768))\n",
    "    return embeddings\n",
    "\n",
    "async def store_in_qdrant(partitions):\n",
    "    dimension = await get_embedding_shape()\n",
    "    if VERBOSE:\n",
    "        print(f\"Model's embedding dimension: {dimension}\")\n",
    "\n",
    "    qdrant_client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config={\"size\": dimension, \"distance\": \"Cosine\"},\n",
    "    )\n",
    "\n",
    "    embeddings = await get_embeddings(partitions)\n",
    "    points = [\n",
    "        PointStruct(id=i, vector=embedding.tolist(), payload={\"text\": partition})\n",
    "        for i, (embedding, partition) in enumerate(zip(embeddings, partitions))\n",
    "    ]\n",
    "    qdrant_client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "async def retrieve_with_rag(query, partitions, k=5):\n",
    "    query_embedding = (await get_embeddings([query]))[0]\n",
    "    search_result = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding.tolist(),\n",
    "        limit=k,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "    retrieved_docs = [hit.payload[\"text\"] for hit in search_result]\n",
    "    combined_docs = \"\\n\".join(retrieved_docs)\n",
    "    \n",
    "    inst = \"Instruction: If you do not find the answer in the context, just say you don't know.\"\n",
    "    rag_prompt = f\"{inst}\\n\\nContext:\\n{combined_docs}\\n\\nQuery: {query}\\nAnswer:\"\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        payload = {\"model\": ollama_model_name, \"prompt\": rag_prompt, \"stream\": False}\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        async with session.post(ollama_url_gen, headers=headers, data=json.dumps(payload)) as response:\n",
    "            return await response.json()\n",
    "\n",
    "async def ask(query):\n",
    "    rag_response = await retrieve_with_rag(query, partitions)\n",
    "    return rag_response.get(\"response\", \"No response available\")\n",
    "\n",
    "partitions = partition_text(all_text, max_length=512)\n",
    "\n",
    "if VERBOSE:\n",
    "    print(f'Total partition count: {len(partitions)}')\n",
    "\n",
    "await store_in_qdrant(partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b423f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install qdrant_client"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
