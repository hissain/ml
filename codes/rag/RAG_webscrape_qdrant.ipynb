{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "560667b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  https://github.com/hissain\n",
      "Processing:  https://github.com/\n",
      "Finished!\n",
      "Total url pased:  2\n",
      "URL: https://github.com/hissain\n",
      "content size: 17221\n",
      "Text: hissain (Md. Sazzad Hissain Khan) · GitHub Skip to content You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your ses...\n",
      "\n",
      "URL: https://github.com/\n",
      "content size: 3958\n",
      "Text: GitHub: Let’s build from here · GitHub Skip to content Just a few days left to join us at Universe ‘24! Get free virtual tickets today. You signed in with another tab or window. Reload to refresh your...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import faiss\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium_stealth import stealth\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urljoin\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque\n",
    "\n",
    "root = 'https://github.com'\n",
    "base_url = 'https://github.com/hissain'\n",
    "\n",
    "ollama_url_inf = \"http://localhost:11434/api/show\"\n",
    "ollama_url_emb = \"http://localhost:11434/api/embeddings\"\n",
    "ollama_url_gen = \"http://localhost:11434/api/generate\"\n",
    "ollama_model_name = \"llama3.2:latest\"\n",
    "\n",
    "VERBOSE = True\n",
    "DEPTH = 1\n",
    "MAX_LEN = 2\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n",
    "]\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--enable-javascript')\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument(f\"user-agent={random.choice(USER_AGENTS)}\")\n",
    "\n",
    "service = Service()\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def apply_stealth(driver):\n",
    "    stealth(driver, languages=[\"en-US\", \"en\"], vendor=\"Google Inc.\", platform=\"Win32\", \n",
    "        webgl_vendor=\"Intel Inc.\", renderer=\"Intel Iris OpenGL Engine\", fix_hairline=True,\n",
    "        run_on_insecure_content=True, fake_media_devices=True)\n",
    "\n",
    "apply_stealth(driver)\n",
    "\n",
    "def extract_clean_text_from_html(html_content):\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Remove unwanted tags\n",
    "    for script in soup([\"script\", \"style\", \"footer\", \"header\", \"nav\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # Get clean text\n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def scrape_recursive(base_url, depth=DEPTH, maxLen=MAX_LEN):\n",
    "\n",
    "    visited_urls = set()\n",
    "    text_data = []\n",
    "    queue = deque([(base_url, 0)])  # (url, current_depth)\n",
    "    \n",
    "    while queue:\n",
    "        url, current_depth = queue.popleft()\n",
    "        if len(visited_urls) >= maxLen:\n",
    "            continue\n",
    "        if url in visited_urls or current_depth > depth:\n",
    "            continue\n",
    "        if not url.startswith(root):\n",
    "            continue\n",
    "        if '#' in url:\n",
    "            continue\n",
    "        print(\"Processing: \", url)\n",
    "        \n",
    "        visited_urls.add(url)\n",
    "        \n",
    "        try:\n",
    "            apply_stealth(driver)\n",
    "            driver.get(url)\n",
    "            time.sleep(1)\n",
    "            \n",
    "            page_text = extract_clean_text_from_html(driver.page_source)\n",
    "            text_data.append((url, page_text))\n",
    "            \n",
    "            if current_depth < depth:\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                links = soup.find_all('a', href=True)\n",
    "                \n",
    "                for link in links:\n",
    "                    new_url = urljoin(base_url, link['href'])\n",
    "                    if new_url not in visited_urls and new_url.startswith(root):\n",
    "                        queue.append((new_url, current_depth + 1))\n",
    "        \n",
    "        except Exception as e:\n",
    "            if VERBOSE:\n",
    "                print(f\"Error with URL {url}: {e}\")\n",
    "    \n",
    "    return text_data\n",
    "\n",
    "try:\n",
    "    scraped_data = scrape_recursive(base_url)\n",
    "finally:\n",
    "    #driver.quit()\n",
    "    print(\"Finished!\")\n",
    "\n",
    "print(\"Total url pased: \", len(scraped_data))\n",
    "\n",
    "for url, text in scraped_data:\n",
    "    print(f\"URL: {url}\\ncontent size: {len(text)}\\nText: {text[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e81ca9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21180\n"
     ]
    }
   ],
   "source": [
    "all_text = '\\n'.join([s for _, s in scraped_data])\n",
    "print(len(all_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f79ce979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a934d899a5cd4368ad8963ccdcce95f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Partitioning text:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partition count: 1\n",
      "Model's embedding dimension: 3072\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c152710e2a48adbe6a27ed0cfffeb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings len: 1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import aiohttp\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "qdrant_url = \"http://localhost:6333\"\n",
    "collection_name = \"github_collection\"\n",
    "VERBOSE = True  # Set to True for debug prints\n",
    "\n",
    "def partition_text(text, max_length):\n",
    "    sentences = text.split('. ')\n",
    "    partitions, current_part, current_length = [], [], 0\n",
    "\n",
    "    for sentence in tqdm(sentences, desc=\"Partitioning text\"):\n",
    "        sentence_length = len(sentence.split())\n",
    "        if current_length + sentence_length > max_length:\n",
    "            partitions.append('. '.join(current_part))\n",
    "            current_part, current_length = [], 0\n",
    "        current_part.append(sentence)\n",
    "        current_length += sentence_length\n",
    "\n",
    "    if current_part:\n",
    "        partitions.append('. '.join(current_part))\n",
    "    return partitions\n",
    "\n",
    "async def get_embedding_shape():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        payload = {\"model\": ollama_model_name}\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        async with session.post(ollama_url_inf, headers=headers, data=json.dumps(payload)) as response:\n",
    "            if response.status == 200:\n",
    "                result = await response.json()\n",
    "                return result['model_info'][\"llama.embedding_length\"]\n",
    "            else:\n",
    "                print(f\"ERROR: Error from Ollama: {response.status}\")\n",
    "                return 0\n",
    "\n",
    "async def get_embeddings(partitions):\n",
    "    embeddings = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for partition in tqdm(partitions, desc=\"Fetching embeddings\"):\n",
    "            payload = {\"model\": ollama_model_name, \"prompt\": partition}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            async with session.post(ollama_url_emb, headers=headers, data=json.dumps(payload)) as response:\n",
    "                if response.status == 200:\n",
    "                    result = await response.json()\n",
    "                    embeddings.append(np.array(result.get('embedding', np.zeros(768))))\n",
    "                else:\n",
    "                    print(f\"ERROR: Error from Ollama: {response.status}\")\n",
    "                    embeddings.append(np.zeros(768))\n",
    "    return embeddings\n",
    "\n",
    "async def create_collection_rest(dimension):\n",
    "    url = f\"{qdrant_url}/collections/{collection_name}\"\n",
    "    payload = {\n",
    "        \"vectors\": {\n",
    "            \"text\": {\n",
    "                \"size\": dimension,\n",
    "                \"distance\": \"Cosine\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Delete existing collection if exists\n",
    "        async with session.delete(url) as response:\n",
    "            if response.status not in [200, 404]:  # Ignore 404 as it means collection does not exist\n",
    "                print(f\"Error deleting collection: {response.status}\")\n",
    "        \n",
    "        # Create new collection\n",
    "        async with session.put(url, json=payload) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Error creating collection: {response.status}\")\n",
    "            return response.status\n",
    "\n",
    "async def upsert_points_rest(embeddings, partitions):\n",
    "    url = f\"{qdrant_url}/collections/{collection_name}/points\"\n",
    "    points = [\n",
    "        {\"id\": i, \"vector\": embedding.tolist(), \"payload\": {\"text\": partition}}\n",
    "        for i, (embedding, partition) in enumerate(zip(embeddings, partitions))\n",
    "    ]\n",
    "    payload = {\"points\": points}\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.put(url, json=payload) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Error upserting points: {response.status}\")\n",
    "            return response.status\n",
    "\n",
    "async def store_in_qdrant(partitions):\n",
    "    dimension = await get_embedding_shape()\n",
    "    if VERBOSE:\n",
    "        print(f\"Model's embedding dimension: {dimension}\")\n",
    "\n",
    "    await create_collection_rest(dimension)\n",
    "    embeddings = await get_embeddings(partitions)\n",
    "    print(\"Embeddings len:\", len(embeddings))\n",
    "    await upsert_points_rest(embeddings, partitions)\n",
    "\n",
    "async def search_points_rest(query_embedding, k=5):\n",
    "    url = f\"{qdrant_url}/collections/{collection_name}/points/search\"\n",
    "    payload = {\n",
    "        \"vector\": query_embedding.tolist(),\n",
    "        \"limit\": k,\n",
    "        \"with_payload\": True\n",
    "    }\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(url, json=payload) as response:\n",
    "            if response.status == 200:\n",
    "                result = await response.json()\n",
    "                return result[\"result\"]\n",
    "            else:\n",
    "                print(f\"Error in search query: {response.status}\")\n",
    "                return []\n",
    "\n",
    "async def retrieve_with_rag(query, k=5):\n",
    "    query_embedding = (await get_embeddings([query]))[0]\n",
    "    search_result = await search_points_rest(query_embedding, k)\n",
    "\n",
    "    retrieved_docs = [hit[\"payload\"][\"text\"] for hit in search_result]\n",
    "    combined_docs = \"\\n\".join(retrieved_docs)\n",
    "    \n",
    "    inst = \"Instruction: If you do not find the answer in the context, just say you don't know.\"\n",
    "    rag_prompt = f\"{inst}\\n\\nContext:\\n{combined_docs}\\n\\nQuery: {query}\\nAnswer:\"\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        payload = {\"model\": ollama_model_name, \"prompt\": rag_prompt, \"stream\": False}\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        async with session.post(ollama_url_gen, headers=headers, data=json.dumps(payload)) as response:\n",
    "            return await response.json()\n",
    "\n",
    "async def ask(query):\n",
    "    rag_response = await retrieve_with_rag(query)\n",
    "    return rag_response.get(\"response\", \"No response available\")\n",
    "\n",
    "partitions = partition_text(all_text[:2000], max_length=512)\n",
    "\n",
    "if VERBOSE:\n",
    "    print(f'Total partition count: {len(partitions)}')\n",
    "\n",
    "try:\n",
    "    await store_in_qdrant(partitions)\n",
    "except Exception as e:\n",
    "    print(f\"Error storing in Qdrant: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eb117848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d544be4cc349879e446f1cebb5a71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in search query: 400\n",
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "res = await ask(\"Whats Hissains special interest?\")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
