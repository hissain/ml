{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b95c0113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  https://github.com/hissain\n",
      "Processing:  https://github.com/\n",
      "Finished!\n",
      "Total url pased:  2\n",
      "24649\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium_stealth import stealth\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urljoin\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque\n",
    "\n",
    "root = 'https://github.com'\n",
    "base_url = 'https://github.com/hissain'\n",
    "\n",
    "ollama_url_inf = \"http://localhost:11434/api/show\"\n",
    "ollama_url_emb = \"http://localhost:11434/api/embeddings\"\n",
    "ollama_url_gen = \"http://localhost:11434/api/generate\"\n",
    "ollama_model_name = \"llama3.2:latest\"\n",
    "\n",
    "VERBOSE = True\n",
    "DEPTH = 1\n",
    "MAX_LEN = 2\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--enable-javascript')\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "\n",
    "service = Service()\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def apply_stealth(driver):\n",
    "    stealth(driver, languages=[\"en-US\", \"en\"], vendor=\"Google Inc.\", platform=\"Win32\", \n",
    "        webgl_vendor=\"Intel Inc.\", renderer=\"Intel Iris OpenGL Engine\", fix_hairline=True,\n",
    "        run_on_insecure_content=True, fake_media_devices=True)\n",
    "\n",
    "apply_stealth(driver)\n",
    "\n",
    "def extract_clean_text_from_html(html_content):\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Remove unwanted tags\n",
    "    for script in soup([\"script\", \"style\", \"footer\", \"header\", \"nav\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # Get clean text\n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def scrape_recursive(base_url, depth=DEPTH, maxLen=MAX_LEN):\n",
    "\n",
    "    visited_urls = set()\n",
    "    text_data = []\n",
    "    queue = deque([(base_url, 0)])  # (url, current_depth)\n",
    "    \n",
    "    while queue and len(visited_urls) < maxLen:\n",
    "        \n",
    "        url, current_depth = queue.popleft()\n",
    "        if url in visited_urls or current_depth > depth:\n",
    "            continue\n",
    "        if not url.startswith(root):\n",
    "            continue\n",
    "        if '#' in url:\n",
    "            continue\n",
    "        print(\"Processing: \", url)\n",
    "        \n",
    "        visited_urls.add(url)\n",
    "        \n",
    "        try:\n",
    "            #apply_stealth(driver)\n",
    "            driver.get(url)\n",
    "            time.sleep(1)\n",
    "            \n",
    "            page_text = extract_clean_text_from_html(driver.page_source)\n",
    "            text_data.append((url, page_text))\n",
    "            \n",
    "            if current_depth < depth:\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                links = soup.find_all('a', href=True)\n",
    "                \n",
    "                for link in links:\n",
    "                    new_url = urljoin(base_url, link['href'])\n",
    "                    if new_url not in visited_urls and new_url.startswith(root):\n",
    "                        queue.append((new_url, current_depth + 1))\n",
    "        \n",
    "        except Exception as e:\n",
    "            if VERBOSE:\n",
    "                print(f\"Error with URL {url}: {e}\")\n",
    "    \n",
    "    return text_data\n",
    "\n",
    "try:\n",
    "    scraped_data = scrape_recursive(base_url)\n",
    "finally:\n",
    "    #driver.quit()\n",
    "    print(\"Finished!\")\n",
    "\n",
    "#scraped_data = []\n",
    "\n",
    "print(\"Total url pased: \", len(scraped_data))\n",
    "all_text = '\\n'.join([s for _, s in scraped_data])\n",
    "print(len(all_text))\n",
    "\n",
    "#for url, text in scraped_data:\n",
    "#    print(f\"URL: {url}\\ncontent size: {len(text)}\\nText: {text[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5868f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc0825e5e37492fbef813cb5553a8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Partitioning text:   0%|          | 0/466 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partition count: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fb9638a3ad48a9bdd8cda710cb2cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings len: 8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from qdrant_client import QdrantClient, models\n",
    "from tqdm.notebook import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "qdrant_url = \"http://localhost:6333\"\n",
    "collection_name = \"github_collection\"\n",
    "VERBOSE = True\n",
    "\n",
    "client = QdrantClient(url=qdrant_url)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def partition_text(text, max_length):\n",
    "    sentences = text.split('. ')\n",
    "    partitions, current_part, current_length = [], [], 0\n",
    "\n",
    "    for sentence in tqdm(sentences, desc=\"Partitioning text\"):\n",
    "        sentence_length = len(sentence.split())\n",
    "        if current_length + sentence_length > max_length:\n",
    "            partitions.append('. '.join(current_part))\n",
    "            current_part, current_length = [], 0\n",
    "        current_part.append(sentence)\n",
    "        current_length += sentence_length\n",
    "\n",
    "    if current_part:\n",
    "        partitions.append('. '.join(current_part))\n",
    "    return partitions\n",
    "\n",
    "def get_embedding(text):\n",
    "    return embedding_model.encode(text)\n",
    "\n",
    "def create_collection(dimension):\n",
    "    try:\n",
    "        client.delete_collection(collection_name=collection_name)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(size=dimension, distance=models.Distance.COSINE),\n",
    "    )\n",
    "    \n",
    "def upsert_points(embeddings, partitions):\n",
    "    points = [\n",
    "        models.PointStruct(id=i, vector=embedding.tolist(), payload={\"text\": partition})\n",
    "        for i, (embedding, partition) in enumerate(zip(embeddings, partitions))\n",
    "    ]\n",
    "    client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "def store_in_qdrant(partitions):\n",
    "    dimension = 384  # Dimension for 'all-MiniLM-L6-v2'\n",
    "    create_collection(dimension)\n",
    "    embeddings = [get_embedding(partition) for partition in tqdm(partitions, desc=\"Generating embeddings\")]\n",
    "    print(\"Embeddings len:\", len(embeddings))\n",
    "    upsert_points(embeddings, partitions)\n",
    "\n",
    "def search_points(query_embedding, k=5):\n",
    "    search_result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding.tolist(),\n",
    "        limit=k,\n",
    "        with_payload=True\n",
    "    )\n",
    "    return [hit.payload[\"text\"] for hit in search_result]\n",
    "\n",
    "def ask(query, k=2, p=False):\n",
    "    query_embedding = get_embedding(query)\n",
    "    retrieved_docs = search_points(query_embedding, k)\n",
    "    combined_docs = \"\\n\".join(retrieved_docs)\n",
    "    \n",
    "    inst = \"Instruction: If you do not find the answer in the context, just say you don't know.\"\n",
    "    rag_prompt = f\"{inst}\\n\\nContext:\\n{combined_docs}\\n\\nQuery: {query}\\nAnswer:\"\n",
    "    if p:\n",
    "        print(rag_prompt)\n",
    "    payload = {\"model\": \"llama3.2:latest\", \"prompt\": rag_prompt, \"stream\": False}\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(ollama_url_gen, headers=headers, data=json.dumps(payload))\n",
    "    print(response.json().get(\"response\", \"No response available\"))\n",
    "\n",
    "partitions = partition_text(all_text, max_length=512)\n",
    "print(f'Total partition count: {len(partitions)}')\n",
    "try:\n",
    "    store_in_qdrant(partitions)\n",
    "except Exception as e:\n",
    "    print(f\"Error storing in Qdrant: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30126d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He has over 13 years of experience in mobile and wearable software development, with expertise in OOP, Android (Java, Kotlin), iOS (Swift, Objective-C), Xcode, Version Control Systems, System Design, Application Architecture, Development Processes, Wearable & Hearable Technology.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"What is Hissains experience?\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b5019e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is interested in several things, including:\n",
      "\n",
      "1. Technological Innovation\n",
      "2. Human-Machine Interaction\n",
      "3. Information Theory\n",
      "4. Astronomy\n",
      "5. Probability\n",
      "6. Theory of Relativity\n",
      "7. Philosophy of Science\n",
      "\n",
      "And also he enjoys music and playing piano and guitar as a special interest\n"
     ]
    }
   ],
   "source": [
    "ask(\"Whats Hissains special interest?\", 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
