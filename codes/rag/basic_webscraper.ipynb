{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc264dcf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "#from transformers import GPT2TokenizerFast\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "#tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "model_path = '/Users/hissain/git/github/models/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def init_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    service = Service()\n",
    "    return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "def extract_text_from_url(url, driver):\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    for element in soup.find_all(\"div\", class_=\"js-yearly-contributions\"):\n",
    "        element.decompose()\n",
    "        \n",
    "    for element in soup([\"script\", \"style\"]):\n",
    "        element.decompose()\n",
    "    text = soup.get_text(separator=\" \")\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)           # Remove extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'\"()-]', '', text)  # Remove special characters\n",
    "    text = text.lower()                         # Normalize to lowercase\n",
    "    return text.strip()\n",
    "\n",
    "def split_sentences(text):\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)  # Split on sentence boundaries\n",
    "    return sentences\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def partition_sentences(sentences, url, max_tokens=512, overlap=1):\n",
    "    chunks, current_chunk = [], []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "        \n",
    "        if current_tokens + sentence_tokens > max_tokens:\n",
    "            chunks.append({\"text\": \" \".join(current_chunk), \"url\": url})\n",
    "            current_chunk = current_chunk[-overlap:]\n",
    "            current_tokens = count_tokens(\" \".join(current_chunk))\n",
    "\n",
    "        current_chunk.append(sentence)\n",
    "        current_tokens += sentence_tokens\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append({\"text\": \" \".join(current_chunk), \"url\": url})\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def process_urls(urls):\n",
    "    driver = init_driver()\n",
    "    all_chunks = []\n",
    "    \n",
    "    for url in tqdm(urls, desc=\"Processing URLs\"):\n",
    "        try:\n",
    "            raw_text = extract_text_from_url(url, driver)\n",
    "            clean_text_content = clean_text(raw_text)\n",
    "            sentences = split_sentences(clean_text_content)\n",
    "            chunks = partition_sentences(sentences, url, max_tokens=512, overlap=1)\n",
    "            all_chunks.extend(chunks)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {url}: {e}\")\n",
    "    \n",
    "    #driver.quit()\n",
    "    return all_chunks\n",
    "\n",
    "urls = [\n",
    "    \"https://github.com/hissain\",\n",
    "    \"https://github.com/hissain/CoronaTracker\",\n",
    "]\n",
    "\n",
    "rag_chunks = process_urls(urls)\n",
    "\n",
    "for i, chunk in enumerate(rag_chunks[:2]):\n",
    "    print(f\"Chunk {i+1}:\\nText: {chunk['text']}\\nURL: {chunk['url']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from qdrant_client import QdrantClient, models\n",
    "from tqdm.notebook import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "import aiohttp\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "qdrant_url = \"http://localhost:6333\"\n",
    "collection_name = \"github_collection\"\n",
    "\n",
    "ollama_url_inf = \"http://localhost:11434/api/show\"\n",
    "ollama_url_emb = \"http://localhost:11434/api/embeddings\"\n",
    "ollama_url_gen = \"http://localhost:11434/api/generate\"\n",
    "ollama_model_name = \"llama3.2:latest\"\n",
    "\n",
    "client = QdrantClient(url=qdrant_url)\n",
    "embedding_model = SentenceTransformer(model_path)\n",
    "\n",
    "def get_embedding(text):\n",
    "    return embedding_model.encode(text)\n",
    "\n",
    "def create_collection(dimension):\n",
    "    try:\n",
    "        client.delete_collection(collection_name=collection_name)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(size=dimension, distance=models.Distance.COSINE),\n",
    "    )\n",
    "    \n",
    "def upsert_points_with_metadata(embeddings, chunks):\n",
    "    points = [\n",
    "        models.PointStruct(id=i, vector=embedding.tolist(), payload={\"text\": chunk[\"text\"], \"url\": chunk[\"url\"]})\n",
    "        for i, (embedding, chunk) in enumerate(zip(embeddings, chunks))\n",
    "    ]\n",
    "    client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "def store_in_qdrant_with_metadata(chunks):\n",
    "    dimension = 384  # Dimension for 'all-MiniLM-L6-v2'\n",
    "    create_collection(dimension)\n",
    "    embeddings = [get_embedding(chunk[\"text\"]) for chunk in tqdm(chunks, desc=\"Generating embeddings\")]\n",
    "    upsert_points_with_metadata(embeddings, chunks)\n",
    "\n",
    "def search_points_with_metadata(query_embedding, k=3):\n",
    "    search_result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding.tolist(),\n",
    "        limit=k,\n",
    "        with_payload=True\n",
    "    )\n",
    "    return [{\"text\": hit.payload[\"text\"], \"url\": hit.payload[\"url\"]} for hit in search_result]\n",
    "\n",
    "async def ask(prompt, stream=True):\n",
    "    global chat_history\n",
    "    in_code_block = False\n",
    "    \n",
    "    query_embedding = get_embedding(query)\n",
    "    retrieved_docs = search_points_with_metadata(query_embedding, k)\n",
    "    \n",
    "    combined_docs = \"\\n\\n\".join([f\"Source: {doc['url']}\\n{doc['text']}\" for doc in retrieved_docs])\n",
    "    inst = \"Instruction: If you do not find the answer in the context, just say you don't know.\"\n",
    "    rag_prompt = f\"{inst}\\n\\nContext:\\n{combined_docs}\\n\\nQuery: {query}\\nAnswer:\"\n",
    "    if p:\n",
    "        print(rag_prompt)\n",
    "        \n",
    "    payload = {\"model\": ollama_model_name, \"prompt\": rag_prompt, \"stream\": True}\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response_text = \"\"\n",
    "    buffer = \"\"\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(ollama_url_chat, headers=headers, data=json.dumps(payload)) as response:\n",
    "            if stream:\n",
    "                async for chunk in response.content.iter_any():\n",
    "                    try:\n",
    "                        data = json.loads(chunk.decode('utf-8'))\n",
    "                        content = data.get(\"response\", \"\")\n",
    "                        buffer += content\n",
    "\n",
    "                        if len(buffer) > 10:\n",
    "                            response_text += buffer\n",
    "                            clear_output(wait=True)\n",
    "                            display(Markdown(response_text))\n",
    "                            buffer = \"\"\n",
    "                            \n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "                response_text += buffer\n",
    "                clear_output(wait=True)\n",
    "                display(Markdown(response_text))\n",
    "            else:\n",
    "                response_text = await response.text()\n",
    "                display(Markdown(response_text))\n",
    "\n",
    "    return response_text\n",
    "\n",
    "try:\n",
    "    store_in_qdrant_with_metadata(rag_chunks)\n",
    "    print(f'Stored {len(rag_chunks)} chunks')\n",
    "except Exception as e:\n",
    "    print(f\"Error storing in Qdrant: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b2bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"What are Hissain's special interests?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
