{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc264dcf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aeb9b42cdc348779326c38839dda6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing URLs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Text: hissain (md. sazzad hissain khan)  github skip to content navigation menu toggle navigation sign in product github copilot write better code with ai security find and fix vulnerabilities actions automate any workflow codespaces instant dev environments issues plan and track work code review manage code changes discussions collaborate outside of code code search find more, search less explore all features documentation github skills blog solutions by company size enterprises small and medium teams startups by use case devsecops devops cicd view all use cases by industry healthcare financial services manufacturing government view all industries view all solutions resources topics ai devops security software development view all explore learning pathways white papers, ebooks, webinars customer stories partners open source github sponsors fund open source developers the readme project github community articles repositories topics trending collections enterprise enterprise platform ai-powered developer platform available add-ons advanced security enterprise-grade security features github copilot enterprise-grade ai features premium support enterprise-grade 247 support pricing search or jump to... search code, repositories, users, issues, pull requests... search clear search syntax tips provide feedback we read every piece of feedback, and take your input very seriously. include my email address so i can be contacted cancel submit feedback saved searches use saved searches to filter your results more quickly name query to see all available qualifiers, see our documentation . cancel create saved search sign in sign up reseting focus you signed in with another tab or window. reload to refresh your session. you signed out in another tab or window. reload to refresh your session. you switched accounts on another tab or window. reload to refresh your session. dismiss alert hissain follow overview repositories 22 projects 0 packages 0 stars 16 more overview repositories projects packages stars hissain follow md. sazzad hissain khan hissain follow associate software architect  innovator  ai enthusiast 7 followers  1 following samsung electronics dhaka httpswww.linkedin.cominhissain block or report block or report hissain block user prevent this user from interacting with your repositories and sending you notifications. learn more about blocking users . you must be logged in to block users.\n",
      "URL: https://github.com/hissain\n",
      "\n",
      "Chunk 2:\n",
      "Text: you must be logged in to block users. add an optional note please don't include any personal information such as legal names or email addresses. maximum 100 characters, markdown supported. this note will be visible to only you. block user report abuse contact github support about this users behavior. learn more about reporting abuse . report abuse overview repositories 22 projects 0 packages 0 stars 16 more overview repositories projects packages stars hissain  readme .md about me i am an accomplished associate architect with over 13 years of experience in mobile and wearables software development. with a deep understanding of the challenges inherent in developing performant, robust, testable, and maintainable applications, from requirement analysis to architecture, design, development, and maintenance, i am confident in my ability to lead software development and commercialization for any organization. my experience includes working on samsung health for ios, an application boasting over 6 million users and approximately 160k dau on the app store market, with a global rating of 4.5. between 2021 and 2024, i achieved six patent applications granted by samsung sipms, currently in the process of being published in uspto among them, one patent has already been published in uspto, wipo, and kr. my technical expertise encompasses oop, android (java, kotlin), ios (swift, objective-c), xcode, version control systems, system design, application architecture, development processes, wearable  hearable technology. additionally, i have experience in tizen app development and windows app development. currently, i am engaged in samsung earbuds device development on rtos, specifically in music streaming over bt classic and le audio, as well as in ballistocardiogram signal processing for stress score generation. my special interests include technological innovation, human-machine interaction, information theory, astronomy, probability, theory of relativity, philosophy of science, piano, guitar, and poetry. furthermore, i have pursued studies in critical thinking and how to make valid argumentation. i value the passion of problem-solving minds and firmly believe that software engineering is the profession that suits me best. i am always eager to learn and grow in my field and am committed to delivering high-quality solutions to my clients and stakeholders.\n",
      "URL: https://github.com/hissain\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "#from transformers import GPT2TokenizerFast\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "#tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "model_path = '/Users/hissain/git/github/models/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def init_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    service = Service()\n",
    "    return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "def extract_text_from_url(url, driver):\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    for element in soup.find_all(\"div\", class_=\"js-yearly-contributions\"):\n",
    "        element.decompose()\n",
    "        \n",
    "    for element in soup([\"script\", \"style\"]):\n",
    "        element.decompose()\n",
    "    text = soup.get_text(separator=\" \")\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)           # Remove extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'\"()-]', '', text)  # Remove special characters\n",
    "    text = text.lower()                         # Normalize to lowercase\n",
    "    return text.strip()\n",
    "\n",
    "def split_sentences(text):\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)  # Split on sentence boundaries\n",
    "    return sentences\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def partition_sentences(sentences, url, max_tokens=512, overlap=1):\n",
    "    chunks, current_chunk = [], []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "        \n",
    "        if current_tokens + sentence_tokens > max_tokens:\n",
    "            chunks.append({\"text\": \" \".join(current_chunk), \"url\": url})\n",
    "            current_chunk = current_chunk[-overlap:]\n",
    "            current_tokens = count_tokens(\" \".join(current_chunk))\n",
    "\n",
    "        current_chunk.append(sentence)\n",
    "        current_tokens += sentence_tokens\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append({\"text\": \" \".join(current_chunk), \"url\": url})\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def process_urls(urls):\n",
    "    driver = init_driver()\n",
    "    all_chunks = []\n",
    "    \n",
    "    for url in tqdm(urls, desc=\"Processing URLs\"):\n",
    "        try:\n",
    "            raw_text = extract_text_from_url(url, driver)\n",
    "            clean_text_content = clean_text(raw_text)\n",
    "            sentences = split_sentences(clean_text_content)\n",
    "            chunks = partition_sentences(sentences, url, max_tokens=512, overlap=1)\n",
    "            all_chunks.extend(chunks)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {url}: {e}\")\n",
    "    \n",
    "    #driver.quit()\n",
    "    return all_chunks\n",
    "\n",
    "urls = [\n",
    "    \"https://github.com/hissain\",\n",
    "    \"https://github.com/hissain/CoronaTracker\",\n",
    "]\n",
    "\n",
    "rag_chunks = process_urls(urls)\n",
    "\n",
    "for i, chunk in enumerate(rag_chunks[:2]):\n",
    "    print(f\"Chunk {i+1}:\\nText: {chunk['text']}\\nURL: {chunk['url']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c686a7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca06d9402614fbe98ead0e690683e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 8 chunks\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from qdrant_client import QdrantClient, models\n",
    "from tqdm.notebook import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "import requests\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "qdrant_url = \"http://localhost:6333\"\n",
    "collection_name = \"github_collection\"\n",
    "\n",
    "ollama_url_inf = \"http://localhost:11434/api/show\"\n",
    "ollama_url_emb = \"http://localhost:11434/api/embeddings\"\n",
    "ollama_url_gen = \"http://localhost:11434/api/generate\"\n",
    "ollama_model_name = \"llama3.2:latest\"\n",
    "\n",
    "client = QdrantClient(url=qdrant_url)\n",
    "embedding_model = SentenceTransformer(model_path)\n",
    "\n",
    "def get_embedding(text):\n",
    "    return embedding_model.encode(text)\n",
    "\n",
    "def create_collection(dimension):\n",
    "    try:\n",
    "        client.delete_collection(collection_name=collection_name)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(size=dimension, distance=models.Distance.COSINE),\n",
    "    )\n",
    "    \n",
    "def upsert_points_with_metadata(embeddings, chunks):\n",
    "    points = [\n",
    "        models.PointStruct(id=i, vector=embedding.tolist(), payload={\"text\": chunk[\"text\"], \"url\": chunk[\"url\"]})\n",
    "        for i, (embedding, chunk) in enumerate(zip(embeddings, chunks))\n",
    "    ]\n",
    "    client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "def store_in_qdrant_with_metadata(chunks):\n",
    "    dimension = 384  # Dimension for 'all-MiniLM-L6-v2'\n",
    "    create_collection(dimension)\n",
    "    embeddings = [get_embedding(chunk[\"text\"]) for chunk in tqdm(chunks, desc=\"Generating embeddings\")]\n",
    "    upsert_points_with_metadata(embeddings, chunks)\n",
    "\n",
    "def search_points_with_metadata(query_embedding, k=3):\n",
    "    search_result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding.tolist(),\n",
    "        limit=k,\n",
    "        with_payload=True\n",
    "    )\n",
    "    return [{\"text\": hit.payload[\"text\"], \"url\": hit.payload[\"url\"]} for hit in search_result]\n",
    "\n",
    "def ask(query, k=3, p=False):\n",
    "    \n",
    "    query_embedding = get_embedding(query)\n",
    "    retrieved_docs = search_points_with_metadata(query_embedding, k)\n",
    "    \n",
    "    combined_docs = \"\\n\\n\".join([f\"Source: {doc['url']}\\n{doc['text']}\" for doc in retrieved_docs])\n",
    "    inst = \"Instruction: If you do not find the answer in the context, just say you don't know.\"\n",
    "    rag_prompt = f\"{inst}\\n\\nContext:\\n{combined_docs}\\n\\nQuery: {query}\\nAnswer:\"\n",
    "    if p:\n",
    "        print(rag_prompt)\n",
    "        \n",
    "    payload = {\"model\": ollama_model_name, \"prompt\": rag_prompt, \"stream\": True}\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response_text = \"\"\n",
    "    buffer = \"\"\n",
    "\n",
    "    response = requests.post(ollama_url_gen, headers=headers, data=json.dumps(payload), stream=True)\n",
    "\n",
    "    # Process the response content as it arrives\n",
    "    if response.status_code == 200:\n",
    "        for chunk in response.iter_content(chunk_size=None):\n",
    "            try:\n",
    "                data = json.loads(chunk.decode('utf-8'))\n",
    "                content = data.get(\"response\", \"\")\n",
    "                buffer += content\n",
    "\n",
    "                # Display output every few characters for real-time effect\n",
    "                if len(buffer) > 10:\n",
    "                    response_text += buffer\n",
    "                    clear_output(wait=True)\n",
    "                    display(Markdown(response_text))\n",
    "                    buffer = \"\"\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "        # Display any remaining buffered content\n",
    "        response_text += buffer\n",
    "        clear_output(wait=True)\n",
    "        display(Markdown(response_text))\n",
    "    else:\n",
    "        print(\"Request failed:\", response.status_code, response.text)\n",
    "\n",
    "    return response_text\n",
    "\n",
    "try:\n",
    "    store_in_qdrant_with_metadata(rag_chunks)\n",
    "    print(f'Stored {len(rag_chunks)} chunks')\n",
    "except Exception as e:\n",
    "    print(f\"Error storing in Qdrant: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32f89900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided context, I found some clues about Hissain's special interests:\n",
       "\n",
       "1. AI: Hissain is described as an \"AI enthusiast\" and has a GitHub profile that mentions using AI features like Copilot.\n",
       "2. Software development: He is a software architect and has contributed to several open-source projects on GitHub.\n",
       "3. Healthcare/COVID-19: His CoronaTracker project is focused on tracing close-contact candidates for COVID-19 patients, suggesting an interest in healthcare or pandemic-related issues.\n",
       "4. Innovation/Entrepreneurship: Hissain's profile description mentions \"innovator\", which suggests he values innovation and entrepreneurial spirit.\n",
       "\n",
       "While these clues don't provide a comprehensive picture of Hissain's special interests, they suggest a focus on AI, software development, healthcare, and innovation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = ask(\"What are Hissain's special interests?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e4cae96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "One, currently in the process of being published, and one already published."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = ask(\"How many USPTO patent Hissain acquired?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
