{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc264dcf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8909bb49eb4b05acb600ffb6841659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing URLs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 38\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "model_path = '/Users/hissain/git/github/models/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def init_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    service = Service()\n",
    "    return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "def extract_text_from_url(url, driver):\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    soup = soup.body\n",
    "    if soup is None: return \"\" \n",
    "    for script in soup([\"script\", \"style\", \"footer\", \"header\", \"nav\", \"a\", \"table\"]):\n",
    "        script.decompose()\n",
    "    return soup.get_text(separator=\" \")\n",
    "\n",
    "def prepare_table_for_rag_by_token_count(url, df, window_len_tokens):\n",
    "    chunks, current_chunk = [], \"\"\n",
    "    header_text = \"Table Headers: \" + \", \".join(map(str, df.columns))\n",
    "    header_tokens = tokenizer.encode(header_text)\n",
    "    if len(header_tokens) > window_len_tokens:\n",
    "        chunks.extend([{\"text\": header_text[i:i+window_len_tokens], \"url\": url} for i in range(0, len(header_text), window_len_tokens)])\n",
    "    else:\n",
    "        current_chunk += header_text + \"\\n\"\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        row_text = [f\"{col}: {str(val)}\" for col, val in row.items()]\n",
    "        row_str = \" | \".join(row_text)\n",
    "        row_tokens = tokenizer.encode(row_str)\n",
    "        \n",
    "        if len(tokenizer.encode(current_chunk)) + len(row_tokens) > window_len_tokens:\n",
    "            chunks.append({\"text\": current_chunk.strip(), \"url\": url})\n",
    "            current_chunk = header_text + \"\\n\" + row_str + \"\\n\"\n",
    "        else:\n",
    "            current_chunk += row_str + \"\\n\"\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append({\"text\": current_chunk.strip(), \"url\": url})\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def extract_table_from_url(url, driver):\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    html_source = driver.page_source\n",
    "    html_file = StringIO(html_source)\n",
    "    tables = pd.read_html(html_file)\n",
    "    \n",
    "    for t in tables:\n",
    "        chunks = prepare_table_for_rag_by_token_count(url, t, 512)\n",
    "        all_chunks.extend(chunks)\n",
    "        \n",
    "    return all_chunks\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'\"()-]', '', text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "def split_sentences(text):\n",
    "    return re.split(r'(?<=[.!?]) +', text)\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def partition_sentences(sentences, url, max_tokens=512, overlap=1):\n",
    "    chunks, current_chunk = [], []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "        \n",
    "        if current_tokens + sentence_tokens > max_tokens:\n",
    "            chunks.append({\"text\": \" \".join(map(str, current_chunk)), \"url\": url})\n",
    "            current_chunk = current_chunk[-overlap:]\n",
    "            current_tokens = count_tokens(\" \".join(map(str, current_chunk)))\n",
    "\n",
    "        current_chunk.append(sentence)\n",
    "        current_tokens += sentence_tokens\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append({\"text\": \" \".join(map(str, current_chunk)), \"url\": url})\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def process_urls(urls):\n",
    "    driver = init_driver()\n",
    "    all_chunks = []\n",
    "    \n",
    "    for url in tqdm(urls, desc=\"Processing URLs\"):\n",
    "        try:\n",
    "            raw_text = extract_text_from_url(url, driver)\n",
    "            clean_text_content = clean_text(raw_text)\n",
    "            sentences = split_sentences(clean_text_content)\n",
    "            chunks = partition_sentences(sentences, url, max_tokens=512, overlap=1)\n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "            table_chunks = extract_table_from_url(url, driver)\n",
    "            all_chunks.extend(table_chunks)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {url}: {e}\")\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "urls = [\n",
    "    \"https://en.wikipedia.org/wiki/List_of_wars_by_death_toll\",\n",
    "]\n",
    "\n",
    "rag_chunks = process_urls(urls)\n",
    "print(f\"Total chunks: {len(rag_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a071ac5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Text: . bercovitch, jacob jackson, richard (1997). . congressional quarterly. . . www.ictj.org . 2009-01-01 . retrieved 2024-09-30 . . 2011-05-22. archived from on 2011-05-22 . retrieved 2024-09-30 . . scribd . retrieved 2024-10-03 . marley, david (1998). . abc-clio. . farcau, bruce w. (1996-05-23). . bloomsbury academic. . web, apunto. . portal guarani (in european spanish) . retrieved 2024-10-01 . tinker-salas, miguel (2015). . oxford university press. . mwakikagile, godfrey (2014-04-21). . new africa press. . arrian (1884). . cornell university library. london, hodder and stoughton. arrian (2018-04-10). . ozymandias press. . staff, historynet (2007-09-17). . historynet . retrieved 2024-10-03 . . herre, bastian rodés-guirao, lucas roser, max (2024-03-20). . our world in data . herre, bastian roser, max (2024-07-15). . our world in data . further reading   (2011). . penguin books. . pp. 832. (see also ) levy, jack s. (1983). . university press of kentucky, usa. . external links   . (2016). published online at ourworldindata.org. retrieved from \" \"  hidden categories\n",
      "URL: https://en.wikipedia.org/wiki/List_of_wars_by_death_toll\n",
      "\n",
      "Chunk 2:\n",
      "Text: Table Headers: 0\n",
      "0: Part of a series on\n",
      "0: War (outline)\n",
      "0: showHistory\n",
      "0: showMilitary\n",
      "0: showBattlespace\n",
      "0: showWeapons\n",
      "0: showTactics\n",
      "0: showOperational\n",
      "0: showStrategy\n",
      "0: showGrand strategy\n",
      "0: showAdministrative\n",
      "0: showOrganization\n",
      "0: showPersonnel\n",
      "0: showLogistics\n",
      "0: showScience\n",
      "0: showLaw\n",
      "0: showTheory\n",
      "0: showNon-warfare\n",
      "0: showCulture\n",
      "0: showRelated\n",
      "0: hideLists Battles Military occupations Military terms Operations Sieges War crimes Wars Weapons Writers\n",
      "0: vte\n",
      "URL: https://en.wikipedia.org/wiki/List_of_wars_by_death_toll\n",
      "\n",
      "Chunk 3:\n",
      "Text: Table Headers: War, Death range, Date, Combatants, Location\n",
      "War: World War II | Death range: 50–85 million[4][5][6] | Date: 1939–1945 | Combatants: Allied Powers vs. Axis Powers | Location: Global\n",
      "War: Mongol invasions and conquests | Death range: 20–60 million[7][8][9][10] | Date: 1207–1405 | Combatants: Mongol Empire vs. various states in Eurasia | Location: Asia and Europe\n",
      "War: Three Kingdoms | Death range: 34 million[10] | Date: 220–280 | Combatants: Multiple sides | Location: China\n",
      "War: Taiping Rebellion | Death range: 20–30 million[11][12] | Date: 1850–1864 | Combatants: Qing Dynasty vs. Taiping Heavenly Kingdom | Location: China\n",
      "War: World War I | Death range: 15–30 million[13][14] | Date: 1914–1918 | Combatants: Allied Powers vs. Central Powers | Location: Global\n",
      "War: Manchu Conquest of China | Death range: 25 million[15][16] | Date: 1618–1683 | Combatants: Manchu vs. Ming Dynasty | Location: China\n",
      "War: Conquests of Timur | Death range: 7–20 million[10] | Date: 1369–1405 | Combatants: Timurid Empire vs. various states in Asia | Location: Central Asia, West Asia, and South Asia\n",
      "War: An Lushan rebellion | Death range: 13 million[17] | Date: 754–763 | Combatants: Tang Dynasty and Uyghur Khaganate vs. Yan Dynasty | Location: China\n",
      "War: Thirty Years' War | Death range: 4–12 million[18] | Date: 1618–1648 | Combatants: Anti-Imperial Alliance vs. Imperial Alliance | Location: Europe\n",
      "War: Spanish conquest of Mexico | Death range: 10.5 million[19] | Date: 1519–1530 | Combatants: Spanish Empire and allies vs. Aztec Empire and allies | Location: Mexico\n",
      "War: Spanish conquest of the Inca Empire | Death range: 10 million[20] | Date: 1533–1572 | Combatants: Spanish Empire vs. Inca Empire | Location: South America\n",
      "URL: https://en.wikipedia.org/wiki/List_of_wars_by_death_toll\n",
      "\n",
      "Chunk 4:\n",
      "Text: Table Headers: War, Death range, Date, Combatants, Location\n",
      "War: Russian Civil War | Death range: 7–10 million[21] | Date: 1917–1922 | Combatants: Multiple sides; Bolsheviks, Anti-Bolshevik left, White Movement, Allied and Central Intervention, as well as various separatists | Location: Russia\n",
      "War: Chinese Civil War | Death range: 4–9 million[22] | Date: 1927–1949[b] | Combatants: Multiple sides; but predominantly Communists vs. Kuomintang | Location: China\n",
      "War: Crusades | Death range: 1–9 million[25][26] | Date: 1095–1291 | Combatants: Originally Byzantine Empire vs. Seljuk Empire, but evolved into Christians vs. Muslims | Location: Europe and the Middle East\n",
      "War: Reconquista | Death range: 7 million[27] | Date: 718–1492 | Combatants: Spanish and Portuguese Christians vs. Spanish and Portuguese Muslims | Location: Iberian Peninsula\n",
      "War: French Revolutionary and Napoleonic Wars | Death range: 5–7 million[28] | Date: 1792–1815 | Combatants: French Republic, later French Empire, vs. Coalition forces | Location: Europe\n",
      "War: Conquests of Menelik II | Death range: 6 million[29] | Date: 1878–1904 | Combatants: Ethiopian Empire vs. Emirate of Harar, Kingdom of Kaffa, Kingdom of Wolaita, and allies | Location: Horn of Africa\n",
      "War: Second Congo War | Death range: 3–5.4 million[30][31][32] | Date: 1998–2003 | Combatants: Multiple sides | Location: Democratic Republic of the Congo\n",
      "War: Spanish conquest of New Granada | Death range: 5.25 million[33][34] | Date: 1525–1540 | Combatants: Spanish Empire and Klein-Venedig vs. Muisca Confederation and other civilizations | Location: Colombia\n",
      "War: Deccan wars | Death range: 4.6–5 million[35] | Date: 1680–1707 | Combatants: Mughal Empire vs. Maratha Confederacy | Location: Indian subcontinent\n",
      "War: Nigerian Civil War | Death range: 3.04–4.1 million[36][37] | Date: 1967–1970 | Combatants: Nigeria vs. Biafra | Location: Nigeria\n",
      "URL: https://en.wikipedia.org/wiki/List_of_wars_by_death_toll\n",
      "\n",
      "Chunk 5:\n",
      "Text: Table Headers: War, Death range, Date, Combatants, Location\n",
      "War: Deluge | Death range: 3–4 milion[38] | Date: 1648–1666 | Combatants: Poland–Lithuania vs Swedish Empire and Russia | Location: Eastern Europe\n",
      "War: French Wars of Religion | Death range: 2–4 million[39] | Date: 1562–1598 | Combatants: French catholics vs Huguenots | Location: France\n",
      "War: Korean War | Death range: 2.5–3.5 million[40][22] | Date: 1950–1953 | Combatants: North Korea and allies vs. South Korea and allies | Location: Korean Peninsula\n",
      "War: Vietnam War | Death range: 1.1–3.4 million[41][42] | Date: 1955–1975 | Combatants: North Vietnam and allies vs. South Vietnam and allies | Location: Indochina\n",
      "War: Hundred Years' War | Death range: 2.3–3.3 million[43][44] | Date: 1337–1453 | Combatants: House of Valois vs. House of Plantagenet | Location: Western Europe\n",
      "War: Afghan conflict | Death range: 1.17–3 million[45][46][47] | Date: 1978–present | Combatants: Multiple sides; Afghan mujahideen, later Islamic Emirate of Afghanistan, United Tajik Opposition vs. Soviet Union, Democratic Republic of Afghanistan, Northern Alliance, Tajikistan, and the United States-led coalition | Location: Afghanistan, Pakistan and Tajikistan\n",
      "War: Delhi Conquest of North India | Death range: 0.5–3 million[48] | Date: 1300–1310 | Combatants: Delhi Sultanate vs. North Indian States | Location: Indian subcontinent\n",
      "War: Bangladesh Liberation War | Death range: 0.3–3 million[49][50] | Date: 1971 | Combatants: India and Provisional Government of Bangladesh vs. Pakistan | Location: Indian subcontinent\n",
      "War: Mexican Revolution | Death range: 1.7–2.7 million[51] | Date: 1910–1920 | Combatants: Anti-government vs. Pro-government | Location: Mexico\n",
      "War: Ethiopian Civil War and Eritrean War of Independence | Death range: 1.75–2 million[52][53][54] | Date: 1961–1991[c] | Combatants: EPRDF, later EPLF vs. Derg and People's Democratic Republic of Ethiopia | Location: Horn of Africa\n",
      "URL: https://en.wikipedia.org/wiki/List_of_wars_by_death_toll\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(rag_chunks[15:20]):\n",
    "    print(f\"Chunk {i+1}:\\nText: {chunk['text']}\\nURL: {chunk['url']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c686a7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4423326a9f6745899dace119610e0448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 38 chunks\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from qdrant_client import QdrantClient, models\n",
    "from tqdm.notebook import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "import requests\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"Connection\": \"keep-alive\", \"Content-Type\": \"application/json\"})\n",
    "\n",
    "qdrant_url = \"http://localhost:6333\"\n",
    "collection_name = \"github_collection\"\n",
    "\n",
    "ollama_url_inf = \"http://localhost:11434/api/show\"\n",
    "ollama_url_emb = \"http://localhost:11434/api/embeddings\"\n",
    "ollama_url_gen = \"http://localhost:11434/api/generate\"\n",
    "ollama_model_name = \"llama3.2:latest\"\n",
    "\n",
    "client = QdrantClient(url=qdrant_url)\n",
    "embedding_model = SentenceTransformer(model_path)\n",
    "\n",
    "def get_embedding(text):\n",
    "    return embedding_model.encode(text)\n",
    "\n",
    "def create_collection(dimension):\n",
    "    try:\n",
    "        client.delete_collection(collection_name=collection_name)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(size=dimension, distance=models.Distance.COSINE),\n",
    "    )\n",
    "    \n",
    "def upsert_points_with_metadata(embeddings, chunks):\n",
    "    points = [\n",
    "        models.PointStruct(id=i, vector=embedding.tolist(), payload={\"text\": chunk[\"text\"], \"url\": chunk[\"url\"]})\n",
    "        for i, (embedding, chunk) in enumerate(zip(embeddings, chunks))\n",
    "    ]\n",
    "    client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "def store_in_qdrant_with_metadata(chunks):\n",
    "    dimension = 384  # Dimension for 'all-MiniLM-L6-v2'\n",
    "    create_collection(dimension)\n",
    "    embeddings = [get_embedding(chunk[\"text\"]) for chunk in tqdm(chunks, desc=\"Generating embeddings\")]\n",
    "    upsert_points_with_metadata(embeddings, chunks)\n",
    "\n",
    "def search_points_with_metadata(query_embedding, k=3):\n",
    "    search_result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding.tolist(),\n",
    "        limit=k,\n",
    "        with_payload=True\n",
    "    )\n",
    "    return [{\"text\": hit.payload[\"text\"], \"url\": hit.payload[\"url\"]} for hit in search_result]\n",
    "\n",
    "def ask(query, k=3, p=False):\n",
    "    \n",
    "    query_embedding = get_embedding(query)\n",
    "    retrieved_docs = search_points_with_metadata(query_embedding, k)\n",
    "    \n",
    "    combined_docs = \"\\n\\n\".join([f\"Source: {doc['url']}\\n{doc['text']}\" for doc in retrieved_docs])\n",
    "    inst = \"Instruction: If you do not find the answer in the context, just say you don't know.\"\n",
    "    rag_prompt = f\"{inst}\\n\\nContext:\\n{combined_docs}\\n\\nQuery: {query}\\nAnswer:\"\n",
    "    if p:\n",
    "        print(rag_prompt)\n",
    "        \n",
    "    payload = {\"model\": ollama_model_name, \"prompt\": rag_prompt, \"stream\": True}\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response_text = \"\"\n",
    "    buffer = \"\"\n",
    "\n",
    "    response = session.post(ollama_url_gen, headers=headers, data=json.dumps(payload), stream=True)\n",
    "\n",
    "    # Process the response content as it arrives\n",
    "    if response.status_code == 200:\n",
    "        for chunk in response.iter_content(chunk_size=None):\n",
    "            try:\n",
    "                data = json.loads(chunk.decode('utf-8'))\n",
    "                content = data.get(\"response\", \"\")\n",
    "                buffer += content\n",
    "\n",
    "                # Display output every few characters for real-time effect\n",
    "                if len(buffer) > 10:\n",
    "                    response_text += buffer\n",
    "                    clear_output(wait=True)\n",
    "                    display(Markdown(response_text))\n",
    "                    buffer = \"\"\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "        # Display any remaining buffered content\n",
    "        response_text += buffer\n",
    "        clear_output(wait=True)\n",
    "        display(Markdown(response_text))\n",
    "    else:\n",
    "        print(\"Request failed:\", response.status_code, response.text)\n",
    "\n",
    "    return response_text\n",
    "\n",
    "try:\n",
    "    store_in_qdrant_with_metadata(rag_chunks)\n",
    "    print(f'Stored {len(rag_chunks)} chunks')\n",
    "except Exception as e:\n",
    "    print(f\"Error storing in Qdrant: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32f89900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "0.45 million[128]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = ask(\"How many death in Colombian conflict?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9187c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_url_chat = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "def chat(query, k=2, p=False, stream=True):\n",
    "    global chat_history\n",
    "    \n",
    "    query_embedding = get_embedding(query)\n",
    "    retrieved_docs = search_points_with_metadata(query_embedding, k)\n",
    "    \n",
    "    combined_docs = \"\\n\\n\".join([f\"Source: {doc['url']}\\n{doc['text']}\" for doc in retrieved_docs])\n",
    "    inst = \"Instruction: If you do not find the answer in the context, just say you don't know.\"\n",
    "    rag_prompt = f\"{inst}\\n\\nContext:\\n{combined_docs}\\n\\nQuery: {query}\\nAnswer:\"\n",
    "    if p:\n",
    "        print(rag_prompt)\n",
    "        \n",
    "    chat_history.append({\"role\": \"user\", \"content\": rag_prompt})\n",
    "    payload = {\"model\": ollama_model_name, \"messages\": chat_history, \"stream\": stream}\n",
    "    headers = {\"Connection\": \"keep-alive\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response_text = \"\"\n",
    "    buffer = \"\"\n",
    "\n",
    "    response = session.post(ollama_url_chat, data=json.dumps(payload), stream=stream)\n",
    "\n",
    "    # Process the response content as it arrives\n",
    "    if response.status_code == 200:\n",
    "        for chunk in response.iter_content(chunk_size=None):\n",
    "            try:\n",
    "                data = json.loads(chunk.decode('utf-8'))\n",
    "                content = data.get(\"message\", {}).get(\"content\", \"\")\n",
    "                buffer += content\n",
    "\n",
    "                # Display output every few characters for real-time effect\n",
    "                if len(buffer) > 10:\n",
    "                    response_text += buffer\n",
    "                    clear_output(wait=True)\n",
    "                    display(Markdown(response_text))\n",
    "                    buffer = \"\"\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "        # Display any remaining buffered content\n",
    "        response_text += buffer\n",
    "        clear_output(wait=True)\n",
    "        display(Markdown(response_text))\n",
    "    else:\n",
    "        print(\"Request failed:\", response.status_code, response.text)\n",
    "\n",
    "    return response_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
