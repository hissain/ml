{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "23828579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e7e82a4b764923bf780694e926c4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping pages:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 299\n",
      "Chunk: contents list of wars by death toll List of wars by death toll | 0 ||| Part of a series on || War (outline) || showHistory || showMilitary || showBattlespace || showWeapons || showTactics || showOperational || showStrategy || showGrand strategy || showAdministrative || showOrganization || showPersonnel || showLogistics || showScience || showLaw || showTheory || showNon-warfare || showCulture || showRelated || hideLists Battles Military occupations Military terms Operations Sieges War crimes Wars Weapons Writers || vte || this list of wars by death toll includes all deaths that are either directly or indirectly caused by war. these numbers include the deaths of military personnel which are the direct results of a battle or other military wartime actions, as well as wartime / war - related deaths of civilians which are often results of war - induced epidemics, famines, genocide, etc. due to incomplete records, the destruction of evidence, differing methods of counting, and various other reasons, death tolls of wars have often been quite uncertain, and heavily debated. while the definition of war isn't entirely clear - cut, there is a general understanding of what it is. merriam - webster defines war as \" a state of usually open and declared armed hostile conflict between states or nations \", [ 1 ] oxford english dictionary defines war as \" hostile contention by means of armed forces, carried on between nations, states, or rulers, or between parties in the same nation or state ; the employment of armed forces against a foreign power, or against an opposing party in the state \", [ 2 ] and encyclopædia britannica defines war as \" a conflict between political groups involving hostilities of considerable duration and magnitude \". [ 3 ] this list excludes mass killings and atrocities not explicitly classified as genocides, as well as genocides occurring outside of wartime, human sacrifices, ethnic cleansing operations, and acts of state terrorism or political repression during peacetime. [ a ] list\n",
      "Source URL: https://en.wikipedia.org/wiki/List_of_wars_by_death_toll\n",
      "\n",
      "Chunk: List | War | Death range | Date | Combatants | Location ||| World War II | 50–85 million[4][5][6] | 1939–1945 | Allied Powers vs. Axis Powers | Global || Mongol invasions and conquests | 20–60 million[7][8][9][10] | 1207–1405 | Mongol Empire vs. various states in Eurasia | Asia and Europe || Three Kingdoms | 34 million[10] | 220–280 | Multiple sides | China || Taiping Rebellion | 20–30 million[11][12] | 1850–1864 | Qing Dynasty vs. Taiping Heavenly Kingdom | China || World War I | 15–30 million[13][14] | 1914–1918 | Allied Powers vs. Central Powers | Global || Manchu Conquest of China | 25 million[15][16] | 1618–1683 | Manchu vs. Ming Dynasty | China || Conquests of Timur | 7–20 million[10] | 1369–1405 | Timurid Empire vs. various states in Asia | Central Asia, West Asia, and South Asia || An Lushan rebellion | 13 million[17] | 754–763 | Tang Dynasty and Uyghur Khaganate vs. Yan Dynasty | China || Thirty Years' War | 4–12 million[18] | 1618–1648 | Anti-Imperial Alliance vs. Imperial Alliance | Europe || Spanish conquest of Mexico | 10.5 million[19] | 1519–1530 | Spanish Empire and allies vs. Aztec Empire and allies | Mexico || Spanish conquest of the Inca Empire | 10 million[20] | 1533–1572 | Spanish Empire vs. Inca Empire | South America || Russian Civil War | 7–10 million[21] | 1917–1922 | Multiple sides; Bolsheviks, Anti-Bolshevik left, White Movement, Allied and Central Intervention, as well as various separatists | Russia || Chinese Civil War | 4–9 million[22] | 1927–1949[b] | Multiple sides; but predominantly Communists vs. Kuomintang | China ||\n",
      "Source URL: https://en.wikipedia.org/wiki/List_of_wars_by_death_toll\n",
      "\n",
      "Chunk: List | War | Death range | Date | Combatants | Location ||| crusades | 1 – 9 million [ 25 ] [ 26 ] | 1095 – 1291 | originally byzantine empire vs. seljuk empire, but evolved into christians vs. muslims | europe and the middle east || Reconquista | 7 million[27] | 718–1492 | Spanish and Portuguese Christians vs. Spanish and Portuguese Muslims | Iberian Peninsula || French Revolutionary and Napoleonic Wars | 5–7 million[28] | 1792–1815 | French Republic, later French Empire, vs. Coalition forces | Europe || Conquests of Menelik II | 6 million[29] | 1878–1904 | Ethiopian Empire vs. Emirate of Harar, Kingdom of Kaffa, Kingdom of Wolaita, and allies | Horn of Africa || Second Congo War | 3–5.4 million[30][31][32] | 1998–2003 | Multiple sides | Democratic Republic of the Congo || Spanish conquest of New Granada | 5.25 million[33][34] | 1525–1540 | Spanish Empire and Klein-Venedig vs. Muisca Confederation and other civilizations | Colombia || Deccan wars | 4.6–5 million[35] | 1680–1707 | Mughal Empire vs. Maratha Confederacy | Indian subcontinent || Nigerian Civil War | 3.04–4.1 million[36][37] | 1967–1970 | Nigeria vs. Biafra | Nigeria || Deluge | 3–4 milion[38] | 1648–1666 | Poland–Lithuania vs Swedish Empire and Russia | Eastern Europe || French Wars of Religion | 2–4 million[39] | 1562–1598 | French catholics vs Huguenots | France || Korean War | 2.5–3.5 million[40][22] | 1950–1953 | North Korea and allies vs. South Korea and allies | Korean Peninsula || Vietnam War | 1.1–3.4 million[41][42] | 1955–1975 | North Vietnam and allies vs. South Vietnam and allies | Indochina ||\n",
      "Source URL: https://en.wikipedia.org/wiki/List_of_wars_by_death_toll\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import time\n",
    "from io import StringIO\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "model_path = '/Users/hissain/git/github/models/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Define maximum token length per chunk\n",
    "max_token_length = 480\n",
    "\n",
    "def init_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    service = Service()\n",
    "    return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "def get_text_content(element):\n",
    "    return ' '.join(element.stripped_strings)\n",
    "\n",
    "def chunk_text(text, max_token_length):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_token_length, len(tokens))\n",
    "        chunk = tokenizer.decode(tokens[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "def merge_small_chunks(chunks, max_token_length):\n",
    "    merged_chunks = []\n",
    "    temp_chunk = \"\"\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if len(tokenizer.encode(temp_chunk + \" \" + chunk)) <= max_token_length:\n",
    "            temp_chunk += \" \" + chunk\n",
    "        else:\n",
    "            # Ensure no chunk exceeds max_token_length\n",
    "            while len(tokenizer.encode(temp_chunk)) > max_token_length:\n",
    "                # Split the temp_chunk if it's too long\n",
    "                split_point = max_token_length - 1  # Choose safe split point\n",
    "                merged_chunks.append(tokenizer.decode(tokenizer.encode(temp_chunk)[:split_point]))\n",
    "                temp_chunk = tokenizer.decode(tokenizer.encode(temp_chunk)[split_point:])\n",
    "                \n",
    "            merged_chunks.append(temp_chunk.strip())\n",
    "            temp_chunk = chunk\n",
    "    \n",
    "    if temp_chunk:\n",
    "        merged_chunks.append(temp_chunk.strip())\n",
    "    \n",
    "    return merged_chunks\n",
    "\n",
    "def chunk_table(df, max_token_length, header_info):\n",
    "    table_chunks = []\n",
    "    current_chunk = header_info + ' ||| '  # Distinct marker between header and rows\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        row_text = ' | '.join([str(cell) for cell in row if pd.notna(cell)])\n",
    "        combined_text = current_chunk + row_text + ' || '\n",
    "        \n",
    "        if len(tokenizer.encode(combined_text)) <= max_token_length:\n",
    "            current_chunk += row_text + ' || '\n",
    "        else:\n",
    "            # Split the row if adding it would exceed max_token_length\n",
    "            row_chunks = chunk_text(row_text, max_token_length)\n",
    "            for sub_chunk in row_chunks:\n",
    "                if len(tokenizer.encode(current_chunk)) + len(tokenizer.encode(sub_chunk)) <= max_token_length:\n",
    "                    current_chunk += sub_chunk + ' || '\n",
    "                else:\n",
    "                    table_chunks.append(current_chunk.strip())\n",
    "                    current_chunk = header_info + ' ||| ' + sub_chunk + ' || '\n",
    "                    \n",
    "    if current_chunk:\n",
    "        table_chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return table_chunks\n",
    "\n",
    "\n",
    "def scrape_and_chunk_page(driver, url):\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    chunks = []\n",
    "    current_url = url\n",
    "    last_header = \"\"\n",
    "\n",
    "    elements = soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'table'])\n",
    "    for element in elements:\n",
    "        if element.name in ['h1', 'h2', 'h3', 'h4']:\n",
    "            header_text = get_text_content(element)\n",
    "            last_header = header_text  # Store this as context for following elements\n",
    "            header_chunks = chunk_text(header_text, max_token_length)\n",
    "            chunks.extend([(chunk, current_url) for chunk in header_chunks])\n",
    "            \n",
    "        elif element.name == 'p':\n",
    "            paragraph_text = get_text_content(element)\n",
    "            paragraph_chunks = chunk_text(paragraph_text, max_token_length)\n",
    "            chunks.extend([(chunk, current_url) for chunk in paragraph_chunks])\n",
    "            \n",
    "        elif element.name == 'table':\n",
    "            table_html = StringIO(str(element))\n",
    "            df = pd.read_html(table_html)[0]\n",
    "            \n",
    "            # Drop empty rows and columns\n",
    "            df.dropna(axis=0, how='all', inplace=True)\n",
    "            df.dropna(axis=1, how='all', inplace=True)\n",
    "            \n",
    "            # Ensure column headers are strings\n",
    "            df.columns = [str(col) for col in df.columns]\n",
    "            header_info = last_header + ' | ' + ' | '.join(df.columns) if not df.columns.empty else last_header\n",
    "            \n",
    "            # Chunk the table content\n",
    "            table_chunks = chunk_table(df, max_token_length, header_info)\n",
    "            chunks.extend([(chunk, current_url) for chunk in table_chunks])\n",
    "\n",
    "    # Merge small chunks where possible\n",
    "    text_chunks = [chunk[0] for chunk in chunks]\n",
    "    final_chunks = merge_small_chunks(text_chunks, max_token_length)\n",
    "    \n",
    "    # Re-associate URLs after merging\n",
    "    return [(chunk, current_url) for chunk in final_chunks]\n",
    "\n",
    "def scrape_and_chunk(urls):\n",
    "    driver = init_driver()\n",
    "    chunks = []\n",
    "    for url in tqdm(urls, desc=\"Scraping pages\"):\n",
    "        chunks.extend(scrape_and_chunk_page(driver, url))\n",
    "    return chunks\n",
    "\n",
    "urls = [\n",
    "    \"https://en.wikipedia.org/wiki/List_of_wars_by_death_toll\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_wars:_1990%E2%80%932002\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_wars:_1945%E2%80%931989\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_wars:_1900%E2%80%931944\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_wars:_2003%E2%80%93present\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_wars:_1800%E2%80%931899\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_wars:_1500%E2%80%931799\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_wars:_1000%E2%80%931499\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_wars:_before_1000\",\n",
    "]\n",
    "\n",
    "scraped_chunks = scrape_and_chunk(urls)\n",
    "\n",
    "print(f\"Total Chunks: {len(scraped_chunks)}\")\n",
    "\n",
    "for chunk, url in scraped_chunks[:3]:\n",
    "    print(f\"Chunk: {chunk}\\nSource URL: {url}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8f543279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091be629c0c243e39fd2f7de4e9e58c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 307 relevant chunks\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient, models\n",
    "from tqdm.notebook import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "import requests\n",
    "import json\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "session = requests.Session()\n",
    "retry = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.headers.update({\"Connection\": \"keep-alive\", \"Content-Type\": \"application/json\"})\n",
    "\n",
    "qdrant_url = \"http://localhost:6333\"\n",
    "collection_name = \"wiki_collection\"\n",
    "ollama_url_gen = \"http://localhost:11434/api/generate\"\n",
    "ollama_model_name = \"llama3.2:latest\"\n",
    "\n",
    "client = QdrantClient(url=qdrant_url)\n",
    "embedding_model = SentenceTransformer(model_path)\n",
    "\n",
    "TOP_K = 5\n",
    "TOP_N = 3\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    return embedding_model.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "def create_collection(dimension):\n",
    "    client.delete_collection(collection_name=collection_name)\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(size=dimension, distance=models.Distance.COSINE),\n",
    "    )\n",
    "\n",
    "def upsert_points_with_metadata(embeddings, chunks):\n",
    "    points = [\n",
    "        models.PointStruct(\n",
    "            id=i,\n",
    "            vector=embedding.tolist(),\n",
    "            payload={\"text\": chunk, \"url\": url}\n",
    "        ) for i, (embedding, (chunk, url)) in enumerate(zip(embeddings, chunks))\n",
    "    ]\n",
    "    client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "def store_in_qdrant_with_metadata(chunks):\n",
    "    dimension = 384\n",
    "    create_collection(dimension)\n",
    "    chunk_texts = [chunk for chunk, _ in chunks]\n",
    "    embeddings = get_embeddings(chunk_texts)\n",
    "    upsert_points_with_metadata(embeddings, chunks)\n",
    "\n",
    "def search_points_with_metadata(query_text, k=TOP_K):\n",
    "    query_embedding = get_embeddings([query_text])[0]\n",
    "    search_result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding.tolist(),\n",
    "        limit=k,\n",
    "        with_payload=True\n",
    "    )\n",
    "    return [{\"text\": hit.payload[\"text\"], \"url\": hit.payload[\"url\"]} for hit in search_result]\n",
    "\n",
    "def process_streamed_response(response, buffer_size=10):\n",
    "    response_text, buffer = \"\", \"\"\n",
    "    for chunk in response.iter_content(chunk_size=None):\n",
    "        try:\n",
    "            data = json.loads(chunk.decode('utf-8'))\n",
    "            content = data.get(\"response\", \"\")\n",
    "            buffer += content\n",
    "\n",
    "            if len(buffer) >= buffer_size:\n",
    "                response_text += buffer\n",
    "                clear_output(wait=True)\n",
    "                display(Markdown(response_text))\n",
    "                buffer = \"\"\n",
    "                \n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "            \n",
    "    response_text += buffer\n",
    "    clear_output(wait=True)\n",
    "    display(Markdown(response_text))\n",
    "    return response_text\n",
    "\n",
    "def calculate_keyword_overlap_score(chunk, query_keywords):\n",
    "    chunk_words = Counter(re.findall(r'\\w+', chunk.lower()))\n",
    "    overlap_count = sum(chunk_words.get(keyword, 0) for keyword in query_keywords)\n",
    "    return overlap_count\n",
    "\n",
    "def get_top_n_chunks_by_combined_score(query_text, retrieved_docs, n=TOP_N, semantic_weight=0.7, keyword_weight=0.3):\n",
    "    query_keywords = set(re.findall(r'\\w+', query_text.lower()))\n",
    "    scored_chunks = []\n",
    "\n",
    "    for doc in retrieved_docs:\n",
    "        semantic_score = doc[\"score\"]\n",
    "        keyword_overlap_score = calculate_keyword_overlap_score(doc[\"text\"], query_keywords)\n",
    "        combined_score = (semantic_weight * semantic_score) + (keyword_weight * keyword_overlap_score)\n",
    "        scored_chunks.append({\"text\": doc[\"text\"], \"url\": doc[\"url\"], \"combined_score\": combined_score})\n",
    "\n",
    "    scored_chunks.sort(key=lambda n: n[\"combined_score\"], reverse=True)\n",
    "    print(f\"Scores: {[n['combined_score'] for n in scored_chunks]}\")\n",
    "    return scored_chunks[:n]\n",
    "\n",
    "def search_points_with_metadata(query_text, k=TOP_K, n=TOP_N, semantic_weight=0.8, keyword_weight=0.2):\n",
    "    query_embedding = get_embeddings([query_text])[0]\n",
    "    search_result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding.tolist(),\n",
    "        limit=k,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "    retrieved_docs = [{\"text\": hit.payload[\"text\"], \"url\": hit.payload[\"url\"], \"score\": hit.score} for hit in search_result]\n",
    "    return get_top_n_chunks_by_combined_score(query_text, retrieved_docs, n=n, semantic_weight=semantic_weight, keyword_weight=keyword_weight)\n",
    "\n",
    "def ask(query, k=5, n=3, verbose=False):\n",
    "    retrieved_docs = search_points_with_metadata(query, k=k, n=n)\n",
    "    combined_docs = \"\\n\\n\".join([f\"Source: {doc['url']}\\n\\n{doc['text']}\" for doc in retrieved_docs])\n",
    "    inst = (\"Instruction: If you do not find the answer within CONTEXT, please respond \"\n",
    "            \"'Answer not found in context.' Do not speculate or create information beyond what is provided.\"\n",
    "            \"'Also respond naturally, dont start with phrase like, 'according to the context' or something similar.\")\n",
    "    rag_prompt = f\"{inst}\\n\\n<CONTEXT>\\n\\n{combined_docs}\\n\\n</CONTEXT>\\n\\nQuery: {query}\\n\"\n",
    "\n",
    "    if verbose:\n",
    "        print(rag_prompt)\n",
    "        \n",
    "    payload = {\"model\": ollama_model_name, \"prompt\": rag_prompt, \"stream\": True}\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = session.post(ollama_url_gen, headers=headers, data=json.dumps(payload), stream=True)\n",
    "    response_text = process_streamed_response(response) if response.status_code == 200 else \"Request failed\"\n",
    "    \n",
    "    return response_text\n",
    "\n",
    "try:\n",
    "    store_in_qdrant_with_metadata(scraped_chunks)\n",
    "    print(f'Stored {len(scraped_chunks)} relevant chunks')\n",
    "except Exception as e:\n",
    "    print(f\"Error storing in Qdrant: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "82154565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Bangladesh Liberation War: 0.3–3 million[49][50] | 1971 | India and Provisional Government of Bangladesh vs. Pakistan | Indian subcontinent"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = ask(\"Bangladesh Liberation War data?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3eb7e384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Answer not found in context."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = ask(\"When was Federal War happened?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9d7782d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Quasi-War occurred from 1798 to 1800."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = ask(\"When did Quasi-War happend?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "13817562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Second Congo War took place in the Democratic Republic of the Congo."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = ask(\"Where did Second Congo War happend?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a653292e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Mass killings, atrocities not explicitly classified as genocides, and genocides occurring outside of wartime, human sacrifices, ethnic cleansing operations, and acts of state terrorism or political repression during peacetime."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = ask(\"What types of killings are excluded in the list?\", k=5, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "73227006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is the requested data in table format:\n",
       "\n",
       "| **Conflict** | **Started** | **Ended** | **Name of Conflict** | **Belligerents (Victorious party if applicable)** | **Belligerents (Defeated party if applicable)** |\n",
       "| --- | --- | --- | --- | --- | --- |\n",
       "| Second Intifada | 2000 | 2005 | Part of the Israeli–Palestinian conflict | Israel, Palestinian Authority, Fatah, PFLP, DFLP, Hamas, Islamic Jihad | Palestinian Authority, Fatah (al-Aqsa Martyrs' Brigades) |\n",
       "\n",
       "Note: There is only one data point for the Arab-Israeli conflict and Lebanese Civil War in the provided text."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = ask(\"Show table format all data for Arab-Israeli conflict and Lebanese Civil War.\", n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8cbaaac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the CONTEXT, the list excludes \"mass killings and atrocities\" that are not explicitly classified as genocides, such as:\n",
       "\n",
       "* Genocides occurring outside of wartime\n",
       "* Human sacrifices\n",
       "* Ethnic cleansing operations\n",
       "* Acts of state terrorism or political repression during peacetime."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = ask(\"this 'list excludes mass killings and atrocities' of what types?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
