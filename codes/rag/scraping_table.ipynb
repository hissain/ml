{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91808bb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "#model_path = '/Users/hissain/git/github/models/all-MiniLM-L6-v2'\n",
    "model_path = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, force_download=True)\n",
    "\n",
    "# Define maximum token length per chunk\n",
    "max_token_length = 350\n",
    "\n",
    "def clean(text):\n",
    "    clean_text = re.sub(r'\\[\\s*\\d+\\s*\\]', '', text)\n",
    "    return clean_text\n",
    "\n",
    "def get_text_content(element):\n",
    "    return ' '.join(str(e) for e in element.stripped_strings)\n",
    "\n",
    "def chunk_text(text, max_token_length):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_token_length, len(tokens))\n",
    "        chunk = tokenizer.decode(tokens[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "def merge_small_chunks(chunks, max_token_length):\n",
    "    merged_chunks = []\n",
    "    temp_chunk = \"\"\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if len(tokenizer.encode(temp_chunk + \" \" + chunk, add_special_tokens=False)) <= max_token_length:\n",
    "            temp_chunk += \" \" + chunk\n",
    "        else:\n",
    "            while len(tokenizer.encode(temp_chunk, add_special_tokens=False)) > max_token_length:\n",
    "                split_point = max_token_length - 1\n",
    "                merged_chunks.append(tokenizer.decode(tokenizer.encode(temp_chunk, add_special_tokens=False)[:split_point]))\n",
    "                temp_chunk = tokenizer.decode(tokenizer.encode(temp_chunk, add_special_tokens=False)[split_point:])\n",
    "                \n",
    "            merged_chunks.append(temp_chunk.strip())\n",
    "            temp_chunk = chunk\n",
    "    \n",
    "    if temp_chunk:\n",
    "        merged_chunks.append(temp_chunk.strip())\n",
    "    \n",
    "    return merged_chunks\n",
    "\n",
    "def chunk_table(df, max_token_length, header_info):\n",
    "    table_chunks = []\n",
    "    current_chunk = header_info + ' ||| '\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        row_text = ' | '.join([str(cell) for cell in row if pd.notna(cell)])\n",
    "        row_text = clean(row_text)\n",
    "        combined_text = current_chunk + row_text + ' || '\n",
    "        \n",
    "        if len(tokenizer.encode(combined_text)) <= max_token_length:\n",
    "            current_chunk += row_text + ' || '\n",
    "        else:\n",
    "            row_chunks = chunk_text(row_text, max_token_length)\n",
    "            for sub_chunk in row_chunks:\n",
    "                if len(tokenizer.encode(current_chunk)) + len(tokenizer.encode(sub_chunk)) <= max_token_length:\n",
    "                    current_chunk += sub_chunk + ' || '\n",
    "                else:\n",
    "                    table_chunks.append(current_chunk.strip())\n",
    "                    current_chunk = header_info + ' ||| ' + sub_chunk + ' || '\n",
    "                    \n",
    "    if current_chunk:\n",
    "        table_chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return table_chunks\n",
    "\n",
    "\n",
    "def scrape_and_chunk_page(content):\n",
    "\n",
    "    soup = BeautifulSoup(content[1], 'html.parser') # index-1 for html\n",
    "    \n",
    "    chunks = []\n",
    "    current_url = content[0] #index-0 for url\n",
    "    last_header = \"\"\n",
    "\n",
    "    elements = soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'table'])\n",
    "    for element in elements:\n",
    "        if element.name in ['h1', 'h2', 'h3', 'h4']:\n",
    "            header_text = get_text_content(element)\n",
    "            header_text = clean(header_text)\n",
    "            last_header = \"\\nTable (\" + header_text + \"):\"\n",
    "            header_chunks = chunk_text(header_text, max_token_length)\n",
    "            chunks.extend([(chunk, current_url) for chunk in header_chunks])\n",
    "            \n",
    "        elif element.name == 'p':\n",
    "            paragraph_text = get_text_content(element)\n",
    "            paragraph_text = clean(paragraph_text)\n",
    "            paragraph_chunks = chunk_text(paragraph_text, max_token_length)\n",
    "            chunks.extend([(chunk, current_url) for chunk in paragraph_chunks])\n",
    "            \n",
    "        elif element.name == 'table':\n",
    "            table_html = StringIO(str(element))\n",
    "            df = pd.read_html(table_html)[0]\n",
    "            \n",
    "            if df.empty:\n",
    "                continue\n",
    "            \n",
    "            df.dropna(axis=0, how='all', inplace=True)\n",
    "            df.dropna(axis=1, how='all', inplace=True)\n",
    "            \n",
    "            df.columns = [str(col) for col in df.columns]\n",
    "            header_info = last_header + ' | ' + ' | '.join(df.columns) if not df.columns.empty else last_header\n",
    "            \n",
    "            table_chunks = chunk_table(df, max_token_length, header_info)\n",
    "            chunks.extend([(chunk, current_url) for chunk in table_chunks])\n",
    "\n",
    "    text_chunks = [chunk[0] for chunk in chunks]\n",
    "    final_chunks = merge_small_chunks(text_chunks, max_token_length)\n",
    "    \n",
    "    return [(chunk, current_url) for chunk in final_chunks]\n",
    "\n",
    "def scrape_and_chunk(html_contents):\n",
    "    chunks = []\n",
    "    for content in tqdm(html_contents, desc=\"Scraping pages\"):\n",
    "        chunks.extend(scrape_and_chunk_page(content))\n",
    "    return chunks\n",
    "\n",
    "with open(\"html_contents.pkl\", \"rb\") as f:\n",
    "    html_contents = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(html_contents)} URLs from pickle file\")\n",
    "scraped_chunks = scrape_and_chunk(html_contents)\n",
    "\n",
    "print(f\"Total Chunks: {len(scraped_chunks)}\")\n",
    "\n",
    "for chunk, url in scraped_chunks[:2]:\n",
    "    print(f\"Chunk: {chunk}\\nSource URL: {url}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f543279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient, models\n",
    "from tqdm.notebook import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Load SpaCy's English model for Named Entity Recognition\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "session = requests.Session()\n",
    "retry = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.headers.update({\"Connection\": \"keep-alive\", \"Content-Type\": \"application/json\"})\n",
    "\n",
    "qdrant_url = \"http://localhost:6333\"\n",
    "collection_name = \"wiki_collection\"\n",
    "ollama_url_gen = \"http://localhost:11434/api/generate\"\n",
    "ollama_model_name = \"llama3.2:latest\"\n",
    "\n",
    "client = QdrantClient(url=qdrant_url)\n",
    "\n",
    "model_path_st = '/Users/hissain/git/github/models/all-MiniLM-L6-v2'\n",
    "embedding_model = SentenceTransformer(model_path_st)\n",
    "\n",
    "TOP_K = 10\n",
    "TOP_N = 4\n",
    "SYM_W = 0.8\n",
    "SYN_W = 0.2\n",
    "NE_BOOST_FACTOR = 1.2\n",
    "NE_FULL_BOOST_FACTOR = 1.2\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    return embedding_model.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "def create_collection(dimension):\n",
    "    client.delete_collection(collection_name=collection_name)\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(size=dimension, distance=models.Distance.COSINE),\n",
    "    )\n",
    "\n",
    "def upsert_points_with_metadata(embeddings, chunks, batch_size=100):\n",
    "    points = [\n",
    "        models.PointStruct(\n",
    "            id=i,\n",
    "            vector=embedding.tolist(),\n",
    "            payload={\"text\": chunk, \"url\": url}\n",
    "        ) for i, (embedding, (chunk, url)) in enumerate(zip(embeddings, chunks))\n",
    "    ]\n",
    "\n",
    "    for i in tqdm(range(0, len(points), batch_size), desc=\"Storing to Qdrant\"):\n",
    "        batch_points = points[i:i + batch_size]\n",
    "        client.upsert(collection_name=collection_name, points=batch_points)\n",
    "        time.sleep(0.01)\n",
    "\n",
    "def store_in_qdrant_with_metadata(chunks):\n",
    "    dimension = 384\n",
    "    create_collection(dimension)\n",
    "    chunk_texts = [chunk for chunk, _ in chunks]\n",
    "    embeddings = get_embeddings(chunk_texts)\n",
    "    upsert_points_with_metadata(embeddings, chunks)\n",
    "\n",
    "def search_points_with_metadata(query_text, k=TOP_K):\n",
    "    query_embedding = get_embeddings([query_text])[0]\n",
    "    search_result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding.tolist(),\n",
    "        limit=k,\n",
    "        with_payload=True\n",
    "    )\n",
    "    return [{\"text\": hit.payload[\"text\"], \"url\": hit.payload[\"url\"], \"score\": hit.score} for hit in search_result]\n",
    "\n",
    "def init_bm25(corpus_texts):\n",
    "    tokenized_corpus = [text.split() for text in corpus_texts]\n",
    "    return BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def calculate_bm25_scores(bm25, query_text):\n",
    "    tokenized_query = query_text.split()\n",
    "    return bm25.get_scores(tokenized_query)\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents]\n",
    "\n",
    "def boost_ne_scores(query_text, docs, bm25_scores, boost_factor=NE_BOOST_FACTOR, full_match_boost=NE_FULL_BOOST_FACTOR):\n",
    "    query_entities = extract_named_entities(query_text)\n",
    "    print(f\"Query Named Entities: {query_entities}\")\n",
    "    \n",
    "    boosted_scores = []\n",
    "    for idx, (doc, bm25_score) in enumerate(zip(docs, bm25_scores)):\n",
    "        doc_entities = extract_named_entities(doc[\"text\"])\n",
    "        matching_ne_count = sum(1 for ne in query_entities if ne in doc_entities)\n",
    "        full_match = all(ne in doc_entities for ne in query_entities)\n",
    "        ne_boost = 1 + (boost_factor * matching_ne_count)\n",
    "        if full_match:\n",
    "            ne_boost *= full_match_boost\n",
    "        boosted_scores.append(bm25_score * ne_boost)\n",
    "\n",
    "    print(f\"First-4 Boosted scores: {boosted_scores[:4]}\")          \n",
    "    return boosted_scores\n",
    "\n",
    "def get_top_n_chunks_by_combined_score(query_text, retrieved_docs, n=TOP_N, semantic_weight=SYM_W, keyword_weight=SYN_W):\n",
    "    \n",
    "    bm25 = init_bm25([doc[\"text\"] for doc in retrieved_docs])\n",
    "    bm25_scores = calculate_bm25_scores(bm25, query_text)\n",
    "    boosted_keyword_scores = boost_ne_scores(query_text, retrieved_docs, bm25_scores)\n",
    "\n",
    "    scored_chunks = []\n",
    "    \n",
    "    for idx, doc in enumerate(retrieved_docs):\n",
    "        semantic_score = doc[\"score\"]\n",
    "        keyword_score = boosted_keyword_scores[idx]\n",
    "        combined_score = (semantic_weight * semantic_score) + (keyword_weight * keyword_score)\n",
    "        scored_chunks.append({\"text\": doc[\"text\"], \"url\": doc[\"url\"], \"combined_score\": combined_score})\n",
    "\n",
    "    scored_chunks.sort(key=lambda n: n[\"combined_score\"], reverse=True)\n",
    "    print(f\"Top-4 Combined scores: {[s['combined_score'] for s in scored_chunks[:4]]}\")\n",
    "    return scored_chunks[:n]\n",
    "\n",
    "def search_points_with_metadata(query_text, k=TOP_K, n=TOP_N, semantic_weight=SYM_W, keyword_weight=SYN_W):\n",
    "    query_embedding = get_embeddings([query_text])[0]\n",
    "    search_result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding.tolist(),\n",
    "        limit=k,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "    retrieved_docs = [{\"text\": hit.payload[\"text\"], \"url\": hit.payload[\"url\"], \"score\": hit.score} for hit in search_result]\n",
    "    \n",
    "    return get_top_n_chunks_by_combined_score(query_text, retrieved_docs, n=n, semantic_weight=semantic_weight, keyword_weight=keyword_weight)\n",
    "\n",
    "def process_streamed_response(response, buffer_size=5):\n",
    "    response_text, buffer = \"\", \"\"\n",
    "    for chunk in response.iter_content(chunk_size=None):\n",
    "        try:\n",
    "            data = json.loads(chunk.decode('utf-8'))\n",
    "            content = data.get(\"response\", \"\")\n",
    "            buffer += content\n",
    "\n",
    "            if len(buffer) >= buffer_size:\n",
    "                response_text += buffer\n",
    "                clear_output(wait=True)\n",
    "                display(Markdown(response_text))\n",
    "                buffer = \"\"\n",
    "                \n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "            \n",
    "    response_text += buffer\n",
    "    clear_output(wait=True)\n",
    "    display(Markdown(response_text))\n",
    "    return response_text\n",
    "\n",
    "def inspect(query, k=TOP_K, n=TOP_N):\n",
    "    retrieved_docs = search_points_with_metadata(query, k=k, n=n)\n",
    "    combined_docs = \"\\n\\n\".join([f\"Source: {doc['url']}\\n\\n{doc['text']}\" for doc in retrieved_docs])\n",
    "    rag_prompt = f\"Documents:\\n\\n<context>\\n\\n{combined_docs}\\n\\n</context>\\n\\nQuestion: {query}\\n\\nAnswer:\\n\"\n",
    "    print(rag_prompt)\n",
    "\n",
    "def ask(query, k=TOP_K, n=TOP_N):\n",
    "    retrieved_docs = search_points_with_metadata(query, k=k, n=n)\n",
    "    combined_docs = \"\\n\\n\".join([f\"Source: {doc['url']}\\n\\n{doc['text']}\" for doc in retrieved_docs])\n",
    "    inst = (\"Instruction: Please answer the following question based on following context.\"\n",
    "            \"If you do not find the answer within the following context, please respond,\"\n",
    "            \"'Answer not found in the context.' without speculation or general knowledge.\"\n",
    "            \"'Do not start with phrase like, 'according to the context', or anything similar.\")\n",
    "    rag_prompt = f\"{inst}\\n\\n<context>\\n\\n{combined_docs}\\n\\n</context>\\n\\nQuestion: {query}\\n\\nAnswer:\\n\"\n",
    "    payload = {\"model\": ollama_model_name, \"prompt\": rag_prompt, \"stream\": True}\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = session.post(ollama_url_gen, headers=headers, data=json.dumps(payload), stream=True)\n",
    "    response_text = process_streamed_response(response) if response.status_code == 200 else \"Request failed\"\n",
    "    return response_text\n",
    "    \n",
    "try:\n",
    "    store_in_qdrant_with_metadata(scraped_chunks)\n",
    "    print(f'Stored {len(scraped_chunks)} relevant chunks')\n",
    "except Exception as e:\n",
    "    print(f\"Error storing in Qdrant: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(q):\n",
    "    return inspect(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5ba643",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ask(\"When was Bangladesh Liberation War happened?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82154565",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ask(\"How many died in Bangladesh Liberation War?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb7e384",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = ask(\"When was Federal War happened?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7782d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ask(\"When did Quasi-War happend?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13817562",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ask(\"Where did Second Congo War happend?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ask(\"What types of killings are excluded in the list of war by death toll?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa182603",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ask(\"Which war started in 1945 ended in 1949?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a809a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ask(\"Ethiopian Empire vs. Emirate of Harar?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
