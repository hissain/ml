{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3b056b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Total Reward: -15, Epsilon: 0.6057704364907278\n",
      "Episode 200, Total Reward: 1, Epsilon: 0.3669578217261671\n",
      "Episode 300, Total Reward: 3, Epsilon: 0.22229219984074702\n",
      "Episode 400, Total Reward: 3, Epsilon: 0.1346580429260134\n",
      "Episode 500, Total Reward: -1, Epsilon: 0.1\n",
      "Episode 600, Total Reward: 3, Epsilon: 0.1\n",
      "Episode 700, Total Reward: 1, Epsilon: 0.1\n",
      "Episode 800, Total Reward: 3, Epsilon: 0.1\n",
      "Episode 900, Total Reward: 1, Epsilon: 0.1\n",
      "Episode 1000, Total Reward: 3, Epsilon: 0.1\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "    def __init__(self, n=5, start_pos=(0, 0), goal_pos=None, block_percentage=5):\n",
    "        super(GridWorldEnv, self).__init__()\n",
    "        \n",
    "        self.n = n\n",
    "        self.grid_size = (n, n)\n",
    "        \n",
    "        # Define the action space: 0 = left, 1 = right, 2 = up, 3 = down\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        # Define the observation space, which is the position of the agent on the grid\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(n),\n",
    "            spaces.Discrete(n)\n",
    "        ))\n",
    "        \n",
    "        # Set the starting position of the agent\n",
    "        self.start_pos = start_pos\n",
    "        self.current_pos = self.start_pos\n",
    "        \n",
    "        # Set the goal position\n",
    "        self.goal_pos = goal_pos if goal_pos else (n-1, n-1)\n",
    "        \n",
    "        # Set the blocked cells, generate random blocked cells\n",
    "        num_cells = n * n\n",
    "        num_blocked = int((block_percentage / 100) * num_cells)\n",
    "        \n",
    "        self.blocked_cells = []\n",
    "        while len(self.blocked_cells) < num_blocked:\n",
    "            cell = (random.randint(0, n-1), random.randint(0, n-1))\n",
    "            if cell != self.start_pos and cell != self.goal_pos and cell not in self.blocked_cells:\n",
    "                self.blocked_cells.append(cell)\n",
    "        \n",
    "        # Create grid with obstacles\n",
    "        self.grid = np.zeros(self.grid_size)\n",
    "        for cell in self.blocked_cells:\n",
    "            self.grid[cell] = -1  # Mark blocked cells as -1\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset agent's position to start\n",
    "        self.current_pos = self.start_pos\n",
    "        return self.current_pos\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.current_pos\n",
    "\n",
    "        # Take action based on the agent's current position\n",
    "        if action == 0:  # Left\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 1:  # Right\n",
    "            y = min(self.n - 1, y + 1)\n",
    "        elif action == 2:  # Up\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 3:  # Down\n",
    "            x = min(self.n - 1, x + 1)\n",
    "        \n",
    "        next_pos = (x, y)\n",
    "\n",
    "        # Check if the next position is blocked\n",
    "        if next_pos in self.blocked_cells:\n",
    "            next_pos = self.current_pos  # Stay in the same position if blocked\n",
    "\n",
    "        # Set the new current position\n",
    "        self.current_pos = next_pos\n",
    "\n",
    "        # Check if the agent reached the goal\n",
    "        if self.current_pos == self.goal_pos:\n",
    "            reward = 10  # Reward for reaching the goal\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1  # Small penalty for each step taken\n",
    "            done = False\n",
    "\n",
    "        return self.current_pos, reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        grid = np.copy(self.grid)\n",
    "\n",
    "        # Mark the agent's position\n",
    "        grid[self.current_pos] = 1\n",
    "\n",
    "        # Mark the goal position\n",
    "        grid[self.goal_pos] = 2\n",
    "\n",
    "        # Display the grid as an image\n",
    "        cmap = colors.ListedColormap(['white', 'blue', 'red', 'black'])\n",
    "        bounds = [-1, 0, 1, 2, 3]\n",
    "        norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(grid, cmap=cmap, norm=norm)\n",
    "\n",
    "        # Remove default grid lines and ticks\n",
    "        ax.set_xticks(np.arange(self.n) + 0.5, minor=True)\n",
    "        ax.set_yticks(np.arange(self.n) + 0.5, minor=True)\n",
    "        ax.grid(which=\"minor\", color=\"black\", linestyle='-', linewidth=2)  # Square boundaries\n",
    "\n",
    "        ax.set_xticks([])  # Remove tick marks\n",
    "        ax.set_yticks([])  # Remove tick marks\n",
    "\n",
    "        # Render agent, goal, and blocked cells in the center of the squares\n",
    "        for i in range(self.n):\n",
    "            for j in range(self.n):\n",
    "                if (i, j) == self.current_pos:\n",
    "                    ax.text(j, i, 'A', ha='center', va='center', fontsize=12, color='blue')\n",
    "                elif (i, j) == self.goal_pos:\n",
    "                    ax.text(j, i, 'G', ha='center', va='center', fontsize=12, color='red')\n",
    "                elif (i, j) in self.blocked_cells:\n",
    "                    ax.text(j, i, 'X', ha='center', va='center', fontsize=12, color='black')\n",
    "\n",
    "        plt.title(f\"Agent at {self.current_pos}, Goal at {self.goal_pos}\")\n",
    "        display(plt.gcf())\n",
    "        clear_output(wait=True)\n",
    "        time.sleep(0.3)\n",
    "        plt.close()\n",
    "\n",
    "    def show_final(self):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(self.grid, cmap=colors.ListedColormap(['white', 'blue', 'red', 'black']),\n",
    "                  norm=colors.BoundaryNorm([-1, 0, 1, 2, 3], 4))\n",
    "\n",
    "        # Remove default grid lines and ticks\n",
    "        ax.set_xticks(np.arange(self.n) + 0.5, minor=True)\n",
    "        ax.set_yticks(np.arange(self.n) + 0.5, minor=True)\n",
    "        ax.grid(which=\"minor\", color=\"black\", linestyle='-', linewidth=2)  # Square boundaries\n",
    "\n",
    "        ax.set_xticks([])  # Remove tick marks\n",
    "        ax.set_yticks([])  # Remove tick marks\n",
    "\n",
    "        # Render agent, start, goal, and blocked cells in the center of the squares\n",
    "        for i in range(self.n):\n",
    "            for j in range(self.n):\n",
    "                if (i, j) == self.current_pos:\n",
    "                    ax.text(j, i, 'A', ha='center', va='center', fontsize=12, color='blue')\n",
    "                elif (i, j) == self.start_pos:\n",
    "                    ax.text(j, i, 'S', ha='center', va='center', fontsize=12, color='green')  # Starting point\n",
    "                elif (i, j) == self.goal_pos:\n",
    "                    ax.text(j, i, 'G', ha='center', va='center', fontsize=12, color='red')  # Goal point\n",
    "                elif (i, j) in self.blocked_cells:\n",
    "                    ax.text(j, i, 'X', ha='center', va='center', fontsize=12, color='black')\n",
    "\n",
    "        plt.title(f\"Agent at {self.current_pos}, Goal at {self.goal_pos}\")\n",
    "        plt.show()\n",
    "\n",
    "# Define policy\n",
    "def run_policy(env, q_table):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    env.render()\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state[0], state[1]])\n",
    "        state, _, done, _ = env.step(action)\n",
    "        env.render()\n",
    "    env.show_final()\n",
    "\n",
    "# Define grid size and blocked cells percentage\n",
    "n = 8\n",
    "start_pos = (0, 0)\n",
    "goal_pos = (n/2, n/2)\n",
    "block_percentage = 20\n",
    "\n",
    "env = GridWorldEnv(n=n, start_pos=start_pos, goal_pos=goal_pos, block_percentage=block_percentage)\n",
    "\n",
    "# Q-learning parameters\n",
    "q_table = np.zeros((n, n, env.action_space.n))\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.995\n",
    "num_episodes = 1000\n",
    "max_steps = 100\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state[0], state[1]])  # Exploit learned values\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Update Q-value\n",
    "        q_table[state[0], state[1], action] = q_table[state[0], state[1], action] + alpha * (\n",
    "            reward + gamma * np.max(q_table[next_state[0], next_state[1]]) - q_table[state[0], state[1], action]\n",
    "        )\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon_min, epsilon_decay * epsilon)\n",
    "\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4eb759b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test the trained agent\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m run_policy(env, q_table)\n",
      "Cell \u001b[0;32mIn[65], line 158\u001b[0m, in \u001b[0;36mrun_policy\u001b[0;34m(env, q_table)\u001b[0m\n\u001b[1;32m    156\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    157\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    160\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(q_table[state[\u001b[38;5;241m0\u001b[39m], state[\u001b[38;5;241m1\u001b[39m]])\n",
      "Cell \u001b[0;32mIn[65], line 92\u001b[0m, in \u001b[0;36mGridWorldEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     89\u001b[0m grid[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_pos] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Mark the goal position\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m grid[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal_pos] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Display the grid as an image\u001b[39;00m\n\u001b[1;32m     95\u001b[0m cmap \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mListedColormap([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "run_policy(env, q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
