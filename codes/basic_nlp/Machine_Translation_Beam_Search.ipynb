{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hissain/mlworks/blob/main/codes/Machine_Translation_Beam_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfw_vcO7mArp"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhDzwNC_cSPU"
      },
      "source": [
        "This notebook is prepared from [this source](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/032d653a4f5a9c1ec32b9fc7c989ffe1/seq2seq_translation_tutorial.ipynb#scrollTo=cE7uNTa2SEpW)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Bblp--hb6JC"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuOQIUqLcG7e"
      },
      "source": [
        "Here, two recurrent neural networks work together to transform one sequence to another. An encoder network condenses an input sequence into a vector, and a decoder network unfolds that vector into a new sequence.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/seq2seq.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mid-76LRgz1N"
      },
      "source": [
        "Here, we will be representing each word in a language as a one-hot\n",
        "vector, or giant vector of zeros except for a single one (at the index\n",
        "of the word). Compared to the dozens of characters that might exist in a\n",
        "language, there are many many more words, so the encoding vector is much\n",
        "larger. We will however cheat a bit and trim the data to only use a few\n",
        "thousand words per language.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/word-encoding.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJmW6pbGmDWo"
      },
      "source": [
        "### Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs3BCOH6OcbJ"
      },
      "outputs": [],
      "source": [
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cldgjcm1bCfp"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVo8HmoiSFpo",
        "outputId": "5d6e3a23-96de-45ba-f662-eb989a9f6165"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE0vfMK4l3GL"
      },
      "source": [
        "### Dataset creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-3d8lOcdBRF"
      },
      "source": [
        "Download the data from [here](https://drive.google.com/file/d/12eWXVWwr3XL96w-CuKlzPfuWIuW_PvGi/view?usp=sharing). This dataset is collected from [link](http://www.manythings.org/anki/ben-eng.zip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-qTtit_8yL8"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_-77bflmx_s"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ij67S9aRhQRH"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "        self.word2index = {\"SOS\":0, \"EOS\":1}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "\n",
        "        self.word2count = {}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.index2word[self.n_words] = word\n",
        "\n",
        "            self.word2count[word] = 1\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def get_tokens(self, sentences):\n",
        "        tokens = []\n",
        "        for sentence in sentences:\n",
        "            sentence_token = [EOS_token] * MAX_LENGTH\n",
        "            for i, word in enumerate(sentence.split()):\n",
        "                if i >= MAX_LENGTH:\n",
        "                    break\n",
        "                sentence_token[i] = self.word2index[word]\n",
        "            tokens.append(torch.tensor(sentence_token))\n",
        "        return torch.stack(tokens, dim=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K6X--AMiwZc"
      },
      "outputs": [],
      "source": [
        "# Lowercase, trim, and remove non-letter characters\n",
        "def preprocess_text(s):\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"([।!?])\", r\" \\1\", s)\n",
        "    return s.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nghizyILjHWc"
      },
      "outputs": [],
      "source": [
        "def read(lang1, lang2):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open(f'{lang1}-{lang2}.txt', encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[preprocess_text(s) for s in l.split('\\t')[:2]] for l in lines]\n",
        "\n",
        "    input_lang = Tokenizer(lang1)\n",
        "    output_lang = Tokenizer(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "lrP1xh_Njx7W",
        "outputId": "d59eb15f-ada0-415e-f249-8e5d728527ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 6509 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "en 3165\n",
            "bn 4471\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    input    output\n",
              "0    Go .     যাও ।\n",
              "1    Go .     যান ।\n",
              "2    Go .      যা ।\n",
              "3  Run  !  পালাও  !\n",
              "4  Run  !  পালান  !"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e6c7aaeb-fe58-40e1-b52a-f8ced598d57a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go .</td>\n",
              "      <td>যাও ।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Go .</td>\n",
              "      <td>যান ।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Go .</td>\n",
              "      <td>যা ।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Run  !</td>\n",
              "      <td>পালাও  !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Run  !</td>\n",
              "      <td>পালান  !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e6c7aaeb-fe58-40e1-b52a-f8ced598d57a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e6c7aaeb-fe58-40e1-b52a-f8ced598d57a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e6c7aaeb-fe58-40e1-b52a-f8ced598d57a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ec2c5ab9-7ef6-47c3-8cf7-be8cc2fe611e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ec2c5ab9-7ef6-47c3-8cf7-be8cc2fe611e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ec2c5ab9-7ef6-47c3-8cf7-be8cc2fe611e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pairs",
              "summary": "{\n  \"name\": \"pairs\",\n  \"rows\": 6509,\n  \"fields\": [\n    {\n      \"column\": \"input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4970,\n        \"samples\": [\n          \"I can manage it .\",\n          \"I'm looking for a room for rent .\",\n          \"The Olympic Games are held every four years .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5972,\n        \"samples\": [\n          \"\\u0986\\u09ae\\u09b0\\u09be \\u0986\\u09aa\\u09a8\\u09be\\u09b0 \\u0995\\u09a5\\u09be\\u0987 \\u09ac\\u09b2\\u099b\\u09bf\\u09b2\\u09be\\u09ae \\u0964\",\n          \"\\u099f\\u09ae\\u0995\\u09c7 \\u0996\\u09c1\\u09ac \\u098f\\u0995\\u099f\\u09be \\u0996\\u09c1\\u09b6\\u09bf \\u09a6\\u09c7\\u0996\\u09be\\u099a\\u09cd\\u099b\\u09c7 \\u09a8\\u09be, \\u09a4\\u09be\\u0987 \\u09a8\\u09be  ?\",\n          \"\\u0986\\u09ae\\u09bf \\u098f\\u09ac\\u09be\\u09b0 \\u09ac\\u09c7\\u09b0\\u09cb\\u09ac\\u09cb \\u0964\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "def prepare_data(lang1, lang2):\n",
        "    input_lang, output_lang, pairs = read(lang1, lang2)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.add_sentence(pair[0])\n",
        "        output_lang.add_sentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    dataset = pd.DataFrame(data=pairs, columns=['input', 'output'])\n",
        "    # dataset['output'] = dataset['output'].apply(lambda x: \"SOS \" + x + \" EOS\")\n",
        "    return input_lang, output_lang, dataset\n",
        "\n",
        "input_lang, output_lang, pairs = prepare_data('en', 'bn')\n",
        "pairs.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl9hZcc-CfoI",
        "outputId": "8c0d794c-44a1-4dc7-fe0f-c3c3cdf5ddfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 20])\n",
            "tensor([[ 2,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1],\n",
            "        [ 2,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1],\n",
            "        [ 2,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1],\n",
            "        [ 4,  6,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1],\n",
            "        [ 4,  6,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1],\n",
            "        [ 7,  8,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1],\n",
            "        [ 9,  6,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1],\n",
            "        [10,  6,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1]])\n"
          ]
        }
      ],
      "source": [
        "inputs = ['Go .', 'Go .', 'Go .', 'Run  !', 'Run  !', 'Who  ?', 'Wow  !', 'Fire  !']\n",
        "inputs = input_lang.get_tokens(inputs)\n",
        "print(inputs.size())\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm9gfLX1l0rF"
      },
      "source": [
        "### Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0em7VJ8PdSK"
      },
      "outputs": [],
      "source": [
        "# Define the Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, input_seqs, hidden):\n",
        "        # print(input_seqs.size())\n",
        "        embedded = self.embedding(input_seqs)\n",
        "        # print(embedded.size())\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teBLDSJNmpfH"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, num_layers):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs, decoder_hidden\n",
        "\n",
        "    def forward_step(self, input, hidden):\n",
        "        output = F.relu(self.embedding(input))\n",
        "        output, hidden = self.rnn(output, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVANMpxt55Ct"
      },
      "outputs": [],
      "source": [
        "# Define the Seq2Seq Model\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seqs, target_seqs=None):\n",
        "        batch_size = input_seqs.size(0)\n",
        "        target_vocab_size = self.decoder.out.out_features\n",
        "\n",
        "        encoder_hidden = self.encoder.init_hidden(batch_size)\n",
        "        encoder_output, encoder_hidden = self.encoder(input_seqs, encoder_hidden)\n",
        "\n",
        "        decoder_output, decoder_hidden = self.decoder(\n",
        "            encoder_output,\n",
        "            encoder_hidden,\n",
        "            target_seqs\n",
        "        )\n",
        "\n",
        "        return decoder_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pdgk7EpnPvxv"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "input_size = input_lang.n_words\n",
        "output_size = output_lang.n_words\n",
        "hidden_size = 256\n",
        "num_layers = 8\n",
        "batch_size = 512\n",
        "\n",
        "encoder = Encoder(input_size, hidden_size, num_layers)\n",
        "decoder = Decoder(hidden_size, output_size, num_layers)\n",
        "\n",
        "# Move model to device\n",
        "model = Seq2Seq(encoder, decoder).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPLFoHduPySS"
      },
      "outputs": [],
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5_ky7W5KQKq"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 1\n",
        "model.train()\n",
        "for epoch in range(N_EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    for i in range(len(pairs) // batch_size + 1):\n",
        "        inputs = pairs.iloc[(i)*batch_size:(i+1)*batch_size, 0]\n",
        "        targets = pairs.iloc[(i)*batch_size:(i+1)*batch_size, 1]\n",
        "\n",
        "        if len(inputs) == 0:\n",
        "            break\n",
        "        # print(inputs.tolist(), targets.tolist())\n",
        "        # inputs = [input_lang.get_one_hot_sentence(input.split()) for input in inputs]\n",
        "        # inputs = torch.row_stack(inputs)\n",
        "        inputs = input_lang.get_tokens(inputs.tolist()).to(device)\n",
        "        targets = output_lang.get_tokens(targets.tolist()).to(device)\n",
        "\n",
        "        # print(inputs.view(-1).size())\n",
        "        # print(targets.size())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(inputs, targets)\n",
        "        # predictions = torch.argmax(predictions, dim=2)\n",
        "\n",
        "        # print(predictions.size(), targets.size())\n",
        "        target_one_hot = F.one_hot(targets, output_lang.n_words)\n",
        "\n",
        "        loss = criterion(predictions.float().flatten(), target_one_hot.float().flatten())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f'Epoch: {epoch+1:02} | Train Loss: {epoch_loss:.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgBfvrQPG0id",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "354a7ec8-6eff-4c63-dcaf-5286d144b463"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "def evaluate(model, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        inputs = input_lang.get_tokens([sentence]).to(device)\n",
        "        predictions = model(inputs)\n",
        "        predictions = torch.argmax(predictions, dim=2)[0]\n",
        "\n",
        "        output = \"\"\n",
        "        for prediction in predictions:\n",
        "            word = output_lang.index2word[prediction.item()]\n",
        "            if word == \"EOS\":\n",
        "                break\n",
        "            output += \" \" + word\n",
        "        return output\n",
        "\n",
        "\n",
        "evaluate(model, \"I love Bangladesh .\", input_lang, output_lang)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuAgCfWhZoSR"
      },
      "source": [
        "### Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(model, sentence, beam_width, input_lang, output_lang):\n",
        "    BEAM_WIDTH = beam_width\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs = input_lang.get_tokens([sentence]).to(device)\n",
        "\n",
        "        batch_size = 1\n",
        "\n",
        "        encoder_hidden = model.encoder.init_hidden(batch_size)\n",
        "        encoder_output, encoder_hidden = model.encoder(inputs, encoder_hidden)\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        # Initialize the beam with the start state\n",
        "        beam = [('SOS', '', 0)]\n",
        "        finished_beam = []\n",
        "\n",
        "        # Iterate until we reach the maximum length or run out of candidates\n",
        "        for t in range(MAX_LENGTH):\n",
        "            candidates = []\n",
        "\n",
        "            # Generate new candidates by expanding each current candidate\n",
        "            for state, seq, score in beam:\n",
        "                decoder_input = torch.tensor([[output_lang.word2index[state]]] * batch_size, dtype=torch.long).to(device)\n",
        "\n",
        "                decoder_output, decoder_hidden = model.decoder.forward_step(decoder_input, decoder_hidden)\n",
        "                decoder_output = torch.log_softmax(decoder_output, dim=-1)\n",
        "\n",
        "                topk_probs, topk_ids = torch.topk(decoder_output, beam_width, dim=-1)\n",
        "\n",
        "                topk_probs = topk_probs.flatten().tolist()\n",
        "                topk_ids = topk_ids.flatten().tolist()\n",
        "\n",
        "                for i in range(beam_width):\n",
        "                    new_state = output_lang.index2word[topk_ids[i]]\n",
        "\n",
        "                    new_seq = seq + ' ' + new_state\n",
        "                    new_score = score + topk_probs[i]\n",
        "\n",
        "                    if new_state == \"EOS\":\n",
        "                        finished_beam.append((new_state, seq, score))\n",
        "                    else:\n",
        "                        candidates.append((new_state, new_seq, new_score))\n",
        "\n",
        "            # Select the top `beam_width` candidates based on their scores\n",
        "            beam = sorted(candidates, key=lambda x: x[2], reverse=True)[:beam_width]\n",
        "\n",
        "            if len(finished_beam) == BEAM_WIDTH:\n",
        "                return finished_beam\n",
        "            elif len(finished_beam) > BEAM_WIDTH:\n",
        "                return sorted(finished_beam, key=lambda x: x[2], reverse=True)[:BEAM_WIDTH]\n",
        "            else:\n",
        "                beam_width = BEAM_WIDTH - len(finished_beam)\n",
        "\n",
        "        finished_beam.extend(beam)\n",
        "        # Return the sequence with the highest score\n",
        "        return finished_beam"
      ],
      "metadata": {
        "id": "x6fokkVh9CCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beam_search(model, \"Go .\", 3, input_lang, output_lang)"
      ],
      "metadata": {
        "id": "Mn2pldF8pg9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6XxDJyHvp4bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "13Dfluu7i43z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}