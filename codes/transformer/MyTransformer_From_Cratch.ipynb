{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d1579a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9de0f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "48650e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2962, -0.7657,  1.6776,  0.1946,  1.3157,  0.5141,  0.6003,\n",
       "           1.3683, -1.2092, -2.0529]],\n",
       "\n",
       "        [[ 1.2193,  1.4391, -0.5128, -1.2088, -0.1759, -0.7791,  0.6225,\n",
       "           0.3070,  0.9709,  0.3641]],\n",
       "\n",
       "        [[-1.2436, -1.4334,  1.2185, -0.1228,  1.0417, -1.4216,  0.2985,\n",
       "           1.7465, -1.1521, -0.7095]],\n",
       "\n",
       "        [[ 1.2193,  1.4391, -0.5128, -1.2088, -0.1759, -0.7791,  0.6225,\n",
       "           0.3070,  0.9709,  0.3641]],\n",
       "\n",
       "        [[-1.2436, -1.4334,  1.2185, -0.1228,  1.0417, -1.4216,  0.2985,\n",
       "           1.7465, -1.1521, -0.7095]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = Embedding(5, 10)\n",
    "x = torch.randint(0, 5, (5, 1), dtype=torch.long)\n",
    "print(x)\n",
    "emb(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a14df772",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embed_dim = embed_size\n",
    "        pe = torch.zeros(max_len, embed_size)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * -(math.log(10000.0) / embed_size))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * math.sqrt(self.embed_dim)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "19be0b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 3],\n",
      "        [1, 0],\n",
      "        [3, 3],\n",
      "        [4, 4],\n",
      "        [4, 0]])\n",
      "tensor([[[ 0.2962, -0.7657,  1.6776,  0.1946,  1.3157,  0.5141,  0.6003,\n",
      "           1.3683, -1.2092, -2.0529],\n",
      "         [ 1.1632, -1.7874,  0.3352,  2.2159,  2.7371, -0.1069,  1.1971,\n",
      "          -1.1965,  0.1703,  1.5804]],\n",
      "\n",
      "        [[ 1.2193,  1.4391, -0.5128, -1.2088, -0.1759, -0.7791,  0.6225,\n",
      "           0.3070,  0.9709,  0.3641],\n",
      "         [ 0.2013,  0.6307, -0.2933,  0.7859, -2.3128,  0.0748,  0.3854,\n",
      "          -0.9105, -1.2003, -1.8793]],\n",
      "\n",
      "        [[ 1.1632, -1.7874,  0.3352,  2.2159,  2.7371, -0.1069,  1.1971,\n",
      "          -1.1965,  0.1703,  1.5804],\n",
      "         [ 1.1632, -1.7874,  0.3352,  2.2159,  2.7371, -0.1069,  1.1971,\n",
      "          -1.1965,  0.1703,  1.5804]],\n",
      "\n",
      "        [[ 0.2962, -0.7657,  1.6776,  0.1946,  1.3157,  0.5141,  0.6003,\n",
      "           1.3683, -1.2092, -2.0529],\n",
      "         [ 0.2962, -0.7657,  1.6776,  0.1946,  1.3157,  0.5141,  0.6003,\n",
      "           1.3683, -1.2092, -2.0529]],\n",
      "\n",
      "        [[ 0.2962, -0.7657,  1.6776,  0.1946,  1.3157,  0.5141,  0.6003,\n",
      "           1.3683, -1.2092, -2.0529],\n",
      "         [ 0.2013,  0.6307, -0.2933,  0.7859, -2.3128,  0.0748,  0.3854,\n",
      "          -0.9105, -1.2003, -1.8793]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9366, -1.4215,  5.3050,  1.6153,  4.1606,  2.6257,  1.8984,\n",
       "           5.3270, -3.8239, -5.4918],\n",
       "         [ 4.5198, -5.1118,  1.2178,  7.9948,  8.6807,  0.6616,  3.7896,\n",
       "          -2.7836,  0.5393,  5.9977]],\n",
       "\n",
       "        [[ 3.8558,  5.5510, -1.6218, -2.8227, -0.5562, -1.4637,  1.9686,\n",
       "           1.9707,  3.0704,  2.1513],\n",
       "         [ 1.4779,  2.5348, -0.7698,  3.4727, -7.2887,  1.2362,  1.2227,\n",
       "          -1.8794, -3.7951, -4.9428]],\n",
       "\n",
       "        [[ 3.6784, -4.6521,  1.0600,  8.0073,  8.6556,  0.6619,  3.7856,\n",
       "          -2.7836,  0.5387,  5.9977],\n",
       "         [ 4.5198, -5.1118,  1.2178,  7.9948,  8.6807,  0.6616,  3.7896,\n",
       "          -2.7836,  0.5393,  5.9977]],\n",
       "\n",
       "        [[ 0.9366, -1.4215,  5.3050,  1.6153,  4.1606,  2.6257,  1.8984,\n",
       "           5.3270, -3.8239, -5.4918],\n",
       "         [ 1.7781, -1.8812,  5.4628,  1.6027,  4.1857,  2.6254,  1.9024,\n",
       "           5.3270, -3.8232, -5.4918]],\n",
       "\n",
       "        [[ 0.9366, -1.4215,  5.3050,  1.6153,  4.1606,  2.6257,  1.8984,\n",
       "           5.3270, -3.8239, -5.4918],\n",
       "         [ 1.4779,  2.5348, -0.7698,  3.4727, -7.2887,  1.2362,  1.2227,\n",
       "          -1.8794, -3.7951, -4.9428]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pEnc = PositionalEncoding(10, 2)\n",
    "x = torch.randint(0, 5, (5, 2), dtype=torch.long)\n",
    "e = emb(x)\n",
    "print(x)\n",
    "print(e)\n",
    "pEnc(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "229072ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size=512, num_heads=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embed_size % num_heads == 0, \"Embedding size must be divisible by number of heads\"\n",
    "        self.embed_size = embed_size # 512 by default\n",
    "        self.num_heads = num_heads # 8 by default\n",
    "        self.head_dim = int(embed_size / num_heads) # 512/8 = 64 by default\n",
    "\n",
    "        # Linear transformations for queries, keys, and values\n",
    "        self.query_projection = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.key_projection = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.value_projection = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "\n",
    "        # Linear transformation for output projection (8*64)x512 or 512x512\n",
    "        self.output_projection = nn.Linear(num_heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask=None):\n",
    "\n",
    "        N, value_len, key_len, query_len = queries.shape[0], values.shape[1], keys.shape[1], queries.shape[1]\n",
    "        values = values.reshape(N, value_len, self.num_heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.num_heads, self.head_dim)\n",
    "        queries = queries.reshape(N, query_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        values = self.value_projection(values)\n",
    "        keys = self.key_projection(keys)\n",
    "        queries = self.query_projection(queries)\n",
    "\n",
    "        keys_transpose = keys.transpose(-2, -1)\n",
    "        scaled_scores = torch.matmul(queries, keys_transpose)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_scores = scaled_scores.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        scaled_scores = scaled_scores * (1.0 / math.sqrt(self.head_dim))\n",
    "        attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "        \n",
    "        attention_output = torch.matmul(attention_weights, values)\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(N, value_len, self.head_dim * self.num_heads)\n",
    "        output = self.output_projection(attention_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "f48971d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4789, 0.4563, 0.8299, 0.9584, 0.7924, 0.7973, 0.6738, 0.6807,\n",
      "          0.7644, 0.3278],\n",
      "         [0.3840, 0.8490, 0.0580, 0.3516, 0.9290, 0.1023, 0.4729, 0.3873,\n",
      "          0.0588, 0.4671]],\n",
      "\n",
      "        [[0.3305, 0.8745, 0.8956, 0.7049, 0.0280, 0.7392, 0.5881, 0.5698,\n",
      "          0.7126, 0.5654],\n",
      "         [0.0745, 0.8963, 0.4403, 0.5683, 0.4776, 0.0545, 0.2787, 0.8023,\n",
      "          0.7805, 0.9913]]])\n",
      "tensor([[[ 0.0386, -0.0310, -0.0048,  0.4215, -0.4450, -0.0599,  0.5028,\n",
      "           0.3107,  0.1961,  0.5020],\n",
      "         [ 0.0385, -0.0319, -0.0041,  0.4222, -0.4451, -0.0609,  0.5040,\n",
      "           0.3112,  0.1966,  0.5031]],\n",
      "\n",
      "        [[ 0.0248, -0.1939, -0.0070,  0.2906, -0.3201, -0.0891,  0.5954,\n",
      "           0.3117,  0.4549,  0.7071],\n",
      "         [ 0.0253, -0.1940, -0.0078,  0.2893, -0.3208, -0.0881,  0.5951,\n",
      "           0.3112,  0.4552,  0.7076]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mAttention = MultiHeadAttention(10, 2)\n",
    "x = torch.rand((2, 2, 10))\n",
    "a = mAttention(x, x, x)\n",
    "print(x)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "33219265",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, embed_size, ff_hidden_size, dropout_rate=0.1):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_size, ff_hidden_size)\n",
    "        self.fc2 = nn.Linear(ff_hidden_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.constant_(self.fc2.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "5b913fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3819, 0.2336, 0.0704, 0.3549, 0.9188, 0.8499, 0.6359, 0.0572,\n",
      "          0.9218, 0.6735],\n",
      "         [0.0855, 0.4643, 0.6691, 0.5916, 0.7340, 0.1227, 0.4601, 0.3107,\n",
      "          0.8145, 0.4057]]])\n",
      "tensor([[[-0.3285, -0.3927, -0.1556, -0.0652, -0.5466,  0.0249, -0.0745,\n",
      "          -0.4057, -0.0870, -0.0800],\n",
      "         [-0.2573, -0.4286, -0.2811,  0.1576, -0.4314,  0.0887, -0.2062,\n",
      "          -0.1129, -0.1918,  0.1081]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForwardNetwork(10, 100, 0.2)\n",
    "x = torch.rand((1, 2, 10))\n",
    "o = ffn(x)\n",
    "print(x)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "02dd97cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_weights(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        nn.init.constant_(module.bias, 0)\n",
    "        nn.init.constant_(module.weight, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "32ab44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_hidden_size, num_heads=8, dropout_rate=0.2):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.feed_forward = FeedForwardNetwork(embed_dim, ff_hidden_size, dropout_rate)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.apply(_init_weights)\n",
    "\n",
    "    def forward(self, value, key, query):\n",
    "        norm_query = self.norm1(query)\n",
    "        attention_out = self.attention(value, key, norm_query)\n",
    "        query = query + self.dropout1(attention_out) # Residual connection\n",
    "        norm_query = self.norm2(query)\n",
    "        feed_fwd_out = self.feed_forward(norm_query)\n",
    "        query = query + self.dropout2(feed_fwd_out) # Residual connection\n",
    "        return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ecef2e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, seq_len, vocab_size, embed_dim, num_layers=2, ff_hidden_size=2048, num_heads=8):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding_layer = Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoder = PositionalEncoding(seq_len, embed_dim)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(embed_dim, ff_hidden_size, num_heads) for i in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed_out = self.embedding_layer(x)\n",
    "        out = self.positional_encoder(embed_out)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out)\n",
    "        return out  #32x10x512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "908a6e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_hidden_size, num_heads=8, dropout_rate=0.2):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, ff_hidden_size, num_heads, dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.apply(_init_weights)\n",
    "\n",
    "    def forward(self, key, query, value, mask):\n",
    "        norm_query = self.norm1(query)\n",
    "        attention = self.attention(value, value, norm_query, mask=mask)\n",
    "        query = query + self.dropout(attention)\n",
    "        return self.transformer_block(key, query, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "89749b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, embed_dim, seq_len, num_layers=2, ff_hidden_size=2048, num_heads=8, dropout_rate=0.2):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(target_vocab_size, embed_dim)\n",
    "        self.position_embedding = PositionalEncoding(seq_len, embed_dim)\n",
    "        self.layers = nn.ModuleList([DecoderBlock(embed_dim, ff_hidden_size, num_heads, dropout_rate) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(embed_dim, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, enc_out, mask):\n",
    "        x = self.word_embedding(x)  # Shape: (batch_size, seq_len, embed_dim)\n",
    "        x = self.position_embedding(x) # Shape: (batch_size, seq_len, embed_dim)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(enc_out, x, enc_out, mask)\n",
    "        return F.softmax(self.fc_out(x), dim=-1) # Shape: (batch_size, seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dd136e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_dim, src_vocab_size, target_vocab_size, seq_length, num_layers=2, ff_hidden_size=2048, num_heads=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.encoder = TransformerEncoder(seq_length, src_vocab_size, embed_dim, num_layers=num_layers, ff_hidden_size=ff_hidden_size, num_heads=num_heads)\n",
    "        self.decoder = TransformerDecoder(target_vocab_size, embed_dim, seq_length, num_layers=num_layers, ff_hidden_size=ff_hidden_size, num_heads=num_heads)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        batch_size, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            batch_size, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask\n",
    "\n",
    "    def decode(self,src,trg):\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src)\n",
    "        out_labels = []\n",
    "        batch_size,seq_len = src.shape[0],src.shape[1]\n",
    "        out = trg\n",
    "        for i in range(seq_len): #10\n",
    "            out = self.decoder(out,enc_out,trg_mask) #bs x seq_len x vocab_dim\n",
    "            out = out[:,-1,:]\n",
    "            out = out.argmax(-1)\n",
    "            out_labels.append(out.item())\n",
    "            out = torch.unsqueeze(out,axis=0)\n",
    "        return out_labels\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src)\n",
    "        outputs = self.decoder(trg, enc_out, trg_mask)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "888e2159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of source sequence: torch.Size([2, 12])\n",
      "Shape of target sequence: torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "src_vocab_size = 11\n",
    "target_vocab_size = 11\n",
    "num_layers = 6\n",
    "seq_length = 12\n",
    "embed_dim = 512\n",
    "ff_hidden_size = 2048  # Feed-forward hidden layer size\n",
    "num_heads = 8\n",
    "\n",
    "src_sequence = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1],\n",
    "                             [0, 2, 8, 7, 3, 4, 5, 6, 7, 2, 10, 1]])\n",
    "target_sequence = torch.tensor([[0, 1, 7, 4, 3, 5, 9, 2, 8, 10, 9, 1],\n",
    "                                [0, 1, 5, 6, 2, 4, 7, 6, 2, 8, 10, 1]])\n",
    "print(\"Shape of source sequence:\", src_sequence.shape)\n",
    "print(\"Shape of target sequence:\", target_sequence.shape)\n",
    "model = Transformer(embed_dim=embed_dim, src_vocab_size=src_vocab_size,\n",
    "                    target_vocab_size=target_vocab_size, seq_length=seq_length,\n",
    "                    num_layers=num_layers, ff_hidden_size=ff_hidden_size, num_heads=num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d9077a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.52016807 -0.8028974  -2.3331447  -0.781131  ]\n",
      " [-0.9884004   0.37007698  0.4746922   1.450523  ]\n",
      " [ 0.602024   -0.35471794  0.213776    0.3411836 ]]\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(embed_dim=4, src_vocab_size=3,\n",
    "                    target_vocab_size=3, seq_length=2,\n",
    "                    num_layers=2, ff_hidden_size=10, num_heads=2)\n",
    "print(model.encoder.embedding_layer.embedding.weight.data.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
