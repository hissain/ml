{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hissain/mlworks/blob/main/codes/Encoder_Decoder_Architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdowMQHxCnue"
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "%matplotlib inline\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5v1qBIWQIj9",
        "outputId": "5f5af5cf-ffea-48e4-9a3e-9dca5ddd73ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x794f143ad9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Encoder\n",
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
        "        super(EncoderLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, input_seqs, hidden):\n",
        "        embedded = self.embedding(input_seqs)\n",
        "        output, hidden = self.lstm(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size))\n"
      ],
      "metadata": {
        "id": "tRJ4_KIJEKG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Decoder\n",
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, num_layers=1):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input_seqs, hidden):\n",
        "        embedded = self.embedding(input_seqs)\n",
        "        output, hidden = self.lstm(embedded, hidden)\n",
        "        output = self.fc(output)\n",
        "        return output, hidden\n"
      ],
      "metadata": {
        "id": "IBPaWKBUEeWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Seq2Seq Model\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seqs, target_seqs):\n",
        "        batch_size = input_seqs.size(0)\n",
        "        target_length = target_seqs.size(1)\n",
        "        target_vocab_size = self.decoder.fc.out_features\n",
        "\n",
        "        encoder_hidden = self.encoder.init_hidden(batch_size)\n",
        "        encoder_output, encoder_hidden = self.encoder(input_seqs, encoder_hidden)\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]] * batch_size, dtype=torch.long)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        outputs = torch.zeros(batch_size, target_length, target_vocab_size)\n",
        "\n",
        "        for t in range(target_length):\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "            outputs[:, t, :] = decoder_output.squeeze(1)\n",
        "            decoder_input = target_seqs[:, t].unsqueeze(1)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "2YimvDf4HJ77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here\n",
        "- We define an `EncoderLSTM` class, which takes input sequences and produces hidden states using an LSTM layer.\n",
        "- We define a `DecoderLSTM` class, which takes hidden states from the encoder and generates output sequences using another LSTM layer followed by a fully connected layer.\n",
        "- We define a `Seq2Seq` class that encapsulates the encoder and decoder. It takes input sequences and target sequences, passes the input sequences through the encoder, and then feeds the hidden states to the decoder to generate output sequences."
      ],
      "metadata": {
        "id": "QTXE0R2IJAOI"
      }
    }
  ]
}