{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6f0bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install farm-haystack[preprocessing] -q\n",
    "#!pip install farm-haystack[inference] -q\n",
    "#!pip install farm-haystack[elasticsearch] -q\n",
    "#!pip install nltk -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a41b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/hissain/#start-of-content\n",
      "https://github.com/hissain/\n",
      "https://github.com/hissain/\n",
      "https://github.com/hissain/\n",
      "https://github.com/hissain/hissain\n",
      "https://github.com/hissain/#about-me\n",
      "https://github.com/hissain/#stackoverflow\n",
      "https://github.com/hissain/CoronaTracker\n",
      "https://github.com/hissain/CoronaTracker/stargazers\n",
      "https://github.com/hissain/CoronaTracker/forks\n",
      "https://github.com/hissain/minimal-cmake\n",
      "https://github.com/hissain/minimal-cmsis-dsp\n",
      "https://github.com/hissain/mlworks\n",
      "https://github.com/hissain/mlworks/stargazers\n",
      "https://github.com/hissain/dsp-spectrogram\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_visitable_links(base_url):\n",
    "    # Send a GET request to the base URL\n",
    "    response = requests.get(base_url)\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve {base_url}\")\n",
    "        return []\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = []\n",
    "    \n",
    "    # Find all anchor tags in the page\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        # Get the full URL\n",
    "        full_url = urljoin(base_url, a_tag['href'])\n",
    "        \n",
    "        # Check if the URL is visitable (not a mailto or javascript link)\n",
    "        if full_url.startswith(base_url):\n",
    "            links.append(full_url)\n",
    "    \n",
    "    return links\n",
    "\n",
    "# Example usage\n",
    "base_url = 'https://github.com/hissain/'  # Replace with your desired URL\n",
    "visitable_links = get_visitable_links(base_url)\n",
    "\n",
    "# Print the collected links\n",
    "for link in visitable_links:\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30202e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Scrape webpages and prepare documents\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from haystack.schema import Document\n",
    "\n",
    "# Replace this list with your actual list of blog URLs\n",
    "#urls = [\n",
    "#    'https://en.wikipedia.org/wiki/Bangladesh',\n",
    "#    'https://en.wikipedia.org/wiki/India',\n",
    "    # Add more URLs as needed\n",
    "#]\n",
    "\n",
    "urls = visitable_links\n",
    "\n",
    "documents = []\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract text from all paragraph tags\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = '\\n'.join([para.get_text() for para in paragraphs])\n",
    "\n",
    "        # Create a Haystack Document with content and optional metadata\n",
    "        doc = Document(content=content, meta={'url': url})\n",
    "        documents.append(doc)\n",
    "    except Exception as e:\n",
    "        print(f'Error processing {url}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f85087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of documents: 15\n"
     ]
    }
   ],
   "source": [
    "print(f'Total no of documents: {len(documents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddaa9272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/hissain/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "Preprocessing: 100%|████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 44.33docs/s]\n",
      "Updating BM25 representation...: 100%|██████████████████████████████████████████████████████| 13/13 [00:00<00:00, 12997.84 docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents after splitting: 13\n",
      "We read every piece of feedback, and take your input very seriously.\n",
      "\n",
      "To see all available qualifiers, see our documentation.\n",
      "\n",
      "Prevent this user from interacting with your repositories and sending you notifications.\n",
      "Learn more about blocking users.\n",
      "\n",
      "You must be logged in to block users.\n",
      "\n",
      "Contact GitHub support about this user’s behavior.\n",
      "Learn more about reporting abuse.\n",
      "\n",
      "I am an accomplished Associate Architect with over 13 years of experience in Mobile and Wearables Software Development. With a deep understanding of the challenges inherent in developing performant, robust, testable, and maintainable applications, from requirement analysis to architecture, design, development, and maintenance, I am confident in my ability to lead software development and commercialization for any organization. My experience includes working on Samsung Health for iOS, an application boasting over 6 million users and approximately 160K DAU on the App Store market, with a global rating of 4.5. Between 2021 and 2024,\n",
      "I achieved six patent applications granted by Samsung SIPMS, currently in the process of being published in USPTO; among them, one patent has already been published in USPTO, WIPO, and KR.\n",
      "My technical expertise encompasses OOP, Android (Java, Kotlin), iOS (Swift, Objective-C), Xcode, Version Control Systems, System Design, Application Architecture, Development Processes, Wearable & Hearable Technology. Additionally, I have experience in Tizen app development and Windows app development. Currently, I am engaged in Samsung Earbuds device development on RTOS, specifically in music streaming over BT classic and LE Audio, as well as in ballistocardiogram signal processing for Stress score generation.\n",
      "My special interests include Technological Innovation, Human-Machine Interaction, Information Theory, Astronomy, Probability, Theory of Relativity, Philosophy of Science, Piano, Guitar, and Poetry. Furthermore, I have pursued studies in Critical Thinking and How to Make Valid Argumentation.\n",
      "\n",
      "{'url': 'https://github.com/hissain/#start-of-content', '_split_id': 0, '_split_overlap': [{'doc_id': '99121415aff273ebf7248c2da7b9ccf6', 'range': (1506, 2010)}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore, ElasticsearchDocumentStore, FAISSDocumentStore\n",
    "from haystack.nodes import DensePassageRetriever, FARMReader, BM25Retriever\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "from haystack.utils import print_answers\n",
    "from haystack.nodes import PreProcessor\n",
    "from haystack.schema import Document\n",
    "\n",
    "# Step 2: Initialize an in-memory document store\n",
    "document_store = InMemoryDocumentStore(use_bm25=True)\n",
    "#document_store = ElasticsearchDocumentStore(host=\"localhost\", port=9200)\n",
    "\n",
    "# Initialize PreProcessor with desired settings\n",
    "preprocessor = PreProcessor(\n",
    "    split_by=\"word\",            # Can also be \"sentence\" if sentence-based splitting is preferred\n",
    "    split_length=300,           # Number of words per chunk\n",
    "    split_respect_sentence_boundary=True,  # Ensure splitting occurs at sentence boundaries\n",
    "    split_overlap=50            # Overlap between chunks to preserve context\n",
    ")\n",
    "\n",
    "# Process the scraped documents to split them\n",
    "processed_documents = preprocessor.process(documents)\n",
    "\n",
    "# Now, 'processed_documents' will contain smaller chunks ready for the document store\n",
    "document_store.write_documents(processed_documents)\n",
    "\n",
    "# Verify the number of chunks after splitting\n",
    "print(f\"Number of documents after splitting: {document_store.get_document_count()}\")\n",
    "\n",
    "# Example: Print a chunk to inspect\n",
    "sample_chunk = document_store.get_all_documents()[0]\n",
    "print(sample_chunk.content)\n",
    "print(sample_chunk.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee3178d1-1568-4ef0-a32d-3064f562f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize a reader model (FARMReader)\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=False)\n",
    "\n",
    "# Step 4: Initialize BM25Retriever for BM25 retrieval (keyword-based retrieval)\n",
    "retriever = BM25Retriever(document_store=document_store)\n",
    "\n",
    "# Step 5: Create a pipeline using BM25Retriever and the FARMReader\n",
    "pipeline = ExtractiveQAPipeline(reader=reader, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7190c576-e3c7-404c-ab12-897c2b53eae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.25s/ Batches]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "'Query: What are the challenges for Covid-19?'\n",
      "'Answers:'\n",
      "[   {   'answer': 'information accuracy, information retrieval delay etc.',\n",
      "        'context': ' this approach has many limitations in terms of '\n",
      "                   'information accuracy, information retrieval delay etc. due '\n",
      "                   'to mostly,\\n'\n",
      "                   'Detail architecture can be explo'},\n",
      "    {   'answer': 'to flatten the curve of infection',\n",
      "        'context': ' challenges for any government of the affected counties is '\n",
      "                   'to flatten the curve of infection. Early detection of '\n",
      "                   'already affected patients is the most'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Ask a question to the RAG pipeline\n",
    "query = \"What are the challenges for Covid-19?\"\n",
    "\n",
    "# Get answers from the pipeline\n",
    "prediction = pipeline.run(\n",
    "    query=query,\n",
    "    params={\n",
    "        \"Retriever\": {\"top_k\": 5},  # Number of documents to retrieve\n",
    "        \"Reader\": {\"top_k\": 2}      # Number of answers to return\n",
    "    }\n",
    ")\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Print the answers\n",
    "print_answers(prediction, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ee8de2f-a242-4ac9-9caa-dff2236ca1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.45s/ Batches]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "'Query: What interests Hissain most?'\n",
      "'Answers:'\n",
      "[   {   'answer': 'Technological Innovation, Human-Machine Interaction, '\n",
      "                  'Information Theory, Astronomy, Probability, Theory of '\n",
      "                  'Relativity, Philosophy of Science, Piano, Guitar, and '\n",
      "                  'Poetry',\n",
      "        'context': ' Technological Innovation, Human-Machine Interaction, '\n",
      "                   'Information Theory, Astronomy, Probability, Theory of '\n",
      "                   'Relativity, Philosophy of Science, Piano, Guitar, and '\n",
      "                   'Poetry'},\n",
      "    {   'answer': 'Technological Innovation, Human-Machine Interaction',\n",
      "        'context': 'ss score generation.\\n'\n",
      "                   'My special interests include Technological Innovation, '\n",
      "                   'Human-Machine Interaction, Information Theory, Astronomy, '\n",
      "                   'Probability, The'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Ask a question to the RAG pipeline\n",
    "query = \"What interests Hissain most?\"\n",
    "\n",
    "# Get answers from the pipeline\n",
    "prediction = pipeline.run(\n",
    "    query=query,\n",
    "    params={\n",
    "        \"Retriever\": {\"top_k\": 5},  # Number of documents to retrieve\n",
    "        \"Reader\": {\"top_k\": 2}      # Number of answers to return\n",
    "    }\n",
    ")\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Print the answers\n",
    "print_answers(prediction, details=\"minimum\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
