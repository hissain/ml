{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hissain/ml/blob/main/codes/reinforcement/Reinforce_CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Reinforcement Learning Algorithm with PyTorch: Reinforce."
      ],
      "metadata": {
        "id": "0qCsPXLNTt24"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjRWziAVU2lZ"
      },
      "source": [
        "\n",
        "In this notebook, you'll code your first Deep Reinforcement Learning algorithm from scratch: Reinforce (also called Monte Carlo Policy Gradient).\n",
        "\n",
        "Reinforce is a *Policy-based method*: a Deep Reinforcement Learning algorithm that tries **to optimize the policy directly without using an action-value function**.\n",
        "\n",
        "More precisely, Reinforce is a *Policy-gradient method*, a subclass of *Policy-based methods* that aims **to optimize the policy directly by estimating the weights of the optimal policy using gradient ascent**.\n",
        "\n",
        "To test its robustness, we're going to train it in 2 different simple environments:\n",
        "- Cartpole-v1\n",
        "- PixelcopterEnv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/envs.gif\" alt=\"Environments\"/>\n"
      ],
      "metadata": {
        "id": "s4rBom2sbo7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🎮 Environments:\n",
        "\n",
        "- [CartPole-v1](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)\n",
        "- [PixelCopter](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)\n"
      ],
      "metadata": {
        "id": "BPLwsPajb1f8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "f4Wh4agpTmDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a virtual display 🖥\n",
        "\n",
        "During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n",
        "\n",
        "Hence the following cell will install the librairies and create and run a virtual screen 🖥"
      ],
      "metadata": {
        "id": "bTpYcVZVMzUI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV6wjQ7Be7p5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install pyglet==1.5.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "Sr-Nuyb1dBm0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb785722-ac7b-4f95-958f-9a59bc66661b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x79b6b7570670>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjrLfPFIW8XK"
      },
      "source": [
        "## Install the dependencies 🔽\n",
        "The first step is to install the dependencies. We’ll install multiple ones:\n",
        "\n",
        "- `gym`\n",
        "- `gym-games`: Extra gym environments made with PyGame.\n",
        "- `huggingface_hub`: 🤗 works as a central place where anyone can share and explore models and datasets. It has versioning, metrics, visualizations, and other features that will allow you to easily collaborate with others.\n",
        "\n",
        "You may be wondering why we install gym and not gymnasium, a more recent version of gym? **Because the gym-games we are using are not updated yet with gymnasium**.\n",
        "\n",
        "The differences you'll encounter here:\n",
        "- In `gym` we don't have `terminated` and `truncated` but only `done`.\n",
        "- In `gym` using `env.step()` returns `state, reward, done, info`\n",
        "\n",
        "You can learn more about the differences between Gym and Gymnasium here 👉 https://gymnasium.farama.org/content/migration-guide/\n",
        "\n",
        "\n",
        "You can see here all the Reinforce models available 👉 https://huggingface.co/models?other=reinforce\n",
        "\n",
        "And you can find all the Deep Reinforcement Learning models here 👉 https://huggingface.co/models?pipeline_tag=reinforcement-learning\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt"
      ],
      "metadata": {
        "id": "e8ZVi-uydpgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55644b85-bc37-46cc-cced-ad1e4b0972ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/ntasfi/PyGame-Learning-Environment.git (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1))\n",
            "  Cloning https://github.com/ntasfi/PyGame-Learning-Environment.git to /tmp/pip-req-build-dtlx986i\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ntasfi/PyGame-Learning-Environment.git /tmp/pip-req-build-dtlx986i\n",
            "  Resolved https://github.com/ntasfi/PyGame-Learning-Environment.git to commit 3dbe79dc0c35559bb441b9359948aabf9bb3d331\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/simoninithomas/gym-games (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2))\n",
            "  Cloning https://github.com/simoninithomas/gym-games to /tmp/pip-req-build-0mrmmhwu\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/simoninithomas/gym-games /tmp/pip-req-build-0mrmmhwu\n",
            "  Resolved https://github.com/simoninithomas/gym-games to commit f31695e4ba028400628dc054ee8a436f28193f0b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (0.24.7)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 4)) (0.5.1)\n",
            "Collecting pyyaml==6.0 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 5))\n",
            "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ple==0.0.1->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from ple==0.0.1->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (10.4.0)\n",
            "Requirement already satisfied: gym>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (0.25.2)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.10/dist-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (75.1.0)\n",
            "Requirement already satisfied: pygame>=1.9.6 in /usr/local/lib/python3.10/dist-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (0.0.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2024.8.30)\n",
            "Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: ple, gym-games\n",
            "  Building wheel for ple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ple: filename=ple-0.0.1-py3-none-any.whl size=50772 sha256=ac5565cc7ff7ed1430e0b402a09248e6be5ade414945202213593d67f9bb1ac2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-q6mwal4c/wheels/f8/31/ca/a64a7ce73540465412d82813780d062db53b90e3f42a4ecb7f\n",
            "  Building wheel for gym-games (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym-games: filename=gym_games-1.0.4-py3-none-any.whl size=17306 sha256=fe2bfa8162445ff9cc3a73c3d87254be0fba771a22cfb13d98874c423ea4a07d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-q6mwal4c/wheels/ca/bf/6b/7d631626202ebb033c908a688d1862ff4d948c34cf621d7dc9\n",
            "Successfully built ple gym-games\n",
            "Installing collected packages: pyyaml, ple, gym-games\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "Successfully installed gym-games-1.0.4 ple-0.0.1 pyyaml-6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAHAq6RZW3rn"
      },
      "source": [
        "## Import the packages 📦\n",
        "In addition to import the installed libraries, we also import:\n",
        "\n",
        "- `imageio`: A library that will help us to generate a replay video\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8oadoJSWp7C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Gym\n",
        "import gym\n",
        "import gym_pygame\n",
        "\n",
        "# Hugging Face Hub\n",
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if we have a GPU\n",
        "\n",
        "- Let's check if we have a GPU\n",
        "- If it's the case you should see `device:cuda0`"
      ],
      "metadata": {
        "id": "RfxJYdMeeVgv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaJu5FeZxXGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "331ea8fd-870c-495c-9396-fbb14c38460e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5TNYa14aRav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "863d988c-95ec-471f-f243-4897f25f5ade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBPecCtBL_pZ"
      },
      "source": [
        "We're now ready to implement our Reinforce algorithm 🔥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KEyKYo2ZSC-"
      },
      "source": [
        "# First agent: Playing CartPole-v1 🤖"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haLArKURMyuF"
      },
      "source": [
        "## Create the CartPole environment and understand how it works\n",
        "### [The environment 🎮](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH_TaLKFXo_8"
      },
      "source": [
        "### Why do we use a simple environment like CartPole-v1?\n",
        "As explained in [Reinforcement Learning Tips and Tricks](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html), when you implement your agent from scratch you need **to be sure that it works correctly and find bugs with easy environments before going deeper**. Since finding bugs will be much easier in simple environments.\n",
        "\n",
        "\n",
        "> Try to have some “sign of life” on toy problems\n",
        "\n",
        "\n",
        "> Validate the implementation by making it run on harder and harder envs (you can compare results against the RL zoo). You usually need to run hyperparameter optimization for that step.\n",
        "___\n",
        "### The CartPole-v1 environment\n",
        "\n",
        "> A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
        "\n",
        "\n",
        "\n",
        "So, we start with CartPole-v1. The goal is to push the cart left or right **so that the pole stays in the equilibrium.**\n",
        "\n",
        "The episode ends if:\n",
        "- The pole Angle is greater than ±12°\n",
        "- Cart Position is greater than ±2.4\n",
        "- Episode length is greater than 500\n",
        "\n",
        "We get a reward 💰 of +1 every timestep the Pole stays in the equilibrium."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POOOk15_K6KA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f491104-0cf7-402a-adc2-b544977884ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "# Create the env\n",
        "env = gym.make(env_id)\n",
        "\n",
        "# Create the evaluation env\n",
        "eval_env = gym.make(env_id)\n",
        "\n",
        "# Get the state space and action space\n",
        "s_size = env.observation_space.shape[0]\n",
        "a_size = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMLFrjiBNLYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b0914d3-104e-4dcf-fd69-be6ad2fda24b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_____OBSERVATION SPACE_____ \n",
            "\n",
            "The State Space is:  4\n",
            "Sample observation [-4.4901800e+00  2.7413068e+38 -3.7616181e-01  9.4178093e+37]\n"
          ]
        }
      ],
      "source": [
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"The State Space is: \", s_size)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu6t4sRNNWkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635db2ad-99f3-47cc-b307-8bb594bb74f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " _____ACTION SPACE_____ \n",
            "\n",
            "The Action Space is:  2\n",
            "Action Space Sample 1\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"The Action Space is: \", a_size)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SJMJj3WaFOz"
      },
      "source": [
        "## Let's build the Reinforce Architecture\n",
        "This implementation is based on two implementations:\n",
        "- [PyTorch official Reinforcement Learning example](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)\n",
        "- [Udacity Reinforce](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)\n",
        "- [Improvement of the integration by Chris1nexus](https://github.com/huggingface/deep-rl-class/pull/95)\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/reinforce.png\" alt=\"Reinforce\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49kogtxBODX8"
      },
      "source": [
        "So we want:\n",
        "- Two fully connected layers (fc1 and fc2).\n",
        "- Using ReLU as activation function of fc1\n",
        "- Using Softmax to output a probability distribution over actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho_UHf49N9i4"
      },
      "outputs": [],
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(s_size, h_size)\n",
        "        self.fc2 = nn.Linear(h_size, a_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        # Sampling an action based on probability\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MXoqetzfIoW"
      },
      "source": [
        "### Let's build the Reinforce Training Algorithm\n",
        "This is the Reinforce algorithm pseudocode:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/pg_pseudocode.png\" alt=\"Policy gradient pseudocode\"/>\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(np.finfo(np.float32))\n",
        "# eps = np.finfo(np.float32).eps.item()\n",
        "# print(eps)"
      ],
      "metadata": {
        "id": "hmhbB5xQL_Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cat([torch.Tensor([1]), torch.Tensor([2]), torch.Tensor([2])]).sum()"
      ],
      "metadata": {
        "id": "_LOfRCfRMuCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCNvyElRStWG"
      },
      "outputs": [],
      "source": [
        "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
        "    # Help us to calculate the score during the training\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    # Line 3 of pseudocode\n",
        "    for i_episode in range(1, n_training_episodes+1):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "        state = env.reset()\n",
        "        # Line 4 of pseudocode\n",
        "        for t in range(max_t):\n",
        "            action, log_prob = policy.act(state)\n",
        "            saved_log_probs.append(log_prob)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            if done:\n",
        "                break\n",
        "        scores_deque.append(sum(rewards))\n",
        "        scores.append(sum(rewards))\n",
        "\n",
        "        # Line 6 of pseudocode: calculate the return\n",
        "        returns = deque(maxlen=max_t)\n",
        "        n_steps = len(rewards)\n",
        "        # Compute the discounted returns at each timestep,\n",
        "        # as\n",
        "        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
        "        #\n",
        "        # In O(N) time, where N is the number of time steps\n",
        "        # (this definition of the discounted return G_t follows the definition of this quantity\n",
        "        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n",
        "        # G_t = r_(t+1) + r_(t+2) + ...\n",
        "\n",
        "        # Given this formulation, the returns at each timestep t can be computed\n",
        "        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n",
        "        # G_t = r_(t+1) + gamma*G_(t+1)\n",
        "        # G_(t-1) = r_t + gamma* G_t\n",
        "        # (this follows a dynamic programming approach, with which we memorize solutions in order\n",
        "        # to avoid computing them multiple times)\n",
        "\n",
        "        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n",
        "        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n",
        "\n",
        "\n",
        "        ## Given the above, we calculate the returns at timestep t as:\n",
        "        #               gamma[t] * return[t] + reward[t]\n",
        "        #\n",
        "        ## We compute this starting from the last timestep to the first, in order\n",
        "        ## to employ the formula presented above and avoid redundant computations that would be needed\n",
        "        ## if we were to do it from first to last.\n",
        "\n",
        "        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n",
        "        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n",
        "        ## a normal python list would instead require O(N) to do this.\n",
        "        for t in range(n_steps)[::-1]:\n",
        "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
        "            returns.appendleft( gamma*disc_return_t + rewards[t]   )\n",
        "\n",
        "        ## standardization of the returns is employed to make training more stable\n",
        "        eps = np.finfo(np.float32).eps.item()\n",
        "        ## eps is the smallest representable float, which is\n",
        "        # added to the standard deviation of the returns to avoid numerical instabilities\n",
        "        returns = torch.tensor(returns)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "        # Line 7:\n",
        "        policy_loss = []\n",
        "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
        "            policy_loss.append(-log_prob * disc_return)\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "        # Line 8: PyTorch prefers gradient descent\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i_episode % print_every == 0:\n",
        "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIWhQyJjfpEt"
      },
      "source": [
        "##  Train it\n",
        "- We're now ready to train our agent.\n",
        "- But first, we define a variable containing all the training hyperparameters.\n",
        "- You can change the training parameters (and should 😉)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utRe1NgtVBYF"
      },
      "outputs": [],
      "source": [
        "cartpole_hyperparameters = {\n",
        "    \"h_size\": 16,\n",
        "    \"n_training_episodes\": 1000,\n",
        "    \"n_evaluation_episodes\": 10,\n",
        "    \"max_t\": 500,\n",
        "    \"gamma\": 1.0,\n",
        "    \"lr\": 1e-2,\n",
        "    \"env_id\": env_id,\n",
        "    \"state_space\": s_size,\n",
        "    \"action_space\": a_size,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3lWyVXBVfl6"
      },
      "outputs": [],
      "source": [
        "# Create policy and place it to the device\n",
        "cartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
        "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGf-hQCnfouB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50cc3e0f-0b2e-4177-fdeb-e1286014a428"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: 72.21\n",
            "Episode 200\tAverage Score: 413.59\n",
            "Episode 300\tAverage Score: 442.58\n",
            "Episode 400\tAverage Score: 432.48\n",
            "Episode 500\tAverage Score: 473.37\n",
            "Episode 600\tAverage Score: 450.69\n",
            "Episode 700\tAverage Score: 474.34\n",
            "Episode 800\tAverage Score: 499.90\n",
            "Episode 900\tAverage Score: 495.28\n",
            "Episode 1000\tAverage Score: 499.33\n"
          ]
        }
      ],
      "source": [
        "scores = reinforce(cartpole_policy,\n",
        "                   cartpole_optimizer,\n",
        "                   cartpole_hyperparameters[\"n_training_episodes\"],\n",
        "                   cartpole_hyperparameters[\"max_t\"],\n",
        "                   cartpole_hyperparameters[\"gamma\"],\n",
        "                   100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qajj2kXqhB3g"
      },
      "source": [
        "## Define evaluation method 📝\n",
        "- Here we define the evaluation method that we're going to use to test our Reinforce agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FamHmxyhBEU"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
        "  \"\"\"\n",
        "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
        "  :param env: The evaluation environment\n",
        "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
        "  :param policy: The Reinforce agent\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in range(n_eval_episodes):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards_ep = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      action, _ = policy.act(state)\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdH2QCrLTrlT"
      },
      "source": [
        "## Evaluate our agent 📈"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohGSXDyHh0xx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ed6af3d-e670-4f72-a5f6-8b0dfdb341c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500.0, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "evaluate_agent(eval_env,\n",
        "               cartpole_hyperparameters[\"max_t\"],\n",
        "               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n",
        "               cartpole_policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo4JH45if81z"
      },
      "outputs": [],
      "source": [
        "def record_video(env, policy, out_directory, fps=30):\n",
        "  \"\"\"\n",
        "  Generate a replay video of the agent\n",
        "  :param env\n",
        "  :param Qtable: Qtable of our agent\n",
        "  :param out_directory\n",
        "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
        "  \"\"\"\n",
        "  images = []\n",
        "  done = False\n",
        "  state = env.reset()\n",
        "  img = env.render(mode='rgb_array')\n",
        "  images.append(img)\n",
        "  while not done:\n",
        "    # Take the action (index) that have the maximum expected future reward given that state\n",
        "    action, _ = policy.act(state)\n",
        "    state, reward, done, info = env.step(action) # We directly put next_state = state for recording logic\n",
        "    img = env.render(mode='rgb_array')\n",
        "    images.append(img)\n",
        "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "record_video(eval_env, cartpole_policy, \"/content/replay-cartpole.mp4\")"
      ],
      "metadata": {
        "id": "AzfsyjkOPhsf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e72e9e31-30da-49c4-bcde-7f1501aeaff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second agent: PixelCopter 🚁\n",
        "\n",
        "### Study the PixelCopter environment 👀\n",
        "- [The Environment documentation](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)\n"
      ],
      "metadata": {
        "id": "JNLVmKKVKA6j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBSc8mlfyin3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d19bf08-5d15-4eb9-ebd6-dbfd427c35d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "couldn't import doomish\n",
            "Couldn't import doom\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "env_id = \"Pixelcopter-PLE-v0\"\n",
        "env = gym.make(env_id)\n",
        "eval_env = gym.make(env_id)\n",
        "s_size = env.observation_space.shape[0]\n",
        "a_size = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"The State Space is: \", s_size)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ],
      "metadata": {
        "id": "L5u_zAHsKBy7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc33942-b118-47b1-924d-d2bd2c36ce56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_____OBSERVATION SPACE_____ \n",
            "\n",
            "The State Space is:  7\n",
            "Sample observation [ 1.0555778  0.9608717 -2.0344515  0.6159086 -0.5302757 -1.1298463\n",
            " -0.5391087]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"The Action Space is: \", a_size)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ],
      "metadata": {
        "id": "D7yJM9YXKNbq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdd9459e-82a9-4ac1-823d-75d5060a070d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " _____ACTION SPACE_____ \n",
            "\n",
            "The Action Space is:  2\n",
            "Action Space Sample 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNWvlyvzalXr"
      },
      "source": [
        "The observation space (7) 👀:\n",
        "- player y position\n",
        "- player velocity\n",
        "- player distance to floor\n",
        "- player distance to ceiling\n",
        "- next block x distance to player\n",
        "- next blocks top y location\n",
        "- next blocks bottom y location\n",
        "\n",
        "The action space(2) 🎮:\n",
        "- Up (press accelerator)\n",
        "- Do nothing (don't press accelerator)\n",
        "\n",
        "The reward function 💰:\n",
        "- For each vertical block it passes through it gains a positive reward of +1. Each time a terminal state reached it receives a negative reward of -1."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the new Policy 🧠\n",
        "- We need to have a deeper neural network since the environment is more complex"
      ],
      "metadata": {
        "id": "aV1466QP8crz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(s_size, h_size)\n",
        "        self.fc2 = nn.Linear(h_size, h_size*2)\n",
        "        self.fc3 = nn.Linear(h_size*2, a_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action)"
      ],
      "metadata": {
        "id": "wrNuVcHC8Xu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM1QiGCSbBkM"
      },
      "source": [
        "### Define the hyperparameters ⚙️\n",
        "- Because this environment is more complex.\n",
        "- Especially for the hidden size, we need more neurons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0uujOR_ypB6"
      },
      "outputs": [],
      "source": [
        "pixelcopter_hyperparameters = {\n",
        "    \"h_size\": 64,\n",
        "    \"n_training_episodes\": 6000,\n",
        "    \"n_evaluation_episodes\": 10,\n",
        "    \"max_t\": 10000,\n",
        "    \"gamma\": 0.99,\n",
        "    \"lr\": 1e-4,\n",
        "    \"env_id\": env_id,\n",
        "    \"state_space\": s_size,\n",
        "    \"action_space\": a_size,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Train it\n",
        "- We're now ready to train our agent 🔥."
      ],
      "metadata": {
        "id": "wyvXTJWm9GJG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mM2P_ckysFE"
      },
      "outputs": [],
      "source": [
        "# Create policy and place it to the device\n",
        "# torch.manual_seed(50)\n",
        "pixelcopter_policy = Policy(pixelcopter_hyperparameters[\"state_space\"], pixelcopter_hyperparameters[\"action_space\"], pixelcopter_hyperparameters[\"h_size\"]).to(device)\n",
        "pixelcopter_optimizer = optim.Adam(pixelcopter_policy.parameters(), lr=pixelcopter_hyperparameters[\"lr\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1HEqP-fy-Rf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9192da59-4645-49b0-cbbe-ec8f1aa11ce7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1000\tAverage Score: 3.33\n",
            "Episode 2000\tAverage Score: 5.05\n",
            "Episode 3000\tAverage Score: 10.30\n",
            "Episode 4000\tAverage Score: 9.96\n",
            "Episode 5000\tAverage Score: 10.38\n",
            "Episode 6000\tAverage Score: 13.03\n"
          ]
        }
      ],
      "source": [
        "scores = reinforce(pixelcopter_policy,\n",
        "                   pixelcopter_optimizer,\n",
        "                   pixelcopter_hyperparameters[\"n_training_episodes\"],\n",
        "                   pixelcopter_hyperparameters[\"max_t\"],\n",
        "                   pixelcopter_hyperparameters[\"gamma\"],\n",
        "                   1000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_agent(eval_env,\n",
        "               pixelcopter_hyperparameters[\"max_t\"],\n",
        "               pixelcopter_hyperparameters[\"n_evaluation_episodes\"],\n",
        "               pixelcopter_policy)"
      ],
      "metadata": {
        "id": "xgSUpoU3UzWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcee0918-c301-422c-f701-da87d38d0108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17.3, 16.08135566424672)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "record_video(eval_env, pixelcopter_policy, \"/content/replay-pixelcopter.mp4\")"
      ],
      "metadata": {
        "id": "0dPaCbeqUluQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SK3avA-CUzDh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0qCsPXLNTt24",
        "f4Wh4agpTmDN"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}