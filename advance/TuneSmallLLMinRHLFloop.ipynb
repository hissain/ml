{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hissain/ml/blob/main/advance/TuneSmallLLMinRHLFloop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                   ‚îÇ   Pretrained Base Model     ‚îÇ\n",
        "                   ‚îÇ  (e.g., TinyLlama/Qwen/etc) ‚îÇ\n",
        "                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                                 ‚îÇ\n",
        "                                 ‚ñº\n",
        "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                    ‚îÇ 1. Supervised Fine-Tuning‚îÇ\n",
        "                    ‚îÇ (SFT on instructions)    ‚îÇ\n",
        "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                                  ‚îÇ\n",
        "                     SFT Model    ‚îÇ\n",
        "                                  ‚ñº\n",
        "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "          ‚îÇ 2. Preference Data Collection (Human or   ‚îÇ\n",
        "          ‚îÇ    Simulated)                             ‚îÇ\n",
        "          ‚îÇ   ‚Ä¢ Generate two (or more) responses      ‚îÇ\n",
        "          ‚îÇ   ‚Ä¢ Rank: ‚Äúchosen‚Äù > ‚Äúrejected‚Äù           ‚îÇ\n",
        "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                                ‚îÇ\n",
        "                                ‚ñº\n",
        "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                    ‚îÇ 3. Reward Model Training ‚îÇ\n",
        "                    ‚îÇ  (learns ranking signal) ‚îÇ\n",
        "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                                   ‚îÇ\n",
        "                                   ‚ñº\n",
        "               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "               ‚îÇ 4. RLHF Loop (e.g., PPO Training) ‚îÇ\n",
        "               ‚îÇ   ‚Ä¢ SFT model becomes the ‚Äúpolicy‚Äù‚îÇ\n",
        "               ‚îÇ   ‚Ä¢ Generate outputs              ‚îÇ\n",
        "               ‚îÇ   ‚Ä¢ Reward model scores them      ‚îÇ\n",
        "               ‚îÇ   ‚Ä¢ PPO updates the policy        ‚îÇ\n",
        "               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                                   ‚îÇ\n",
        "                                   ‚ñº\n",
        "                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                   ‚îÇ       RLHF-Trained Model     ‚îÇ\n",
        "                   ‚îÇ (aligned, safer, higher-quality) ‚îÇ\n",
        "                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ],
      "metadata": {
        "id": "WJsa82uLJ2G_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üéØ Explanation of Flow\n",
        "## 1. Pretrained Base Model\n",
        "\n",
        "A small LLM (0.5B‚Äì3B parameters) serves as the starting point.\n",
        "\n",
        "## 2. SFT (Supervised Fine-Tuning)\n",
        "\n",
        "The model is trained on human-written or synthetic instruction/response pairs.\n",
        "This makes the model follow instructions before RLHF.\n",
        "\n",
        "## 3. Preference Data Collection\n",
        "\n",
        "For each prompt:\n",
        "\n",
        "Generate multiple candidate responses\n",
        "\n",
        "A human (or synthetic reward function) ranks them\n",
        "\n",
        "‚ÄúChosen‚Äù > ‚ÄúRejected‚Äù\n",
        "\n",
        "This is the core dataset for RLHF.\n",
        "\n",
        "## 4. Reward Model Training\n",
        "\n",
        "A reward model learns to predict the ranking.\n",
        "It turns preferences into a scalar score.\n",
        "\n",
        "## 5. PPO (or another RL method)\n",
        "\n",
        "The SFT model becomes the policy model.\n",
        "\n",
        "**Process**\n",
        "\n",
        "\n",
        "1. Generate responses from the policy\n",
        "\n",
        "2. Reward model scores them\n",
        "\n",
        "3. PPO updates weights toward high-reward behavior\n",
        "\n",
        "4. KL penalty prevents drifting too far from SFT behavior\n",
        "\n",
        "**Output: RLHF Final Model**\n",
        "\n",
        " Safer\n",
        "\n",
        " More helpful\n",
        "\n",
        "More human-aligned\n",
        "\n",
        "Higher-quality responses"
      ],
      "metadata": {
        "id": "m1uE0FokKDT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proximal Policy Optimization (PPO)\n",
        "\n",
        "**PPO** is a reinforcement learning algorithm used in RLHF to fine-tune language models.\n",
        "Its goal is to update a model‚Äôs behavior toward preferred outputs **without letting the\n",
        "model change too much at once**, which prevents instability or collapse.\n",
        "\n",
        "---\n",
        "\n",
        "## üî∂ Why PPO?\n",
        "\n",
        "Large language models are sensitive. If updates are too big, the model may:\n",
        "- forget language ability  \n",
        "- generate unstable text  \n",
        "- diverge during training  \n",
        "\n",
        "PPO limits the update size using a **clipped objective**, ensuring training remains stable.\n",
        "\n",
        "---\n",
        "\n",
        "## üî∂ Core Idea\n",
        "\n",
        "Compute the probability ratio between the new and old policy:\n",
        "\n"
      ],
      "metadata": {
        "id": "tLaYhQMIf5Bg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "H5QRJw90H6MH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Cv8lTj4H4hJ",
        "outputId": "b843a940-37b4-4966-da35-9d6c467b1c60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m465.5/465.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate datasets trl peft bitsandbytes sentencepiece wandb\n",
        "!pip install -q einops\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "ZU57kx-OIJRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import Dataset, load_dataset\n",
        "from trl import SFTTrainer, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import wandb\n"
      ],
      "metadata": {
        "id": "rZiPC0MuIG8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "p4a9gPiAIV9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "DEVICE = \"cuda\"\n",
        "\n",
        "USE_WANDB = False\n",
        "WANDB_PROJECT = \"rlhf-simulated-feedback\"\n",
        "\n",
        "if USE_WANDB:\n",
        "    wandb.init(project=WANDB_PROJECT)\n"
      ],
      "metadata": {
        "id": "pdcyDNJLIX8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Tokenizer & Model (8-bit for Colab)"
      ],
      "metadata": {
        "id": "oEGMBzCrIc0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "KnUiIqZVh8UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Loading without 8-bit quantization to bypass persistent ImportError\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        ").eval()"
      ],
      "metadata": {
        "id": "0uqgHLD0IZsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Synthetic Training Data"
      ],
      "metadata": {
        "id": "voEoiZKvIg92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_data = {\n",
        "    \"prompt\": [\n",
        "        \"Explain reinforcement learning in one paragraph.\",\n",
        "        \"Write a short poem about rockets.\",\n",
        "        \"Describe why fine-tuning is useful.\",\n",
        "        \"Explain the difference between PPO and DQN.\"\n",
        "    ] * 250,\n",
        "\n",
        "    \"response\": [\n",
        "        \"Reinforcement learning is a computational framework where agents learn optimal behaviors through trial and error...\",\n",
        "        \"Rockets roar into the sky, carrying dreams of distant stars...\",\n",
        "        \"Fine-tuning allows pretrained models to specialize...\",\n",
        "        \"PPO is policy-gradient based while DQN is value-based...\"\n",
        "    ] * 250\n",
        "}\n",
        "\n",
        "ds_sft = Dataset.from_dict(synthetic_data)\n",
        "ds_sft"
      ],
      "metadata": {
        "id": "ipEliWLkImC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare LoRA for Efficient Training"
      ],
      "metadata": {
        "id": "GvBm-wb_IsHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "fDAPufZ8IwDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SFT Training"
      ],
      "metadata": {
        "id": "CZwYv7wgIzXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer # Added AutoTokenizer\n",
        "from trl import SFTTrainer, SFTConfig # Import SFTConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # Added MODEL_NAME definition\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Loading without 8-bit quantization to bypass persistent ImportError\n",
        "model_sft = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model_sft = get_peft_model(model_sft, lora_config)\n",
        "\n",
        "# Replace TrainingArguments with SFTConfig\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=\"./sft-output\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=50,\n",
        "    learning_rate=2e-4,\n",
        "    # Set completion_only_loss to False to train on the full prompt+response\n",
        "    completion_only_loss=False,\n",
        "    bf16=False, # Explicitly disable bf16\n",
        "    fp16=True,  # Explicitly enable fp16 if available\n",
        ")\n",
        "\n",
        "def formatting_func(example):\n",
        "    # Combine prompt and response into a single string as expected by SFTTrainer for full text training\n",
        "    return f\"Prompt: {example['prompt']}\\nResponse: {example['response']}\"\n",
        "\n",
        "# Add 'completion' field to ds_sft to satisfy SFTTrainer's internal tokenizer\n",
        "def add_completion_field(example):\n",
        "    example[\"completion\"] = example[\"response\"]\n",
        "    return example\n",
        "\n",
        "ds_sft_processed = ds_sft.map(add_completion_field)\n",
        "\n",
        "sft_trainer = SFTTrainer(\n",
        "    model=model_sft,\n",
        "    train_dataset=ds_sft_processed, # Use the processed dataset\n",
        "    formatting_func=formatting_func, # Use the updated formatting_func\n",
        "    args=sft_config, # Pass SFTConfig object here\n",
        ")\n",
        "\n",
        "sft_trainer.train()\n",
        "model_sft.save_pretrained(\"./sft-trained\")"
      ],
      "metadata": {
        "id": "wXdHEoHSI19A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate ‚ÄúBetter‚Äù and ‚ÄúWorse‚Äù Responses (Simulated Human Feedback)"
      ],
      "metadata": {
        "id": "Bg0dvgiII8Lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from transformers import AutoTokenizer # Added AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # Added MODEL_NAME definition\n",
        "DEVICE = \"cuda\" # Added DEVICE definition\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def generate_variants(prompt):\n",
        "    # low-effort response\n",
        "    short = model_sft.generate(\n",
        "        **tokenizer(prompt, return_tensors=\"pt\").to(DEVICE),\n",
        "        max_new_tokens=32\n",
        "    )\n",
        "    # detailed response\n",
        "    long = model_sft.generate(\n",
        "        **tokenizer(prompt, return_tensors=\"pt\").to(DEVICE),\n",
        "        max_new_tokens=128\n",
        "    )\n",
        "    return (\n",
        "        tokenizer.decode(short[0], skip_special_tokens=True),\n",
        "        tokenizer.decode(long[0], skip_special_tokens=True)\n",
        "    )\n",
        "\n",
        "pairs = []\n",
        "for p in synthetic_data[\"prompt\"][:600]:\n",
        "    bad, good = generate_variants(p)\n",
        "    pairs.append({\"prompt\": p, \"chosen\": good, \"rejected\": bad})\n",
        "\n",
        "ds_preferences = Dataset.from_list(pairs)\n",
        "ds_preferences"
      ],
      "metadata": {
        "id": "g6Uiz5heI_RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a Reward Model"
      ],
      "metadata": {
        "id": "Z6viOyREJTfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=1,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "def preprocess_reward(batch):\n",
        "    text = [f\"Prompt: {p}\\nResponse: {c}\" for p, c in zip(batch[\"prompt\"], batch[\"chosen\"])]\n",
        "    text_bad = [f\"Prompt: {p}\\nResponse: {r}\" for p, r in zip(batch[\"prompt\"], batch[\"rejected\"])]\n",
        "    batch[\"text_chosen\"] = text\n",
        "    batch[\"text_rejected\"] = text_bad\n",
        "    return batch\n",
        "\n",
        "displayed_ds_preferences = ds_preferences # Ensure ds_preferences is defined and accessible here\n",
        "ds_reward = displayed_ds_preferences.map(preprocess_reward)\n",
        "\n",
        "from trl import RewardTrainer\n",
        "\n",
        "reward_trainer = RewardTrainer(\n",
        "    model=reward_model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=ds_reward,\n",
        "    args=dict(\n",
        "        per_device_train_batch_size=4,\n",
        "        num_train_epochs=1,\n",
        "        output_dir=\"./reward-model\"\n",
        "    )\n",
        ")\n",
        "\n",
        "reward_trainer.train()\n",
        "reward_model.save_pretrained(\"./reward-model\")"
      ],
      "metadata": {
        "id": "9PyJfRH8JdVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RLHF PPO Training"
      ],
      "metadata": {
        "id": "HMQ4krbrJjHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from trl import AutoModelForCausalLMWithValueHead, PPOTrainer\n",
        "\n",
        "# Load the base SFT-trained model first\n",
        "base_ppo_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"./sft-trained\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Then wrap it with AutoModelForCausalLMWithValueHead\n",
        "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
        "    base_ppo_model # Pass the loaded base model\n",
        ")\n",
        "\n",
        "ppo_trainer = PPOTrainer(\n",
        "    model=ppo_model,\n",
        "    tokenizer=tokenizer,\n",
        "    reward_model=reward_model,\n",
        "    dataset=ds_sft,\n",
        "    args=dict(\n",
        "        learning_rate=1e-5,\n",
        "        batch_size=4,\n",
        "        mini_batch_size=2,\n",
        "        target_kl=1.0,\n",
        "        output_dir=\"./ppo-rlhf\"\n",
        "    )\n",
        ")\n",
        "\n",
        "generation_kwargs = dict(\n",
        "    max_new_tokens=100,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "ppo_trainer.train(generation_kwargs=generation_kwargs)\n",
        "ppo_model.save_pretrained(\"./ppo-final\")"
      ],
      "metadata": {
        "id": "0Wx2hrcAJkLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the Final Model"
      ],
      "metadata": {
        "id": "VeDG1YNwJnG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain why PPO works well in reinforcement learning.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "output = ppo_model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=150,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "YmdDrN7cJqlV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}