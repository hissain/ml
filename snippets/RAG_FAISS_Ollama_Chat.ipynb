{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db6fefc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  https://en.wikipedia.org/wiki/Main_Page\n",
      "Finished!\n",
      "Total text size:  1\n",
      "URL: https://en.wikipedia.org/wiki/Main_Page\n",
      "Text: Wikipedia, the free encyclopedia Jump to content Wiki Loves Monuments: Photograph a monument, help Wikipedia and win! Learn more From Wikipedia, the free encyclopedia Welcome to Wikipedia , the free e...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import time\n",
    "import faiss\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium_stealth import stealth\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urljoin\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque\n",
    "\n",
    "# Constants\n",
    "root = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "base_url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "\n",
    "ollama_url_inf = \"http://localhost:11434/api/show\"\n",
    "ollama_url_emb = \"http://localhost:11434/api/embeddings\"\n",
    "ollama_url_gen = \"http://localhost:11434/api/generate\"\n",
    "ollama_model_name = \"llama3.2:latest\"\n",
    "\n",
    "VERBOSE = True\n",
    "DEPTH = 0\n",
    "\n",
    "# Configure Selenium Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "service = Service()\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "stealth(driver, languages=[\"en-US\", \"en\"], vendor=\"Google Inc.\", platform=\"Win32\", \n",
    "        webgl_vendor=\"Intel Inc.\", renderer=\"Intel Iris OpenGL Engine\", fix_hairline=True)\n",
    "\n",
    "def extract_clean_text_from_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Remove unwanted tags\n",
    "    for script in soup([\"script\", \"style\", \"footer\", \"header\", \"nav\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # Get clean text\n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    text = ' '.join(text.split())  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "def scrape_recursive(base_url, depth=DEPTH):\n",
    "    \"\"\"Scrape text recursively up to a specified depth.\"\"\"\n",
    "    visited_urls = set()\n",
    "    text_data = []\n",
    "    queue = deque([(base_url, 0)])  # (url, current_depth)\n",
    "    \n",
    "    while queue:\n",
    "        url, current_depth = queue.popleft()\n",
    "\n",
    "        print(\"Processing: \", url)\n",
    "        \n",
    "        if url in visited_urls or current_depth > depth:\n",
    "            continue\n",
    "        \n",
    "        if not url.startswith(root):\n",
    "            continue\n",
    "            \n",
    "        if '#' in url:\n",
    "            continue\n",
    "        \n",
    "        visited_urls.add(url)\n",
    "\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(1)\n",
    "\n",
    "            page_text = extract_clean_text_from_html(driver.page_source)\n",
    "            text_data.append((url, page_text))\n",
    "            \n",
    "            if current_depth < depth:\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                links = soup.find_all('a', href=True)\n",
    "                \n",
    "                for link in links:\n",
    "                    new_url = urljoin(base_url, link['href'])\n",
    "                    if new_url not in visited_urls and new_url.startswith(root):\n",
    "                        queue.append((new_url, current_depth + 1))\n",
    "        \n",
    "        except Exception as e:\n",
    "            if VERBOSE:\n",
    "                print(f\"Error with URL {url}: {e}\")\n",
    "    \n",
    "    return text_data\n",
    "\n",
    "# Run scraping and clean up driver\n",
    "try:\n",
    "    scraped_data = scrape_recursive(base_url)\n",
    "finally:\n",
    "    driver.quit()\n",
    "    print(\"Finished!\")\n",
    "\n",
    "print(\"Total text size: \", len(scraped_data))\n",
    "\n",
    "for url, text in scraped_data:\n",
    "    print(f\"URL: {url}\\nText: {text[:200]}...\\n\")  # Print a snippet of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b402c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372415cdd82b4a2cad810d94ec8f54ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Partitioning text:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partition count: 3\n",
      "Model's embedding dimension: 3072\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7651cf3eec473a8ed7b87029d7dbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching embeddings:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import faiss\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Function to partition text efficiently\n",
    "def partition_text(text, max_length):\n",
    "    sentences = text.split('. ')\n",
    "    partitions = []\n",
    "    current_part = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in tqdm(sentences, desc=\"Partitioning text\"):\n",
    "        sentence_length = len(sentence.split())\n",
    "        \n",
    "        if current_length + sentence_length > max_length:\n",
    "            partitions.append('. '.join(current_part))\n",
    "            current_part = []\n",
    "            current_length = 0\n",
    "        \n",
    "        current_part.append(sentence)\n",
    "        current_length += sentence_length\n",
    "    \n",
    "    if current_part:\n",
    "        partitions.append('. '.join(current_part))\n",
    "\n",
    "    return partitions\n",
    "\n",
    "\n",
    "# Asynchronous function to fetch model embedding shape\n",
    "async def get_embedding_shape():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        payload = {\"model\": ollama_model_name}\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        async with session.post(ollama_url_inf, headers=headers, data=json.dumps(payload)) as response:\n",
    "            if response.status == 200:\n",
    "                result = await response.json()\n",
    "                if 'model_info' in result and 'llama.embedding_length' in result['model_info']:\n",
    "                    return result['model_info'][\"llama.embedding_length\"]\n",
    "                else:\n",
    "                    print(\"ERROR: Embedding length not found in model info.\")\n",
    "                    return 0\n",
    "            else:\n",
    "                print(f\"ERROR: Error from Ollama: {response.status}\")\n",
    "                return 0\n",
    "\n",
    "# Asynchronous function to fetch embeddings for a batch of text partitions\n",
    "async def get_embeddings(partitions):\n",
    "    embeddings = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for partition in tqdm(partitions, desc=\"Fetching embeddings\"):\n",
    "            payload = {\"model\": ollama_model_name, \"prompt\": partition}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            async with session.post(ollama_url_emb, headers=headers, data=json.dumps(payload)) as response:\n",
    "                if response.status == 200:\n",
    "                    result = await response.json()\n",
    "                    embeddings.append(np.array(result.get('embedding', np.zeros(768))))  # Adjust based on model\n",
    "                else:\n",
    "                    print(f\"ERROR: Error from Ollama: {response.status}\")\n",
    "                    embeddings.append(np.zeros(768))\n",
    "    return embeddings\n",
    "\n",
    "# Store embeddings in FAISS with optimized bulk addition\n",
    "async def store_in_faiss(partitions):\n",
    "    dimension = await get_embedding_shape()\n",
    "    if VERBOSE:\n",
    "        print(f\"Model's embedding dimension: {dimension}\")\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    embeddings = await get_embeddings(partitions)\n",
    "    if embeddings:\n",
    "        index.add(np.array(embeddings))  # Bulk add for efficiency\n",
    "    return index, list(range(len(partitions)))\n",
    "\n",
    "# Retrieve function with RAG applied to FAISS\n",
    "async def retrieve_with_rag(query, faiss_index, partitions, k=5):\n",
    "    query_embedding = (await get_embeddings([query]))[0]\n",
    "    distances, indices = faiss_index.search(np.array([query_embedding]), k=k)\n",
    "\n",
    "    retrieved_docs = []\n",
    "    for i in indices[0]:\n",
    "        if i < len(partitions):\n",
    "            retrieved_docs.append(partitions[i])\n",
    "\n",
    "    combined_docs = \"\\n\".join(retrieved_docs)\n",
    "    rag_prompt = f\"Instruction: If you do not find the exact answer in the context, simply say you dont know.\\nContext:\\n{combined_docs}\\n\\nQuery: {query}\\nAnswer:\"\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        payload = {\"model\": ollama_model_name, \"prompt\": rag_prompt, \"stream\": False}\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        async with session.post(ollama_url_gen, headers=headers, data=json.dumps(payload)) as response:\n",
    "            return await response.json()\n",
    "\n",
    "# Function to initiate retrieval with RAG\n",
    "async def ask(query):\n",
    "    rag_response = await retrieve_with_rag(query, faiss_index, partitions)\n",
    "    return rag_response.get(\"response\", \"No response available\")\n",
    "\n",
    "# Assuming you have `scraped_data` initialized as a list of text data\n",
    "all_text = scraped_data[0][1]\n",
    "partitions = partition_text(all_text, max_length=512)\n",
    "\n",
    "if VERBOSE:\n",
    "    print(f'Total partition count: {len(partitions)}')\n",
    "\n",
    "faiss_index, doc_ids = await store_in_faiss(partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eba59ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_text len: 7884\n"
     ]
    }
   ],
   "source": [
    "print(f'all_text len: {len(all_text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ea9865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de936e41e704e81995f35dc9a3fd938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The page you're referring to is the Wikipedia Main Page.\n"
     ]
    }
   ],
   "source": [
    "ans = await ask(\"Whats the page about?\")\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7aea5235",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_url_chat = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "async def chat(prompt):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        messages = {'role':'user', 'content':prompt}\n",
    "        payload = {\"model\": ollama_model_name, \"messages\": [messages], \"stream\": False}\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        async with session.post(ollama_url_chat, headers=headers, data=json.dumps(payload)) as response:\n",
    "            return await response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e618bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await chat(\"What is wikipedia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccb3e39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3.2:latest',\n",
       " 'created_at': '2024-10-26T14:53:19.204098Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': \"Wikipedia is a free online encyclopedia that allows anyone with an internet connection to access and contribute to its vast collection of articles on various subjects. It was founded in 2001 by Jimmy Wales and Larry Sanger, and it has since become one of the most popular websites in the world.\\n\\nWikipedia's content is edited and maintained by volunteers from around the globe, who contribute their knowledge and expertise to create and improve articles. The site uses a collaborative model, where anyone can register and start editing articles, but also relies on a system of checks and balances to ensure that information is accurate and reliable.\\n\\nHere are some key features of Wikipedia:\\n\\n1. **Free access**: Anyone with an internet connection can access Wikipedia without paying any fees or subscription.\\n2. **Collaborative model**: Anyone can contribute to and edit articles, but must follow community guidelines and rules.\\n3. **Verified sources**: Articles are based on verifiable sources, such as academic journals, books, and primary sources.\\n4. **Peer review**: Articles are reviewed by other editors before being published, ensuring accuracy and reliability.\\n5. **Content creation**: New content is constantly being added to Wikipedia, covering a vast range of topics.\\n\\nWikipedia has become an essential source of information for many people, students, researchers, and professionals. Its sheer size, breadth of coverage, and collaborative nature make it an invaluable resource for learning and knowledge-sharing.\\n\\nHowever, Wikipedia also faces challenges, such as:\\n\\n1. **Bias and inaccuracies**: Articles can reflect biases or errors in contributor research.\\n2. **Lack of expertise**: Some contributors may not have the necessary expertise to accurately represent a topic.\\n3. **Maintenance and updates**: With millions of articles, keeping information up-to-date and accurate can be a significant challenge.\\n\\nDespite these challenges, Wikipedia remains an essential tool for knowledge-sharing and education, and its impact on modern society cannot be overstated.\"},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 48452482627,\n",
       " 'load_duration': 2860889980,\n",
       " 'prompt_eval_count': 29,\n",
       " 'prompt_eval_duration': 1364760000,\n",
       " 'eval_count': 385,\n",
       " 'eval_duration': 44222545000}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0742a467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia is a free online encyclopedia that allows users to access and contribute to a vast repository of knowledge on various subjects. It was launched in January 2001 by Jimmy Wales and Larry Sanger, and it has since become one of the most popular and widely-used reference sources on the internet.\n",
      "\n",
      "Wikipedia's core principle is based on the concept of collaborative editing, where anyone with an internet connection can create an account and contribute to articles by adding or modifying content. The platform relies on a community-driven approach, where editors work together to review, verify, and improve the accuracy and quality of the information presented.\n",
      "\n",
      "Here are some key features that define Wikipedia:\n",
      "\n",
      "1. **Free and open**: Wikipedia is free to access and use, with no subscription or payment required.\n",
      "2. **Collaborative editing**: Anyone can create an account and contribute to articles, but this requires adherence to a set of guidelines and rules.\n",
      "3. **Encyclopedic content**: Articles are written in a neutral and objective tone, following a specific format and structure.\n",
      "4. **Community-driven**: The community of editors, administrators, and volunteers work together to maintain the quality and accuracy of the content.\n",
      "\n",
      "Wikipedia's vast collection of articles covers an incredible range of topics, including history, science, culture, entertainment, and more. While it is not considered a primary source or academic publication in many fields, Wikipedia has become a widely-used reference tool for many people around the world.\n",
      "\n",
      "However, it's worth noting that Wikipedia also faces challenges and criticisms related to:\n",
      "\n",
      "1. **Accuracy**: Articles can be subjective, biased, or incomplete, which can lead to inaccuracies.\n",
      "2. **Lack of expertise**: Anyone with internet access can edit articles, but this means that not all content is created by experts in a particular field.\n",
      "3. **Vandalism**: Some individuals may intentionally alter or delete articles, compromising the integrity of the platform.\n",
      "\n",
      "Despite these challenges, Wikipedia remains an essential resource for many people seeking information on various subjects, and it continues to evolve as a dynamic and collaborative knowledge-sharing platform."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Wikipedia is a free online encyclopedia that allows users to access and contribute to a vast repository of knowledge on various subjects. It was launched in January 2001 by Jimmy Wales and Larry Sanger, and it has since become one of the most popular and widely-used reference sources on the internet.\\n\\nWikipedia's core principle is based on the concept of collaborative editing, where anyone with an internet connection can create an account and contribute to articles by adding or modifying content. The platform relies on a community-driven approach, where editors work together to review, verify, and improve the accuracy and quality of the information presented.\\n\\nHere are some key features that define Wikipedia:\\n\\n1. **Free and open**: Wikipedia is free to access and use, with no subscription or payment required.\\n2. **Collaborative editing**: Anyone can create an account and contribute to articles, but this requires adherence to a set of guidelines and rules.\\n3. **Encyclopedic content**: Articles are written in a neutral and objective tone, following a specific format and structure.\\n4. **Community-driven**: The community of editors, administrators, and volunteers work together to maintain the quality and accuracy of the content.\\n\\nWikipedia's vast collection of articles covers an incredible range of topics, including history, science, culture, entertainment, and more. While it is not considered a primary source or academic publication in many fields, Wikipedia has become a widely-used reference tool for many people around the world.\\n\\nHowever, it's worth noting that Wikipedia also faces challenges and criticisms related to:\\n\\n1. **Accuracy**: Articles can be subjective, biased, or incomplete, which can lead to inaccuracies.\\n2. **Lack of expertise**: Anyone with internet access can edit articles, but this means that not all content is created by experts in a particular field.\\n3. **Vandalism**: Some individuals may intentionally alter or delete articles, compromising the integrity of the platform.\\n\\nDespite these challenges, Wikipedia remains an essential resource for many people seeking information on various subjects, and it continues to evolve as a dynamic and collaborative knowledge-sharing platform.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import aiohttp\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "ollama_url_chat = \"http://localhost:11434/api/chat\"\n",
    "ollama_model_name = \"llama3.2:latest\"\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "async def chat(prompt, stream=True):\n",
    "    global chat_history\n",
    "    chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "    payload = {\"model\": ollama_model_name, \"messages\": chat_history, \"stream\": stream}\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    response_text = \"\"\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(ollama_url_chat, headers=headers, data=json.dumps(payload)) as response:\n",
    "            if stream:\n",
    "                async for chunk in response.content.iter_any():\n",
    "                    try:\n",
    "                        data = json.loads(chunk.decode('utf-8'))\n",
    "                        content = data.get(\"message\", {}).get(\"content\", \"\")\n",
    "                        response_text += content\n",
    "                        print(content, end='')\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "            else:\n",
    "                response_text = await response.text()\n",
    "                print(response_text)\n",
    "    \n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "    \n",
    "    return response_text\n",
    "\n",
    "await chat(\"What is Wikipedia?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb806b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia's vision is to create a free online encyclopedia that provides accurate and reliable information on any topic in the world, accessible to anyone with an internet connection.\n",
      "\n",
      "Specifically, Wikipedia aims to:\n",
      "\n",
      "1. **Make knowledge universally accessible**: Provide free access to high-quality educational resources for everyone, regardless of their geographical location or socio-economic background.\n",
      "2. **Foster a collaborative community**: Encourage and support a global community of editors, administrators, and volunteers who work together to create, review, and improve the content.\n",
      "3. **Provide accurate and reliable information**: Strive for accuracy, neutrality, and objectivity in all articles, ensuring that users receive trustworthy information on various subjects.\n",
      "\n",
      "Wikipedia's vision is guided by its core values:\n",
      "\n",
      "1. **Free access**: Make knowledge freely available to anyone with an internet connection.\n",
      "2. **Collaborative editing**: Encourage and support a global community of editors and contributors who work together to create and improve content.\n",
      "3. **Neutral point of view**: Ensure that articles present information in a neutral and objective manner, avoiding bias or personal opinions.\n",
      "4. **Continuous improvement**: Strive for ongoing improvement and refinement of the content, based on user feedback and expert input.\n",
      "\n",
      "By achieving its vision, Wikipedia aims to:\n",
      "\n",
      "1. **Empower individuals**: Provide users with the knowledge and skills they need to make informed decisions in their daily lives.\n",
      "2. **Foster global understanding**: Promote cross-cultural understanding and exchange of ideas by providing a platform for diverse perspectives and experiences.\n",
      "3. **Support education and research**: Serve as a valuable resource for educators, researchers, and students, helping them access high-quality information on various subjects.\n",
      "\n",
      "Overall, Wikipedia's vision is to create a dynamic and inclusive knowledge-sharing platform that empowers individuals and communities around the world."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Wikipedia's vision is to create a free online encyclopedia that provides accurate and reliable information on any topic in the world, accessible to anyone with an internet connection.\\n\\nSpecifically, Wikipedia aims to:\\n\\n1. **Make knowledge universally accessible**: Provide free access to high-quality educational resources for everyone, regardless of their geographical location or socio-economic background.\\n2. **Foster a collaborative community**: Encourage and support a global community of editors, administrators, and volunteers who work together to create, review, and improve the content.\\n3. **Provide accurate and reliable information**: Strive for accuracy, neutrality, and objectivity in all articles, ensuring that users receive trustworthy information on various subjects.\\n\\nWikipedia's vision is guided by its core values:\\n\\n1. **Free access**: Make knowledge freely available to anyone with an internet connection.\\n2. **Collaborative editing**: Encourage and support a global community of editors and contributors who work together to create and improve content.\\n3. **Neutral point of view**: Ensure that articles present information in a neutral and objective manner, avoiding bias or personal opinions.\\n4. **Continuous improvement**: Strive for ongoing improvement and refinement of the content, based on user feedback and expert input.\\n\\nBy achieving its vision, Wikipedia aims to:\\n\\n1. **Empower individuals**: Provide users with the knowledge and skills they need to make informed decisions in their daily lives.\\n2. **Foster global understanding**: Promote cross-cultural understanding and exchange of ideas by providing a platform for diverse perspectives and experiences.\\n3. **Support education and research**: Serve as a valuable resource for educators, researchers, and students, helping them access high-quality information on various subjects.\\n\\nOverall, Wikipedia's vision is to create a dynamic and inclusive knowledge-sharing platform that empowers individuals and communities around the world.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chat(\"What is it's vision?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
