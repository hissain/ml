{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db6fefc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  https://en.wikipedia.org/wiki/Main_Page\n",
      "Finished!\n",
      "Total text size:  1\n",
      "URL: https://en.wikipedia.org/wiki/Main_Page\n",
      "Text: Wikipedia, the free encyclopedia Jump to content Wiki Loves Monuments: Photograph a monument, help Wikipedia and win! Learn more From Wikipedia, the free encyclopedia Welcome to Wikipedia , the free e...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import time\n",
    "import faiss\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium_stealth import stealth\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urljoin\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque\n",
    "\n",
    "# Constants\n",
    "root = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "base_url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "\n",
    "ollama_url_inf = \"http://localhost:11434/api/show\"\n",
    "ollama_url_emb = \"http://localhost:11434/api/embeddings\"\n",
    "ollama_url_gen = \"http://localhost:11434/api/generate\"\n",
    "ollama_model_name = \"llama3.2:latest\"\n",
    "\n",
    "VERBOSE = True\n",
    "DEPTH = 0\n",
    "\n",
    "# Configure Selenium Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "service = Service()\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "stealth(driver, languages=[\"en-US\", \"en\"], vendor=\"Google Inc.\", platform=\"Win32\", \n",
    "        webgl_vendor=\"Intel Inc.\", renderer=\"Intel Iris OpenGL Engine\", fix_hairline=True)\n",
    "\n",
    "def extract_clean_text_from_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Remove unwanted tags\n",
    "    for script in soup([\"script\", \"style\", \"footer\", \"header\", \"nav\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # Get clean text\n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    text = ' '.join(text.split())  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "def scrape_recursive(base_url, depth=DEPTH):\n",
    "    \"\"\"Scrape text recursively up to a specified depth.\"\"\"\n",
    "    visited_urls = set()\n",
    "    text_data = []\n",
    "    queue = deque([(base_url, 0)])  # (url, current_depth)\n",
    "    \n",
    "    while queue:\n",
    "        url, current_depth = queue.popleft()\n",
    "\n",
    "        print(\"Processing: \", url)\n",
    "        \n",
    "        if url in visited_urls or current_depth > depth:\n",
    "            continue\n",
    "        \n",
    "        if not url.startswith(root):\n",
    "            continue\n",
    "            \n",
    "        if '#' in url:\n",
    "            continue\n",
    "        \n",
    "        visited_urls.add(url)\n",
    "\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(1)\n",
    "\n",
    "            page_text = extract_clean_text_from_html(driver.page_source)\n",
    "            text_data.append((url, page_text))\n",
    "            \n",
    "            if current_depth < depth:\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                links = soup.find_all('a', href=True)\n",
    "                \n",
    "                for link in links:\n",
    "                    new_url = urljoin(base_url, link['href'])\n",
    "                    if new_url not in visited_urls and new_url.startswith(root):\n",
    "                        queue.append((new_url, current_depth + 1))\n",
    "        \n",
    "        except Exception as e:\n",
    "            if VERBOSE:\n",
    "                print(f\"Error with URL {url}: {e}\")\n",
    "    \n",
    "    return text_data\n",
    "\n",
    "# Run scraping and clean up driver\n",
    "try:\n",
    "    scraped_data = scrape_recursive(base_url)\n",
    "finally:\n",
    "    driver.quit()\n",
    "    print(\"Finished!\")\n",
    "\n",
    "print(\"Total text size: \", len(scraped_data))\n",
    "\n",
    "for url, text in scraped_data:\n",
    "    print(f\"URL: {url}\\nText: {text[:200]}...\\n\")  # Print a snippet of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b402c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372415cdd82b4a2cad810d94ec8f54ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Partitioning text:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partition count: 3\n",
      "Model's embedding dimension: 3072\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7651cf3eec473a8ed7b87029d7dbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching embeddings:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import faiss\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Function to partition text efficiently\n",
    "def partition_text(text, max_length):\n",
    "    sentences = text.split('. ')\n",
    "    partitions = []\n",
    "    current_part = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in tqdm(sentences, desc=\"Partitioning text\"):\n",
    "        sentence_length = len(sentence.split())\n",
    "        \n",
    "        if current_length + sentence_length > max_length:\n",
    "            partitions.append('. '.join(current_part))\n",
    "            current_part = []\n",
    "            current_length = 0\n",
    "        \n",
    "        current_part.append(sentence)\n",
    "        current_length += sentence_length\n",
    "    \n",
    "    if current_part:\n",
    "        partitions.append('. '.join(current_part))\n",
    "\n",
    "    return partitions\n",
    "\n",
    "\n",
    "# Asynchronous function to fetch model embedding shape\n",
    "async def get_embedding_shape():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        payload = {\"model\": ollama_model_name}\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        async with session.post(ollama_url_inf, headers=headers, data=json.dumps(payload)) as response:\n",
    "            if response.status == 200:\n",
    "                result = await response.json()\n",
    "                if 'model_info' in result and 'llama.embedding_length' in result['model_info']:\n",
    "                    return result['model_info'][\"llama.embedding_length\"]\n",
    "                else:\n",
    "                    print(\"ERROR: Embedding length not found in model info.\")\n",
    "                    return 0\n",
    "            else:\n",
    "                print(f\"ERROR: Error from Ollama: {response.status}\")\n",
    "                return 0\n",
    "\n",
    "# Asynchronous function to fetch embeddings for a batch of text partitions\n",
    "async def get_embeddings(partitions):\n",
    "    embeddings = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for partition in tqdm(partitions, desc=\"Fetching embeddings\"):\n",
    "            payload = {\"model\": ollama_model_name, \"prompt\": partition}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            async with session.post(ollama_url_emb, headers=headers, data=json.dumps(payload)) as response:\n",
    "                if response.status == 200:\n",
    "                    result = await response.json()\n",
    "                    embeddings.append(np.array(result.get('embedding', np.zeros(768))))  # Adjust based on model\n",
    "                else:\n",
    "                    print(f\"ERROR: Error from Ollama: {response.status}\")\n",
    "                    embeddings.append(np.zeros(768))\n",
    "    return embeddings\n",
    "\n",
    "# Store embeddings in FAISS with optimized bulk addition\n",
    "async def store_in_faiss(partitions):\n",
    "    dimension = await get_embedding_shape()\n",
    "    if VERBOSE:\n",
    "        print(f\"Model's embedding dimension: {dimension}\")\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    embeddings = await get_embeddings(partitions)\n",
    "    if embeddings:\n",
    "        index.add(np.array(embeddings))  # Bulk add for efficiency\n",
    "    return index, list(range(len(partitions)))\n",
    "\n",
    "# Retrieve function with RAG applied to FAISS\n",
    "async def retrieve_with_rag(query, faiss_index, partitions, k=5):\n",
    "    query_embedding = (await get_embeddings([query]))[0]\n",
    "    distances, indices = faiss_index.search(np.array([query_embedding]), k=k)\n",
    "\n",
    "    retrieved_docs = []\n",
    "    for i in indices[0]:\n",
    "        if i < len(partitions):\n",
    "            retrieved_docs.append(partitions[i])\n",
    "\n",
    "    combined_docs = \"\\n\".join(retrieved_docs)\n",
    "    rag_prompt = f\"Instruction: If you do not find the exact answer in the context, simply say you dont know.\\nContext:\\n{combined_docs}\\n\\nQuery: {query}\\nAnswer:\"\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        payload = {\"model\": ollama_model_name, \"prompt\": rag_prompt, \"stream\": False}\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        async with session.post(ollama_url_gen, headers=headers, data=json.dumps(payload)) as response:\n",
    "            return await response.json()\n",
    "\n",
    "# Function to initiate retrieval with RAG\n",
    "async def ask(query):\n",
    "    rag_response = await retrieve_with_rag(query, faiss_index, partitions)\n",
    "    return rag_response.get(\"response\", \"No response available\")\n",
    "\n",
    "# Assuming you have `scraped_data` initialized as a list of text data\n",
    "all_text = scraped_data[0][1]\n",
    "partitions = partition_text(all_text, max_length=512)\n",
    "\n",
    "if VERBOSE:\n",
    "    print(f'Total partition count: {len(partitions)}')\n",
    "\n",
    "faiss_index, doc_ids = await store_in_faiss(partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eba59ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_text len: 7884\n"
     ]
    }
   ],
   "source": [
    "print(f'all_text len: {len(all_text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ea9865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de936e41e704e81995f35dc9a3fd938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The page you're referring to is the Wikipedia Main Page.\n"
     ]
    }
   ],
   "source": [
    "ans = await ask(\"Whats the page about?\")\n",
    "print(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
