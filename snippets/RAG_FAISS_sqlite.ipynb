{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8db6fefc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  https://en.wikipedia.org/wiki/Bias\n",
      "Finished!\n",
      "Total url pased:  1\n",
      "URL: https://en.wikipedia.org/wiki/Bias\n",
      "content size: 80478\n",
      "Text: Bias - Wikipedia Jump to content The Bangla Wikivoyage Article Contest 2024 is now underway. Participate for a chance to win prizes! From Wikipedia, the free encyclopedia Inclination for or against Fo...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import time\n",
    "import faiss\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium_stealth import stealth\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urljoin\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque\n",
    "\n",
    "# Constants\n",
    "root = 'https://en.wikipedia.org/wiki/Bias'\n",
    "base_url = 'https://en.wikipedia.org/wiki/Bias'\n",
    "\n",
    "ollama_url_inf = \"http://localhost:11434/api/show\"\n",
    "ollama_url_emb = \"http://localhost:11434/api/embeddings\"\n",
    "ollama_url_gen = \"http://localhost:11434/api/generate\"\n",
    "ollama_model_name = \"llama3.2:latest\"\n",
    "\n",
    "VERBOSE = True\n",
    "DEPTH = 0\n",
    "MAX_LEN = 3\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n",
    "]\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--enable-javascript')\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument(f\"user-agent={random.choice(USER_AGENTS)}\")\n",
    "\n",
    "service = Service()\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def apply_stealth(driver):\n",
    "    stealth(driver, languages=[\"en-US\", \"en\"], vendor=\"Google Inc.\", platform=\"Win32\", \n",
    "        webgl_vendor=\"Intel Inc.\", renderer=\"Intel Iris OpenGL Engine\", fix_hairline=True,\n",
    "        run_on_insecure_content=True, fake_media_devices=True)\n",
    "\n",
    "apply_stealth(driver)\n",
    "\n",
    "def extract_clean_text_from_html(html_content):\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Remove unwanted tags\n",
    "    for script in soup([\"script\", \"style\", \"footer\", \"header\", \"nav\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # Get clean text\n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def scrape_recursive(base_url, depth=DEPTH, maxLen=MAX_LEN):\n",
    "\n",
    "    visited_urls = set()\n",
    "    text_data = []\n",
    "    queue = deque([(base_url, 0)])  # (url, current_depth)\n",
    "    \n",
    "    while queue:\n",
    "        url, current_depth = queue.popleft()\n",
    "        \n",
    "        if len(visited_urls) >= maxLen:\n",
    "            continue\n",
    "        \n",
    "        if url in visited_urls or current_depth > depth:\n",
    "            continue\n",
    "        \n",
    "        if not url.startswith(root):\n",
    "            continue\n",
    "            \n",
    "        if '#' in url:\n",
    "            continue\n",
    "        \n",
    "        print(\"Processing: \", url)\n",
    "        \n",
    "        visited_urls.add(url)\n",
    "        \n",
    "        try:\n",
    "            #apply_stealth(driver)\n",
    "            driver.get(url)\n",
    "            time.sleep(1)\n",
    "            \n",
    "            page_text = extract_clean_text_from_html(driver.page_source)\n",
    "            text_data.append((url, page_text))\n",
    "            \n",
    "            if current_depth < depth:\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                links = soup.find_all('a', href=True)\n",
    "                \n",
    "                for link in links:\n",
    "                    new_url = urljoin(base_url, link['href'])\n",
    "                    if new_url not in visited_urls and new_url.startswith(root):\n",
    "                        queue.append((new_url, current_depth + 1))\n",
    "        \n",
    "        except Exception as e:\n",
    "            if VERBOSE:\n",
    "                print(f\"Error with URL {url}: {e}\")\n",
    "    \n",
    "    return text_data\n",
    "\n",
    "\n",
    "try:\n",
    "    scraped_data = scrape_recursive(base_url)\n",
    "finally:\n",
    "    #driver.quit()\n",
    "    print(\"Finished!\")\n",
    "\n",
    "print(\"Total url pased: \", len(scraped_data))\n",
    "\n",
    "for url, text in scraped_data:\n",
    "    print(f\"URL: {url}\\ncontent size: {len(text)}\\nText: {text[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b402c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00fcb438b04486f8ba1ed83d8e8a914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Partitioning text:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's embedding dimension: 3072\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890ef9f75f9b4baaac7214c8c942375e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching embeddings:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 155\u001b[0m\n\u001b[1;32m    153\u001b[0m setup_db()\n\u001b[1;32m    154\u001b[0m partitions \u001b[38;5;241m=\u001b[39m partition_text(scraped_data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m][:\u001b[38;5;241m7000\u001b[39m], max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m--> 155\u001b[0m faiss_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m store_in_faiss(partitions)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Run a query\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mask\u001b[39m(query):\n",
      "Cell \u001b[0;32mIn[17], line 121\u001b[0m, in \u001b[0;36mstore_in_faiss\u001b[0;34m(partitions)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Load or create FAISS index\u001b[39;00m\n\u001b[1;32m    120\u001b[0m index \u001b[38;5;241m=\u001b[39m load_faiss_index(dimension)\n\u001b[0;32m--> 121\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m get_embeddings(partitions)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Save embeddings to FAISS index and database\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings:\n",
      "Cell \u001b[0;32mIn[17], line 104\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(partitions)\u001b[0m\n\u001b[1;32m    102\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: OLLAMA_MODEL_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: partition}\n\u001b[1;32m    103\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mpost(OLLAMA_URL_EMB, headers\u001b[38;5;241m=\u001b[39mheaders, data\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(payload)) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    106\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/aiohttp/client.py:1141\u001b[0m, in \u001b[0;36m_BaseRequestContextManager.__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _RetType:\n\u001b[0;32m-> 1141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coro\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/aiohttp/client.py:560\u001b[0m, in \u001b[0;36mClientSession._request\u001b[0;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, proxy_headers, trace_request_ctx, read_bufsize)\u001b[0m\n\u001b[1;32m    558\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m req\u001b[38;5;241m.\u001b[39msend(conn)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstart(conn)\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    562\u001b[0m     resp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/aiohttp/client_reqrep.py:899\u001b[0m, in \u001b[0;36mClientResponse.start\u001b[0;34m(self, connection)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    898\u001b[0m     protocol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\n\u001b[0;32m--> 899\u001b[0m     message, payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mread()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m http\u001b[38;5;241m.\u001b[39mHttpProcessingError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[1;32m    902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_info,\n\u001b[1;32m    903\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    906\u001b[0m         headers\u001b[38;5;241m=\u001b[39mexc\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    907\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/aiohttp/streams.py:616\u001b[0m, in \u001b[0;36mDataQueue.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mcreate_future()\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (asyncio\u001b[38;5;241m.\u001b[39mCancelledError, asyncio\u001b[38;5;241m.\u001b[39mTimeoutError):\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque\n",
    "from urllib.parse import urljoin\n",
    "from aiohttp import ClientTimeout\n",
    "\n",
    "DB_PATH = \"../datasets/rag_faiss.db\"\n",
    "INDEX_PATH = \"../datasets/faiss_index.bin\"\n",
    "\n",
    "OLLAMA_URL_INF = \"http://localhost:11434/api/show\"\n",
    "OLLAMA_URL_EMB = \"http://localhost:11434/api/embeddings\"\n",
    "OLLAMA_URL_GEN = \"http://localhost:11434/api/generate\"\n",
    "OLLAMA_MODEL_NAME = \"llama3.2:latest\"\n",
    "VERBOSE = True\n",
    "\n",
    "# Database setup for partition metadata\n",
    "def setup_db():\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('''CREATE TABLE IF NOT EXISTS partitions (id INTEGER PRIMARY KEY, text TEXT)''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Store partitions in database\n",
    "def store_partitions_in_db(partitions):\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cur = conn.cursor()\n",
    "    for part in partitions:\n",
    "        cur.execute(\"INSERT INTO partitions (text) VALUES (?)\", (part,))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Load partitions from the database\n",
    "def load_partitions_from_db():\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT text FROM partitions\")\n",
    "    partitions = [row[0] for row in cur.fetchall()]\n",
    "    conn.close()\n",
    "    return partitions\n",
    "\n",
    "# Save FAISS index to disk\n",
    "def save_faiss_index(index):\n",
    "    faiss.write_index(index, INDEX_PATH)\n",
    "\n",
    "# Load FAISS index from disk if available\n",
    "def load_faiss_index(dimension):\n",
    "    if os.path.exists(INDEX_PATH):\n",
    "        return faiss.read_index(INDEX_PATH)\n",
    "    else:\n",
    "        return faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Partition text into manageable lengths\n",
    "def partition_text(text, max_length):\n",
    "    sentences = text.split('. ')\n",
    "    partitions, current_part, current_length = [], [], 0\n",
    "\n",
    "    for sentence in tqdm(sentences, desc=\"Partitioning text\"):\n",
    "        sentence_length = len(sentence.split())\n",
    "        \n",
    "        if current_length + sentence_length > max_length:\n",
    "            partitions.append('. '.join(current_part))\n",
    "            current_part, current_length = [], 0\n",
    "        \n",
    "        current_part.append(sentence)\n",
    "        current_length += sentence_length\n",
    "    \n",
    "    if current_part:\n",
    "        partitions.append('. '.join(current_part))\n",
    "    \n",
    "    return partitions\n",
    "\n",
    "# Fetch model embedding dimension\n",
    "async def get_embedding_shape():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        payload = {\"model\": OLLAMA_MODEL_NAME}\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        async with session.post(OLLAMA_URL_INF, headers=headers, data=json.dumps(payload)) as response:\n",
    "            if response.status == 200:\n",
    "                result = await response.json()\n",
    "                return result['model_info'][\"llama.embedding_length\"]\n",
    "            else:\n",
    "                print(f\"ERROR: Error from Ollama: {response.status}\")\n",
    "                return 0\n",
    "\n",
    "from aiohttp import ClientTimeout\n",
    "TIMEOUT = ClientTimeout(total=180)\n",
    "\n",
    "async def get_embeddings(partitions):\n",
    "    embeddings = []\n",
    "    dimension = await get_embedding_shape()\n",
    "    async with aiohttp.ClientSession(timeout=TIMEOUT) as session:\n",
    "        for partition in tqdm(partitions, desc=\"Fetching embeddings\"):\n",
    "            payload = {\"model\": OLLAMA_MODEL_NAME, \"prompt\": partition}\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            async with session.post(OLLAMA_URL_EMB, headers=headers, data=json.dumps(payload)) as response:\n",
    "                if response.status == 200:\n",
    "                    result = await response.json()\n",
    "                    embeddings.append(np.array(result.get('embedding', np.zeros(dimension))))\n",
    "                else:\n",
    "                    embeddings.append(np.zeros(dimension))\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Store embeddings in persistent FAISS\n",
    "async def store_in_faiss(partitions):\n",
    "    dimension = await get_embedding_shape()\n",
    "    if VERBOSE:\n",
    "        print(f\"Model's embedding dimension: {dimension}\")\n",
    "    \n",
    "    # Load or create FAISS index\n",
    "    index = load_faiss_index(dimension)\n",
    "    embeddings = await get_embeddings(partitions)\n",
    "    \n",
    "    # Save embeddings to FAISS index and database\n",
    "    if embeddings:\n",
    "        index.add(np.array(embeddings))\n",
    "        save_faiss_index(index)\n",
    "        store_partitions_in_db(partitions)\n",
    "    \n",
    "    return index\n",
    "\n",
    "# Retrieve function with RAG applied to FAISS\n",
    "async def retrieve_with_rag(query, faiss_index, k=5):\n",
    "    query_embedding = (await get_embeddings([query]))[0]\n",
    "    distances, indices = faiss_index.search(np.array([query_embedding]), k=k)\n",
    "\n",
    "    partitions = load_partitions_from_db()\n",
    "    retrieved_docs = []\n",
    "    for i in indices[0]:\n",
    "        if i < len(partitions):\n",
    "            retrieved_docs.append(partitions[i])\n",
    "\n",
    "    combined_docs = \"\\n\".join(retrieved_docs)\n",
    "    inst = \"Instruction: If you do not find the answer in the context, just say you don't know.\"\n",
    "    rag_prompt = f\"{inst}\\n\\nContext:\\n{combined_docs}\\n\\nQuery: {query}\\nAnswer:\"\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        payload = {\"model\": OLLAMA_MODEL_NAME, \"prompt\": rag_prompt, \"stream\": False}\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        async with session.post(OLLAMA_URL_GEN, headers=headers, data=json.dumps(payload)) as response:\n",
    "            return await response.json()\n",
    "\n",
    "# Initialize DB and process partitions\n",
    "setup_db()\n",
    "partitions = partition_text(scraped_data[0][1][:7000], max_length=512)\n",
    "faiss_index = await store_in_faiss(partitions)\n",
    "\n",
    "# Run a query\n",
    "async def ask(query):\n",
    "    response = await retrieve_with_rag(query, faiss_index)\n",
    "    return response.get(\"response\", \"No response available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2ea9865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f286bf57b1d5430e9a78ed8be166d723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The page is about biases, specifically types of biases such as:\n",
      "\n",
      "* Cognitive biases (repeating or basic missteps in thinking, assessing, recollecting, etc.)\n",
      "* Statistical bias (a systematic error resulting from unfair sampling or estimation processes)\n",
      "\n",
      "It seems to be an informative article that explains the concept and types of biases.\n"
     ]
    }
   ],
   "source": [
    "ans = await ask(\"Whats the page about?\")\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "772418ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35ce84a3e6c47dc8fe8434b70767207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It appears that the text is a passage about \"Biases\". It discusses various types of biases, including:\n",
      "\n",
      "1. Cognitive biases\n",
      "2. Statistical bias\n",
      "3. Innate or learned biases\n",
      "\n",
      "The passage also provides an etymology of the word \"bias\" and its origins in Old Provençal and French.\n",
      "\n",
      "Would you like me to summarize the main points or provide more information on a specific type of bias?\n"
     ]
    }
   ],
   "source": [
    "ans = await ask(\"Whats the page about?\")\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3551a3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8c47c26e5c46dfb527f9007ed860c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I couldn't find any information on biases related to Bangladesh in my training data. However, I can tell you that biases exist in various forms in the country, including:\n",
      "\n",
      "1. **Social biases**: Bangladesh is a socially conservative country with strong cultural traditions. People may hold biases against individuals from different castes, ethnicities, or regions.\n",
      "2. **Economic biases**: Bangladesh has a large wealth gap, and some people may hold biases against those who are perceived as wealthy or from the upper class.\n",
      "3. **Education biases**: Education is highly valued in Bangladesh, but some people may hold biases against those who are less educated or come from disadvantaged backgrounds.\n",
      "\n",
      "Some specific examples of biases in Bangladesh include:\n",
      "\n",
      "1. **Caste-based biases**: Historically, the country has a caste system, and some people may hold biases against individuals from lower castes.\n",
      "2. **Regional biases**: Bangladesh is a diverse country with different regions having distinct cultures and languages. Some people may hold biases against those from other regions.\n",
      "3. **Socioeconomic biases**: Bangladesh has a significant wealth gap, and some people may hold biases against those who are perceived as wealthy or from the upper class.\n",
      "\n",
      "It's essential to recognize that these biases exist in various forms and can impact individuals and communities in different ways.\n"
     ]
    }
   ],
   "source": [
    "ans = await ask(\"Is ther anything about Bangladesh?\")\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "354c97be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d940b24589f34bc8897f1893a7da3f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The page is about the concept of bias in various contexts, including:\n",
      "\n",
      "* Cognitive biases: systematic errors or missteps in thinking and perception\n",
      "* Statistical bias: systematic errors in statistical sampling or estimation processes\n",
      "* Biases in science and engineering: systematic errors that can lead to inaccurate results\n",
      "* Types of biases: including innate and learned biases, cognitive biases, and others\n",
      "\n",
      "The text provides definitions, examples, and etymology of the term \"bias\", as well as information on how biases can affect perception, judgment, and behavior.\n"
     ]
    }
   ],
   "source": [
    "ans = await ask(\"Whats the page about?\")\n",
    "print(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
